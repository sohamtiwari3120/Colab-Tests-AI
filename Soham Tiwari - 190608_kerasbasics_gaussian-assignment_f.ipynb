{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Soham Tiwari - 190608_kerasbasics_gaussian-assignment_f.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QrZqmW-YtUKw"},"source":["# Keras Basics\n","We will learn about\n","* Dense layers\n","* Categorical cross-entropy\n","\n","A toy example to show how to train a classifier with Keras and use it. The data comes from three gaussian distributions."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eWyR310TtUKy","colab":{}},"source":["## DATA GENERATION\n","import numpy as np\n","\n","def generateX(cls):\n","    '''\n","    Inputs:\n","        cls: class {0, 1, 2}\n","    Outputs:\n","        x: a sample from cls; a np array of shape (2,)\n","    '''\n","    assert cls in [0,1,2]\n","    if cls==0:\n","        x = np.random.normal(np.array([0,0]),100)\n","#         100 is std deviation. std deviation is zero when x=xmean\n","    elif cls==1:\n","        x = np.random.normal(np.array([200,200]),100)\n","    elif cls==2:\n","        x = np.random.normal(np.array([-200,200]),100)\n","    return x\n","Nx = 2 # shape of a sample is (2,)\n","Ny = 3 # 3 classes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUsaxiU_du3v","colab_type":"code","outputId":"55bcc913-fd46-4f94-d53d-b0e1ff7b014f","executionInfo":{"status":"ok","timestamp":1560421075840,"user_tz":-330,"elapsed":1082,"user":{"displayName":"Soham Tiwari","photoUrl":"https://lh4.googleusercontent.com/-XiZ5rEdluPQ/AAAAAAAAAAI/AAAAAAAAH24/lvfjFM0g8Uw/s64/photo.jpg","userId":"06949155908663757402"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(generateX(1))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[269.2475539 267.0045024]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"24sOXTIttUK2"},"source":["Could you write a function to generate N samples from class 0 and N samples from class 1?"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"9W3ccvYYtUK4","nbgrader":{"checksum":"71c5837a9d68fac4398e11bcb87c3bd2","grade":false,"grade_id":"cell-6ee804e3860f2ff6","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["def generateXY(N):\n","    '''\n","    Inputs:\n","        N: no. of samples of each class\n","    Outputs:\n","        X: np array of samples; shape = (3*N, 2)\n","        Y: np array of samples; shape = (3*N, 1)\n","    '''\n","    # YOUR CODE HERE\n","    X=[]\n","    Y=[]\n","    for i in range(0,3):\n","      for j in range(0,N):\n","        X+=[generateX(i)]\n","        Y+=[[i]]\n","    \n","#     print(Y.shape)\n","#     print(X,Y)\n","    X=np.array(X).reshape(3*N,2)\n","    Y=np.array(Y).reshape(3*N,1)\n","#     print(X,\" \",Y)\n","    return X, Y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"HTJ1wSumvGGZ","nbgrader":{"checksum":"97c71a585cd83b64fc45836cd79d93a3","grade":false,"grade_id":"cell-d48d4d901cf257df","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["\n","# def generateXY(N):\n","#     '''\n","#     Inputs:\n","#         N: no. of samples of each class\n","#     Outputs:\n","#         X: np array of samples; shape = (3*N, 2)\n","#         Y: np array of samples; shape = (3*N, 1)\n","#     '''\n","#     # YOUR CODE HERE\n","#     return X, Y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"sUqeO_NWtUK7","nbgrader":{"checksum":"c0183471e369c049b734441886caaff4","grade":true,"grade_id":"cell-ad908829419fd089","locked":true,"points":1,"schema_version":1,"solution":false},"outputId":"bce00fbc-d35a-43a6-963e-f5ea71410092","executionInfo":{"status":"ok","timestamp":1560421076676,"user_tz":-330,"elapsed":1859,"user":{"displayName":"Soham Tiwari","photoUrl":"https://lh4.googleusercontent.com/-XiZ5rEdluPQ/AAAAAAAAAAI/AAAAAAAAH24/lvfjFM0g8Uw/s64/photo.jpg","userId":"06949155908663757402"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def test_generateXY():\n","    X_train, Y_train = generateXY(50)\n","    assert X_train.shape==(150,2)\n","    assert Y_train.shape==(150,1)\n","    print('Test passed', '\\U0001F44D')\n","test_generateXY()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Test passed ðŸ‘\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YJ3TNcj-tULI"},"source":["### One-hot encoding\n","\n","Now our Y is in the form [0], [1] and [2]. We want to convert them to [1,0,0], [0,1,0] and [0,0,1], respectively. \n","Could you write a code to convert Y (with one column) into one-hot encoded Y (with 3 columns)?"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"_n4fYMC0tULJ","nbgrader":{"checksum":"2920fc139021b2f772982b2e16731703","grade":false,"grade_id":"cell-db496b9b86c28424","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["def oneHot(y, Ny):\n","    '''\n","    Input:\n","        y: an int in {0, 1, 2}\n","        Ny: Number of classes, e.g., 3 here.\n","    Output:\n","        Y: a vector of Ny (=3) tuples\n","    '''\n","    # YOUR CODE HERE\n","    Y=np.eye(Ny)\n","#     print(Y.shape)\n","    return Y[y]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"fq8OZ0cxtULM","nbgrader":{"checksum":"8612cec704a627b66ff552899569f828","grade":true,"grade_id":"cell-24fb717c7ea66826","locked":true,"points":2,"schema_version":1,"solution":false},"outputId":"ef6371e9-de76-4def-98d5-d66a78a6b063","executionInfo":{"status":"ok","timestamp":1560421076680,"user_tz":-330,"elapsed":1835,"user":{"displayName":"Soham Tiwari","photoUrl":"https://lh4.googleusercontent.com/-XiZ5rEdluPQ/AAAAAAAAAAI/AAAAAAAAH24/lvfjFM0g8Uw/s64/photo.jpg","userId":"06949155908663757402"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def test_oneHot():\n","    assert np.all(oneHot(0,3)==np.array([1,0,0]))\n","    assert np.all(oneHot(1,3)==np.array([0,1,0]))\n","    assert np.all(oneHot(2,3)==np.array([0,0,1]))\n","    print('Test passed', '\\U0001F44D')\n","test_oneHot()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Test passed ðŸ‘\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EzQywDiStULR"},"source":["### Input Normalization\n","X can lie in any unbounded range. We need to curtail to a narrow range close to zero. This helps in enhancing the stability of training and hyper-parameter tuning.\n","This is normally achieved by scaling the X to have zero mean and unit standard deviation (std).\n","\n","$X \\leftarrow \\frac{X-mean(X)}{std(X)}$, where this is element wise division\n","\n","Could you use training samples to find mean and std, and normalize your X_train with that?"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"_v_HEe10tULS","nbgrader":{"checksum":"4d88f9abc4004f238b182e54336e76e2","grade":false,"grade_id":"cell-8564364c76ddcdc7","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["def findMeanStddev(X):\n","    '''\n","    Input: \n","        X: a matrix of size (no. of samples, dimension of each sample)\n","    Output:\n","        mean: mean of samples in X; shape is (dimension of each sample,)\n","        stddev: element-wise std dev of sample in X; shape is (dimension of each sample,)\n","    '''\n","    # YOUR CODE HERE\n","    mean=[]\n","    stddev=[]\n","    X=X.T\n","    for i in range(0,X.shape[0]):\n","      mean+=[np.mean(X[i])]\n","      stddev+=[np.std(X[i])]\n","    return mean, stddev"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"qYUioNiqxRyi","nbgrader":{"checksum":"5d5ccf5b778b190a8607fe045aebce74","grade":true,"grade_id":"cell-c060c271af9064e7","locked":true,"points":2,"schema_version":1,"solution":false},"outputId":"70d80158-fbd6-4b10-aafa-9734f1f2273a","executionInfo":{"status":"ok","timestamp":1560421076683,"user_tz":-330,"elapsed":1818,"user":{"displayName":"Soham Tiwari","photoUrl":"https://lh4.googleusercontent.com/-XiZ5rEdluPQ/AAAAAAAAAAI/AAAAAAAAH24/lvfjFM0g8Uw/s64/photo.jpg","userId":"06949155908663757402"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def test_findMeanStddev():\n","    X = np.array([[3,2,6],[7,4,2],[3,5,1]])\n","    mean, stddev = findMeanStddev(X)\n","    assert np.isclose(mean, np.array([4.33, 3.66, 3.]), atol=0.1).all()\n","    assert np.isclose(stddev, np.array([1.88, 1.24, 2.16]), atol=0.1).all()\n","    print('Test passed', '\\U0001F44D')\n","test_findMeanStddev()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Test passed ðŸ‘\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"gG4sm2WfwRqu","nbgrader":{"checksum":"6c5fc8807584cce426d6dd8fd10dfc7c","grade":false,"grade_id":"cell-80ad17d9f5962f88","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["def normalizeX(X, mean, stddev):\n","    '''\n","    Input:\n","        X: a matrix of size (no. of samples, dimension of each sample)\n","        mean: mean of samples in X (same size as X)\n","        stddev: element-wise std dev of sample in X (same size as X) \n","    Output:\n","        Xn: X modified to have 0 mean and 1 std dev\n","    '''\n","    # YOUR CODE HERE\n","#     mean=np.array(mean).reshape(X.shape[0],X.shape[1])\n","#     stddev=stddev.reshape(X.shape[0],X.shape[1])\n","    X=X.T\n","    for i in range(0,X.shape[0]):\n","      X[i]-=mean[i]\n","      if(not(stddev[i]==0)):\n","        X[i]/=stddev[i]\n","    \n","#     X=X/stddev\n","#     for i in range(0,X.shape[0]):\n","#       X[i]-=mean\n","#       X[i]/=stddev\n","    return X.T"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"t4JdFDb7tULZ","nbgrader":{"checksum":"cb4af2655a94d3e991efe9c64ba57a8c","grade":true,"grade_id":"cell-0880b9b53201680b","locked":true,"points":2,"schema_version":1,"solution":false},"outputId":"d988f6c3-5c3c-4305-cd4b-7c58cf31313b","executionInfo":{"status":"ok","timestamp":1560421076686,"user_tz":-330,"elapsed":1807,"user":{"displayName":"Soham Tiwari","photoUrl":"https://lh4.googleusercontent.com/-XiZ5rEdluPQ/AAAAAAAAAAI/AAAAAAAAH24/lvfjFM0g8Uw/s64/photo.jpg","userId":"06949155908663757402"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def test_normalizeX():\n","    X = np.ones((3,3))\n","    m,s = findMeanStddev(X)\n","    assert np.all(m==np.ones(3))\n","    assert np.all(s==np.zeros(3))\n","    assert np.all(normalizeX(X,m,s)==0*X)\n","    # test on random X\n","    X = np.random.random((5,3))\n","    m,s = findMeanStddev(X)\n","    Xn = normalizeX(X,m,s)\n","    mn, sn = findMeanStddev(Xn)\n","    assert np.allclose(mn, np.zeros(3))\n","    assert np.allclose(sn, np.ones(3))\n","    print('Test passed', '\\U0001F44D')\n","test_normalizeX()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Test passed ðŸ‘\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1zpEX8EEtULe"},"source":["### Plotting\n","Could you plot all the samples in X_train with different colors for different classes?"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JfkkWGWZtULf","colab":{}},"source":["import matplotlib.pyplot as plt\n","colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n","def plotXY(X, Y):\n","    '''\n","    Inputs:\n","        X: a matrix of size (no. of samples, dimension of each sample)\n","        Y: a matrix of size (no. of samples, no. of classes) - these are one-hot vectors\n","    Action:\n","        Plots the samples in X, their color depends on Y\n","    '''\n","    Ny = Y.shape[1]\n","    for cls in range(Ny):\n","        idx = np.where(Y[:,cls]==1)[0]\n","        plt.plot(X[idx,0], X[idx,1], colors[cls]+'.')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kB3jK6IrtULk"},"source":["## Creating the Network\n","We now create the network with dense layers: \n","$y = f(Wx)$\n","\n","ReLU activation: \n","$f(h) = h, h>0; 0, h\\le 0$\n","\n","Softmax activation: \n","$f(h_i) = \\frac{\\exp(h_i)}{\\sum_j \\exp(h_j)}$\n","\n","Categorical cross-entropy loss:\n","$\\mathcal{L} = -\\sum_t y^d_t \\log y_t$\n","\n","Stochastic Gradient Descent:\n","$w_{ij} \\leftarrow w_{ij} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_{ij}}$"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"g60Qem92tULl","nbgrader":{"checksum":"1f332e46663382a71f1dfa333725d807","grade":false,"grade_id":"cell-e18133df577f7820","locked":false,"schema_version":1,"solution":true},"outputId":"7b0aa46f-d4c5-4f9e-fbd7-c4ae77117211","executionInfo":{"status":"ok","timestamp":1560421077119,"user_tz":-330,"elapsed":2218,"user":{"displayName":"Soham Tiwari","photoUrl":"https://lh4.googleusercontent.com/-XiZ5rEdluPQ/AAAAAAAAAAI/AAAAAAAAH24/lvfjFM0g8Uw/s64/photo.jpg","userId":"06949155908663757402"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import keras\n","from keras.layers import Input, Dense\n","from keras.models import Model\n","from keras import optimizers\n","\n","def makeNN(Nx, Nh, Ny):\n","    '''\n","    Input:\n","        Nx: int; no. of input nodes; shape of each sample; i.e., X.shape[1:] \n","        Nh: int; no. of hidden neurons\n","        Ny: int; no. of output nodes; shape of output; i.e., Y.shape[1]\n","    Output:\n","        model: keras NN model with Input layer, Dense layer with Nh neurons, \n","                and Dense output layer with softmax non-linearity, loss function\n","                categorical-crossentropy, optimizer SGD.\n","    '''\n","    # YOUR CODE HERE\n","    input_layer=Input(shape=(Nx,))\n","#     print(input_layer.shape)\n","    hidden_layer=Dense(Nh, activation=\"softmax\")(input_layer)\n","#     print(hidden_layer.shape)\n","    output_layer=Dense(Ny, activation=\"softmax\")(hidden_layer)\n","#     print(output_layer.shape)\n","    \n","    model=Model(inputs=[input_layer],outputs=[output_layer])\n","    model.compile(optimizer=optimizers.Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n","    model.summary()\n","    return model"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Q-Gf4DSltULt"},"source":["### Plotting the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0qkPv1xptULu","colab":{}},"source":["def plotModel(model):\n","    from keras.utils import plot_model\n","    plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n","    from IPython.display import Image\n","    Image(retina=True, filename='model.png')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XO26y8VZtULz"},"source":["### Training\n"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"Yfvm6pH5tUL0","nbgrader":{"checksum":"78b5d80f2ec4bcbc57d80a6bc475f285","grade":false,"grade_id":"cell-8a4f621147d44a84","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["def trainNN(model, X_train, Y_train, Nepochs):\n","    '''\n","    Action:\n","        Train model with model.fit\n","    '''\n","    # YOUR CODE HERE\n","    history=    model.fit(X_train, Y_train, validation_split = 0.1, epochs=Nepochs)\n","# Use 10% of samples for validation, validation_split is the relevant parameter\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"QZRghwIG2I5c","nbgrader":{"checksum":"5d34ead5470a39daddbd54aa7f19ef1f","grade":false,"grade_id":"cell-c45fc6de4c3fc4c9","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["def trainModel(N, Nh, Nepochs):\n","    '''\n","    generateXY, normalizeX, oneHot, makeNN, trainNN\n","    Input:\n","        N: int; no. of training samples per class\n","        Nh: int; no. of neurons in hidden layer\n","    Output:\n","        model: keras NN model trained with the training data\n","        mean_train, stddev_train: mean and stddev of training data - you will \n","                            need this for normalizing your test data\n","    '''\n","    # YOUR CODE HERE\n","    X, Y=generateXY(N)\n","    mean ,stddev=findMeanStddev(X)\n","    X=normalizeX(X,mean,stddev)\n","#     print(np.unique(Y).shape[0])\n","    Y=oneHot(Y,np.unique(Y).shape[0])\n","    Y=Y.reshape(Y.shape[0],Y.shape[-1])\n","#     print(X.shape,Y.shape,Y)\n","    index=np.arange(3*N)\n","    np.random.shuffle(index)\n","    X=X[index]\n","    Y=Y[index] \n","    model=makeNN(X.shape[1],Nh,Y.shape[1])\n","    trainNN(model,X,Y,Nepochs)\n","    mean_train=mean\n","    stddev_train=stddev\n","    return model, mean_train, stddev_train"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y7a1LEcgtUL4"},"source":["### Evaluation\n","Could you:\n","- Generate 20 samples from each class\n","- Normalize them with mean_train and stddev_train\n","- Get Y_test as one hot encoded labels"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"dkj4meV_tUL9","nbgrader":{"checksum":"1c367e3d2d36a80eb1fffbb27eb32da8","grade":false,"grade_id":"cell-02b3b15dc3f435fe","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["def testModel(model, Ntest, mean_train, stddev_train):\n","    '''\n","    generateXY for test, normalize, onehot, evaluate the model\n","    Inputs:\n","        model: trained Keras NN model\n","        Ntest: int; number of test samples per class\n","    Output:\n","        accuracy: float; accuracy on the test data\n","        CM: confusion matrix on the test data\n","    '''\n","    # YOUR CODE HERE\n","    Xtest,Ytest=generateXY(Ntest)\n","#     mean,stddev=findMeanStddev(Xtest)\n","    Xtest=normalizeX(Xtest,mean_train,stddev_train)\n","    Ytest=oneHot(Ytest,np.unique(Ytest).shape[0])    \n","    Ytest=Ytest.reshape(Ytest.shape[0],Ytest.shape[-1])\n","#     trainNN(model,Xtest,Ytest,50)\n","    from sklearn.metrics import confusion_matrix\n","    loss, accuracy = model.evaluate(Xtest, Ytest,verbose=0)  # Evaluate the model\n","    print('Accuracy :%0.3f'%accuracy)\n","\n","    pred_Ytest = model.predict(Xtest)\n","    cm = confusion_matrix(Ytest.argmax(axis=1), pred_Ytest.argmax(axis=1))\n","    print(cm )\n","\n","    return accuracy, cm\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"N9AKg5YBtUMB","outputId":"bf1faa2e-95ea-4f5c-a869-d9581c069e03","executionInfo":{"status":"ok","timestamp":1560421089674,"user_tz":-330,"elapsed":14737,"user":{"displayName":"Soham Tiwari","photoUrl":"https://lh4.googleusercontent.com/-XiZ5rEdluPQ/AAAAAAAAAAI/AAAAAAAAH24/lvfjFM0g8Uw/s64/photo.jpg","userId":"06949155908663757402"}},"colab":{"base_uri":"https://localhost:8080/","height":17240}},"source":["model, mean_train, stddev_train = trainModel(50, 20, 500)\n","accuracy, CM = testModel(model, 10, mean_train, stddev_train)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 2)                 0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 20)                60        \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 3)                 63        \n","=================================================================\n","Total params: 123\n","Trainable params: 123\n","Non-trainable params: 0\n","_________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Train on 135 samples, validate on 15 samples\n","Epoch 1/500\n","135/135 [==============================] - 1s 8ms/step - loss: 1.0969 - acc: 0.3407 - val_loss: 1.0998 - val_acc: 0.2667\n","Epoch 2/500\n","135/135 [==============================] - 0s 150us/step - loss: 1.0949 - acc: 0.3407 - val_loss: 1.0979 - val_acc: 0.2667\n","Epoch 3/500\n","135/135 [==============================] - 0s 153us/step - loss: 1.0932 - acc: 0.3407 - val_loss: 1.0964 - val_acc: 0.2667\n","Epoch 4/500\n","135/135 [==============================] - 0s 146us/step - loss: 1.0915 - acc: 0.3407 - val_loss: 1.0952 - val_acc: 0.2667\n","Epoch 5/500\n","135/135 [==============================] - 0s 153us/step - loss: 1.0898 - acc: 0.3407 - val_loss: 1.0941 - val_acc: 0.2667\n","Epoch 6/500\n","135/135 [==============================] - 0s 141us/step - loss: 1.0881 - acc: 0.3407 - val_loss: 1.0927 - val_acc: 0.2667\n","Epoch 7/500\n","135/135 [==============================] - 0s 139us/step - loss: 1.0864 - acc: 0.3407 - val_loss: 1.0911 - val_acc: 0.2667\n","Epoch 8/500\n","135/135 [==============================] - 0s 147us/step - loss: 1.0846 - acc: 0.3407 - val_loss: 1.0893 - val_acc: 0.2667\n","Epoch 9/500\n","135/135 [==============================] - 0s 149us/step - loss: 1.0830 - acc: 0.3407 - val_loss: 1.0875 - val_acc: 0.2667\n","Epoch 10/500\n","135/135 [==============================] - 0s 144us/step - loss: 1.0812 - acc: 0.3481 - val_loss: 1.0859 - val_acc: 0.3333\n","Epoch 11/500\n","135/135 [==============================] - 0s 143us/step - loss: 1.0795 - acc: 0.3630 - val_loss: 1.0842 - val_acc: 0.4000\n","Epoch 12/500\n","135/135 [==============================] - 0s 136us/step - loss: 1.0777 - acc: 0.4000 - val_loss: 1.0826 - val_acc: 0.4000\n","Epoch 13/500\n","135/135 [==============================] - 0s 144us/step - loss: 1.0760 - acc: 0.4148 - val_loss: 1.0810 - val_acc: 0.4000\n","Epoch 14/500\n","135/135 [==============================] - 0s 135us/step - loss: 1.0743 - acc: 0.4148 - val_loss: 1.0797 - val_acc: 0.4000\n","Epoch 15/500\n","135/135 [==============================] - 0s 159us/step - loss: 1.0726 - acc: 0.4296 - val_loss: 1.0780 - val_acc: 0.4000\n","Epoch 16/500\n","135/135 [==============================] - 0s 143us/step - loss: 1.0709 - acc: 0.4370 - val_loss: 1.0760 - val_acc: 0.4667\n","Epoch 17/500\n","135/135 [==============================] - 0s 169us/step - loss: 1.0693 - acc: 0.4444 - val_loss: 1.0743 - val_acc: 0.4667\n","Epoch 18/500\n","135/135 [==============================] - 0s 143us/step - loss: 1.0676 - acc: 0.4667 - val_loss: 1.0729 - val_acc: 0.4000\n","Epoch 19/500\n","135/135 [==============================] - 0s 146us/step - loss: 1.0658 - acc: 0.4963 - val_loss: 1.0712 - val_acc: 0.4000\n","Epoch 20/500\n","135/135 [==============================] - 0s 143us/step - loss: 1.0639 - acc: 0.5259 - val_loss: 1.0700 - val_acc: 0.4000\n","Epoch 21/500\n","135/135 [==============================] - 0s 144us/step - loss: 1.0621 - acc: 0.5259 - val_loss: 1.0685 - val_acc: 0.4000\n","Epoch 22/500\n","135/135 [==============================] - 0s 131us/step - loss: 1.0601 - acc: 0.5852 - val_loss: 1.0666 - val_acc: 0.4000\n","Epoch 23/500\n","135/135 [==============================] - 0s 138us/step - loss: 1.0583 - acc: 0.6148 - val_loss: 1.0652 - val_acc: 0.4667\n","Epoch 24/500\n","135/135 [==============================] - 0s 128us/step - loss: 1.0564 - acc: 0.6296 - val_loss: 1.0636 - val_acc: 0.4667\n","Epoch 25/500\n","135/135 [==============================] - 0s 132us/step - loss: 1.0545 - acc: 0.6519 - val_loss: 1.0617 - val_acc: 0.5333\n","Epoch 26/500\n","135/135 [==============================] - 0s 158us/step - loss: 1.0525 - acc: 0.6667 - val_loss: 1.0604 - val_acc: 0.6000\n","Epoch 27/500\n","135/135 [==============================] - 0s 154us/step - loss: 1.0505 - acc: 0.6444 - val_loss: 1.0590 - val_acc: 0.6000\n","Epoch 28/500\n","135/135 [==============================] - 0s 143us/step - loss: 1.0485 - acc: 0.6519 - val_loss: 1.0574 - val_acc: 0.6000\n","Epoch 29/500\n","135/135 [==============================] - 0s 159us/step - loss: 1.0465 - acc: 0.6519 - val_loss: 1.0557 - val_acc: 0.6000\n","Epoch 30/500\n","135/135 [==============================] - 0s 145us/step - loss: 1.0445 - acc: 0.6593 - val_loss: 1.0536 - val_acc: 0.6000\n","Epoch 31/500\n","135/135 [==============================] - 0s 127us/step - loss: 1.0425 - acc: 0.6741 - val_loss: 1.0518 - val_acc: 0.6000\n","Epoch 32/500\n","135/135 [==============================] - 0s 157us/step - loss: 1.0404 - acc: 0.6815 - val_loss: 1.0499 - val_acc: 0.6000\n","Epoch 33/500\n","135/135 [==============================] - 0s 126us/step - loss: 1.0384 - acc: 0.7037 - val_loss: 1.0479 - val_acc: 0.6000\n","Epoch 34/500\n","135/135 [==============================] - 0s 144us/step - loss: 1.0362 - acc: 0.7185 - val_loss: 1.0462 - val_acc: 0.6000\n","Epoch 35/500\n","135/135 [==============================] - 0s 137us/step - loss: 1.0340 - acc: 0.7259 - val_loss: 1.0442 - val_acc: 0.6000\n","Epoch 36/500\n","135/135 [==============================] - 0s 142us/step - loss: 1.0318 - acc: 0.7481 - val_loss: 1.0422 - val_acc: 0.6000\n","Epoch 37/500\n","135/135 [==============================] - 0s 161us/step - loss: 1.0296 - acc: 0.7556 - val_loss: 1.0402 - val_acc: 0.6000\n","Epoch 38/500\n","135/135 [==============================] - 0s 129us/step - loss: 1.0273 - acc: 0.7704 - val_loss: 1.0383 - val_acc: 0.6000\n","Epoch 39/500\n","135/135 [==============================] - 0s 165us/step - loss: 1.0250 - acc: 0.7704 - val_loss: 1.0364 - val_acc: 0.6000\n","Epoch 40/500\n","135/135 [==============================] - 0s 129us/step - loss: 1.0227 - acc: 0.7704 - val_loss: 1.0346 - val_acc: 0.6000\n","Epoch 41/500\n","135/135 [==============================] - 0s 166us/step - loss: 1.0204 - acc: 0.7630 - val_loss: 1.0328 - val_acc: 0.6000\n","Epoch 42/500\n","135/135 [==============================] - 0s 138us/step - loss: 1.0179 - acc: 0.7630 - val_loss: 1.0308 - val_acc: 0.6000\n","Epoch 43/500\n","135/135 [==============================] - 0s 140us/step - loss: 1.0155 - acc: 0.7630 - val_loss: 1.0286 - val_acc: 0.6000\n","Epoch 44/500\n","135/135 [==============================] - 0s 145us/step - loss: 1.0131 - acc: 0.7704 - val_loss: 1.0265 - val_acc: 0.6000\n","Epoch 45/500\n","135/135 [==============================] - 0s 143us/step - loss: 1.0108 - acc: 0.7704 - val_loss: 1.0245 - val_acc: 0.6000\n","Epoch 46/500\n","135/135 [==============================] - 0s 144us/step - loss: 1.0081 - acc: 0.7704 - val_loss: 1.0224 - val_acc: 0.6000\n","Epoch 47/500\n","135/135 [==============================] - 0s 155us/step - loss: 1.0057 - acc: 0.7704 - val_loss: 1.0200 - val_acc: 0.6000\n","Epoch 48/500\n","135/135 [==============================] - 0s 142us/step - loss: 1.0031 - acc: 0.7704 - val_loss: 1.0176 - val_acc: 0.6000\n","Epoch 49/500\n","135/135 [==============================] - 0s 158us/step - loss: 1.0005 - acc: 0.7704 - val_loss: 1.0152 - val_acc: 0.6000\n","Epoch 50/500\n","135/135 [==============================] - 0s 166us/step - loss: 0.9980 - acc: 0.7778 - val_loss: 1.0127 - val_acc: 0.6000\n","Epoch 51/500\n","135/135 [==============================] - 0s 217us/step - loss: 0.9951 - acc: 0.7778 - val_loss: 1.0104 - val_acc: 0.6000\n","Epoch 52/500\n","135/135 [==============================] - 0s 197us/step - loss: 0.9924 - acc: 0.7926 - val_loss: 1.0079 - val_acc: 0.6000\n","Epoch 53/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.9897 - acc: 0.7926 - val_loss: 1.0053 - val_acc: 0.6000\n","Epoch 54/500\n","135/135 [==============================] - 0s 144us/step - loss: 0.9868 - acc: 0.8074 - val_loss: 1.0030 - val_acc: 0.6667\n","Epoch 55/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.9841 - acc: 0.7926 - val_loss: 1.0009 - val_acc: 0.6667\n","Epoch 56/500\n","135/135 [==============================] - 0s 142us/step - loss: 0.9811 - acc: 0.7926 - val_loss: 0.9984 - val_acc: 0.6667\n","Epoch 57/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.9782 - acc: 0.8000 - val_loss: 0.9957 - val_acc: 0.6667\n","Epoch 58/500\n","135/135 [==============================] - 0s 149us/step - loss: 0.9753 - acc: 0.8000 - val_loss: 0.9929 - val_acc: 0.6667\n","Epoch 59/500\n","135/135 [==============================] - 0s 161us/step - loss: 0.9724 - acc: 0.8000 - val_loss: 0.9900 - val_acc: 0.6667\n","Epoch 60/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.9695 - acc: 0.8074 - val_loss: 0.9869 - val_acc: 0.6667\n","Epoch 61/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.9664 - acc: 0.8148 - val_loss: 0.9838 - val_acc: 0.6667\n","Epoch 62/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.9634 - acc: 0.8519 - val_loss: 0.9806 - val_acc: 0.6667\n","Epoch 63/500\n","135/135 [==============================] - 0s 142us/step - loss: 0.9604 - acc: 0.8519 - val_loss: 0.9777 - val_acc: 0.7333\n","Epoch 64/500\n","135/135 [==============================] - 0s 155us/step - loss: 0.9575 - acc: 0.8519 - val_loss: 0.9750 - val_acc: 0.7333\n","Epoch 65/500\n","135/135 [==============================] - 0s 147us/step - loss: 0.9543 - acc: 0.8519 - val_loss: 0.9724 - val_acc: 0.8000\n","Epoch 66/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.9513 - acc: 0.8519 - val_loss: 0.9696 - val_acc: 0.8667\n","Epoch 67/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.9483 - acc: 0.8444 - val_loss: 0.9667 - val_acc: 0.8667\n","Epoch 68/500\n","135/135 [==============================] - 0s 151us/step - loss: 0.9452 - acc: 0.8444 - val_loss: 0.9637 - val_acc: 0.8667\n","Epoch 69/500\n","135/135 [==============================] - 0s 142us/step - loss: 0.9420 - acc: 0.8444 - val_loss: 0.9604 - val_acc: 0.8667\n","Epoch 70/500\n","135/135 [==============================] - 0s 146us/step - loss: 0.9387 - acc: 0.8593 - val_loss: 0.9570 - val_acc: 0.8667\n","Epoch 71/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.9355 - acc: 0.8667 - val_loss: 0.9536 - val_acc: 0.8667\n","Epoch 72/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.9322 - acc: 0.8667 - val_loss: 0.9501 - val_acc: 0.8667\n","Epoch 73/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.9289 - acc: 0.8667 - val_loss: 0.9469 - val_acc: 0.8667\n","Epoch 74/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.9256 - acc: 0.8667 - val_loss: 0.9433 - val_acc: 0.8667\n","Epoch 75/500\n","135/135 [==============================] - 0s 135us/step - loss: 0.9221 - acc: 0.8667 - val_loss: 0.9399 - val_acc: 0.8667\n","Epoch 76/500\n","135/135 [==============================] - 0s 171us/step - loss: 0.9188 - acc: 0.8667 - val_loss: 0.9367 - val_acc: 0.8667\n","Epoch 77/500\n","135/135 [==============================] - 0s 152us/step - loss: 0.9154 - acc: 0.8667 - val_loss: 0.9332 - val_acc: 0.8667\n","Epoch 78/500\n","135/135 [==============================] - 0s 147us/step - loss: 0.9121 - acc: 0.8667 - val_loss: 0.9296 - val_acc: 0.8667\n","Epoch 79/500\n","135/135 [==============================] - 0s 137us/step - loss: 0.9086 - acc: 0.8667 - val_loss: 0.9261 - val_acc: 0.8667\n","Epoch 80/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.9051 - acc: 0.8667 - val_loss: 0.9226 - val_acc: 0.8667\n","Epoch 81/500\n","135/135 [==============================] - 0s 143us/step - loss: 0.9017 - acc: 0.8667 - val_loss: 0.9189 - val_acc: 0.8667\n","Epoch 82/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.8982 - acc: 0.8667 - val_loss: 0.9154 - val_acc: 0.8667\n","Epoch 83/500\n","135/135 [==============================] - 0s 174us/step - loss: 0.8948 - acc: 0.8741 - val_loss: 0.9114 - val_acc: 0.8667\n","Epoch 84/500\n","135/135 [==============================] - 0s 146us/step - loss: 0.8914 - acc: 0.8815 - val_loss: 0.9074 - val_acc: 0.8667\n","Epoch 85/500\n","135/135 [==============================] - 0s 162us/step - loss: 0.8880 - acc: 0.8815 - val_loss: 0.9034 - val_acc: 0.8667\n","Epoch 86/500\n","135/135 [==============================] - 0s 147us/step - loss: 0.8845 - acc: 0.8815 - val_loss: 0.8998 - val_acc: 0.8667\n","Epoch 87/500\n","135/135 [==============================] - 0s 133us/step - loss: 0.8809 - acc: 0.8815 - val_loss: 0.8962 - val_acc: 0.8667\n","Epoch 88/500\n","135/135 [==============================] - 0s 146us/step - loss: 0.8777 - acc: 0.8889 - val_loss: 0.8924 - val_acc: 0.8667\n","Epoch 89/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.8740 - acc: 0.8889 - val_loss: 0.8888 - val_acc: 0.8667\n","Epoch 90/500\n","135/135 [==============================] - 0s 133us/step - loss: 0.8705 - acc: 0.8889 - val_loss: 0.8855 - val_acc: 0.8667\n","Epoch 91/500\n","135/135 [==============================] - 0s 134us/step - loss: 0.8669 - acc: 0.8889 - val_loss: 0.8818 - val_acc: 0.8667\n","Epoch 92/500\n","135/135 [==============================] - 0s 142us/step - loss: 0.8632 - acc: 0.8889 - val_loss: 0.8779 - val_acc: 0.8667\n","Epoch 93/500\n","135/135 [==============================] - 0s 149us/step - loss: 0.8597 - acc: 0.8815 - val_loss: 0.8740 - val_acc: 0.8667\n","Epoch 94/500\n","135/135 [==============================] - 0s 143us/step - loss: 0.8560 - acc: 0.8815 - val_loss: 0.8704 - val_acc: 0.8667\n","Epoch 95/500\n","135/135 [==============================] - 0s 156us/step - loss: 0.8524 - acc: 0.8815 - val_loss: 0.8666 - val_acc: 0.8667\n","Epoch 96/500\n","135/135 [==============================] - 0s 143us/step - loss: 0.8487 - acc: 0.8815 - val_loss: 0.8625 - val_acc: 0.8667\n","Epoch 97/500\n","135/135 [==============================] - 0s 169us/step - loss: 0.8450 - acc: 0.8815 - val_loss: 0.8588 - val_acc: 0.8667\n","Epoch 98/500\n","135/135 [==============================] - 0s 146us/step - loss: 0.8414 - acc: 0.8889 - val_loss: 0.8549 - val_acc: 0.8667\n","Epoch 99/500\n","135/135 [==============================] - 0s 128us/step - loss: 0.8378 - acc: 0.8963 - val_loss: 0.8511 - val_acc: 0.8667\n","Epoch 100/500\n","135/135 [==============================] - 0s 187us/step - loss: 0.8342 - acc: 0.8963 - val_loss: 0.8474 - val_acc: 0.8667\n","Epoch 101/500\n","135/135 [==============================] - 0s 244us/step - loss: 0.8306 - acc: 0.8963 - val_loss: 0.8436 - val_acc: 0.8667\n","Epoch 102/500\n","135/135 [==============================] - 0s 132us/step - loss: 0.8269 - acc: 0.8963 - val_loss: 0.8396 - val_acc: 0.8667\n","Epoch 103/500\n","135/135 [==============================] - 0s 138us/step - loss: 0.8231 - acc: 0.8889 - val_loss: 0.8357 - val_acc: 0.8667\n","Epoch 104/500\n","135/135 [==============================] - 0s 165us/step - loss: 0.8193 - acc: 0.8889 - val_loss: 0.8317 - val_acc: 0.8667\n","Epoch 105/500\n","135/135 [==============================] - 0s 151us/step - loss: 0.8157 - acc: 0.8889 - val_loss: 0.8275 - val_acc: 0.8667\n","Epoch 106/500\n","135/135 [==============================] - 0s 167us/step - loss: 0.8119 - acc: 0.8889 - val_loss: 0.8236 - val_acc: 0.8667\n","Epoch 107/500\n","135/135 [==============================] - 0s 150us/step - loss: 0.8082 - acc: 0.8889 - val_loss: 0.8200 - val_acc: 0.8667\n","Epoch 108/500\n","135/135 [==============================] - 0s 147us/step - loss: 0.8045 - acc: 0.8889 - val_loss: 0.8160 - val_acc: 0.8667\n","Epoch 109/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.8006 - acc: 0.8889 - val_loss: 0.8118 - val_acc: 0.8667\n","Epoch 110/500\n","135/135 [==============================] - 0s 146us/step - loss: 0.7968 - acc: 0.8963 - val_loss: 0.8076 - val_acc: 0.8667\n","Epoch 111/500\n","135/135 [==============================] - 0s 133us/step - loss: 0.7929 - acc: 0.8963 - val_loss: 0.8034 - val_acc: 0.8667\n","Epoch 112/500\n","135/135 [==============================] - 0s 131us/step - loss: 0.7891 - acc: 0.8963 - val_loss: 0.7992 - val_acc: 0.8667\n","Epoch 113/500\n","135/135 [==============================] - 0s 137us/step - loss: 0.7850 - acc: 0.8963 - val_loss: 0.7951 - val_acc: 0.8667\n","Epoch 114/500\n","135/135 [==============================] - 0s 152us/step - loss: 0.7810 - acc: 0.8963 - val_loss: 0.7914 - val_acc: 0.8667\n","Epoch 115/500\n","135/135 [==============================] - 0s 143us/step - loss: 0.7771 - acc: 0.8963 - val_loss: 0.7876 - val_acc: 0.8667\n","Epoch 116/500\n","135/135 [==============================] - 0s 142us/step - loss: 0.7731 - acc: 0.8963 - val_loss: 0.7837 - val_acc: 0.8667\n","Epoch 117/500\n","135/135 [==============================] - 0s 130us/step - loss: 0.7692 - acc: 0.8963 - val_loss: 0.7799 - val_acc: 0.8667\n","Epoch 118/500\n","135/135 [==============================] - 0s 167us/step - loss: 0.7653 - acc: 0.8963 - val_loss: 0.7757 - val_acc: 0.8667\n","Epoch 119/500\n","135/135 [==============================] - 0s 165us/step - loss: 0.7613 - acc: 0.8963 - val_loss: 0.7718 - val_acc: 0.8667\n","Epoch 120/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.7575 - acc: 0.8963 - val_loss: 0.7676 - val_acc: 0.8667\n","Epoch 121/500\n","135/135 [==============================] - 0s 154us/step - loss: 0.7536 - acc: 0.8963 - val_loss: 0.7635 - val_acc: 0.8667\n","Epoch 122/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.7496 - acc: 0.8963 - val_loss: 0.7599 - val_acc: 0.8667\n","Epoch 123/500\n","135/135 [==============================] - 0s 143us/step - loss: 0.7457 - acc: 0.8963 - val_loss: 0.7562 - val_acc: 0.8667\n","Epoch 124/500\n","135/135 [==============================] - 0s 138us/step - loss: 0.7419 - acc: 0.8963 - val_loss: 0.7525 - val_acc: 0.8667\n","Epoch 125/500\n","135/135 [==============================] - 0s 144us/step - loss: 0.7379 - acc: 0.8963 - val_loss: 0.7486 - val_acc: 0.8667\n","Epoch 126/500\n","135/135 [==============================] - 0s 139us/step - loss: 0.7340 - acc: 0.8963 - val_loss: 0.7448 - val_acc: 0.8667\n","Epoch 127/500\n","135/135 [==============================] - 0s 142us/step - loss: 0.7301 - acc: 0.8963 - val_loss: 0.7408 - val_acc: 0.8667\n","Epoch 128/500\n","135/135 [==============================] - 0s 135us/step - loss: 0.7262 - acc: 0.8963 - val_loss: 0.7371 - val_acc: 0.8667\n","Epoch 129/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.7222 - acc: 0.8963 - val_loss: 0.7333 - val_acc: 0.8667\n","Epoch 130/500\n","135/135 [==============================] - 0s 174us/step - loss: 0.7182 - acc: 0.8963 - val_loss: 0.7292 - val_acc: 0.8667\n","Epoch 131/500\n","135/135 [==============================] - 0s 139us/step - loss: 0.7143 - acc: 0.8963 - val_loss: 0.7252 - val_acc: 0.8667\n","Epoch 132/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.7103 - acc: 0.8963 - val_loss: 0.7211 - val_acc: 0.8667\n","Epoch 133/500\n","135/135 [==============================] - 0s 136us/step - loss: 0.7065 - acc: 0.8963 - val_loss: 0.7169 - val_acc: 0.8667\n","Epoch 134/500\n","135/135 [==============================] - 0s 163us/step - loss: 0.7026 - acc: 0.8963 - val_loss: 0.7130 - val_acc: 0.8667\n","Epoch 135/500\n","135/135 [==============================] - 0s 137us/step - loss: 0.6987 - acc: 0.8963 - val_loss: 0.7093 - val_acc: 0.8667\n","Epoch 136/500\n","135/135 [==============================] - 0s 138us/step - loss: 0.6947 - acc: 0.8963 - val_loss: 0.7056 - val_acc: 0.8667\n","Epoch 137/500\n","135/135 [==============================] - 0s 163us/step - loss: 0.6909 - acc: 0.8963 - val_loss: 0.7015 - val_acc: 0.8667\n","Epoch 138/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.6870 - acc: 0.8963 - val_loss: 0.6976 - val_acc: 0.8667\n","Epoch 139/500\n","135/135 [==============================] - 0s 143us/step - loss: 0.6832 - acc: 0.8963 - val_loss: 0.6937 - val_acc: 0.8667\n","Epoch 140/500\n","135/135 [==============================] - 0s 131us/step - loss: 0.6793 - acc: 0.9037 - val_loss: 0.6902 - val_acc: 0.8667\n","Epoch 141/500\n","135/135 [==============================] - 0s 159us/step - loss: 0.6756 - acc: 0.9037 - val_loss: 0.6868 - val_acc: 0.8667\n","Epoch 142/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.6718 - acc: 0.9037 - val_loss: 0.6834 - val_acc: 0.8667\n","Epoch 143/500\n","135/135 [==============================] - 0s 147us/step - loss: 0.6680 - acc: 0.9037 - val_loss: 0.6800 - val_acc: 0.8667\n","Epoch 144/500\n","135/135 [==============================] - 0s 151us/step - loss: 0.6642 - acc: 0.9037 - val_loss: 0.6765 - val_acc: 0.8667\n","Epoch 145/500\n","135/135 [==============================] - 0s 143us/step - loss: 0.6604 - acc: 0.9037 - val_loss: 0.6731 - val_acc: 0.8667\n","Epoch 146/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.6566 - acc: 0.9037 - val_loss: 0.6699 - val_acc: 0.8667\n","Epoch 147/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.6529 - acc: 0.9037 - val_loss: 0.6664 - val_acc: 0.8667\n","Epoch 148/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.6490 - acc: 0.9037 - val_loss: 0.6629 - val_acc: 0.8667\n","Epoch 149/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.6452 - acc: 0.9037 - val_loss: 0.6595 - val_acc: 0.8667\n","Epoch 150/500\n","135/135 [==============================] - 0s 203us/step - loss: 0.6414 - acc: 0.9037 - val_loss: 0.6560 - val_acc: 0.8667\n","Epoch 151/500\n","135/135 [==============================] - 0s 219us/step - loss: 0.6378 - acc: 0.9037 - val_loss: 0.6524 - val_acc: 0.8667\n","Epoch 152/500\n","135/135 [==============================] - 0s 151us/step - loss: 0.6340 - acc: 0.9037 - val_loss: 0.6489 - val_acc: 0.8667\n","Epoch 153/500\n","135/135 [==============================] - 0s 136us/step - loss: 0.6304 - acc: 0.9037 - val_loss: 0.6454 - val_acc: 0.8667\n","Epoch 154/500\n","135/135 [==============================] - 0s 169us/step - loss: 0.6267 - acc: 0.9037 - val_loss: 0.6423 - val_acc: 0.8667\n","Epoch 155/500\n","135/135 [==============================] - 0s 143us/step - loss: 0.6230 - acc: 0.9037 - val_loss: 0.6392 - val_acc: 0.8667\n","Epoch 156/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.6194 - acc: 0.9037 - val_loss: 0.6358 - val_acc: 0.8667\n","Epoch 157/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.6158 - acc: 0.9037 - val_loss: 0.6328 - val_acc: 0.8667\n","Epoch 158/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.6121 - acc: 0.9037 - val_loss: 0.6299 - val_acc: 0.8667\n","Epoch 159/500\n","135/135 [==============================] - 0s 157us/step - loss: 0.6085 - acc: 0.9037 - val_loss: 0.6273 - val_acc: 0.8667\n","Epoch 160/500\n","135/135 [==============================] - 0s 192us/step - loss: 0.6051 - acc: 0.9037 - val_loss: 0.6245 - val_acc: 0.8667\n","Epoch 161/500\n","135/135 [==============================] - 0s 136us/step - loss: 0.6017 - acc: 0.9037 - val_loss: 0.6216 - val_acc: 0.8667\n","Epoch 162/500\n","135/135 [==============================] - 0s 158us/step - loss: 0.5983 - acc: 0.9037 - val_loss: 0.6188 - val_acc: 0.8667\n","Epoch 163/500\n","135/135 [==============================] - 0s 146us/step - loss: 0.5948 - acc: 0.9037 - val_loss: 0.6159 - val_acc: 0.8667\n","Epoch 164/500\n","135/135 [==============================] - 0s 166us/step - loss: 0.5912 - acc: 0.9037 - val_loss: 0.6129 - val_acc: 0.8667\n","Epoch 165/500\n","135/135 [==============================] - 0s 182us/step - loss: 0.5879 - acc: 0.8963 - val_loss: 0.6098 - val_acc: 0.8667\n","Epoch 166/500\n","135/135 [==============================] - 0s 153us/step - loss: 0.5842 - acc: 0.8963 - val_loss: 0.6068 - val_acc: 0.8667\n","Epoch 167/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.5809 - acc: 0.8963 - val_loss: 0.6035 - val_acc: 0.8667\n","Epoch 168/500\n","135/135 [==============================] - 0s 163us/step - loss: 0.5773 - acc: 0.8963 - val_loss: 0.6004 - val_acc: 0.8667\n","Epoch 169/500\n","135/135 [==============================] - 0s 164us/step - loss: 0.5739 - acc: 0.8963 - val_loss: 0.5972 - val_acc: 0.8667\n","Epoch 170/500\n","135/135 [==============================] - 0s 161us/step - loss: 0.5704 - acc: 0.8963 - val_loss: 0.5941 - val_acc: 0.8667\n","Epoch 171/500\n","135/135 [==============================] - 0s 151us/step - loss: 0.5670 - acc: 0.8963 - val_loss: 0.5910 - val_acc: 0.8667\n","Epoch 172/500\n","135/135 [==============================] - 0s 142us/step - loss: 0.5637 - acc: 0.8963 - val_loss: 0.5880 - val_acc: 0.8667\n","Epoch 173/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.5601 - acc: 0.8963 - val_loss: 0.5853 - val_acc: 0.8667\n","Epoch 174/500\n","135/135 [==============================] - 0s 150us/step - loss: 0.5569 - acc: 0.8963 - val_loss: 0.5829 - val_acc: 0.8667\n","Epoch 175/500\n","135/135 [==============================] - 0s 157us/step - loss: 0.5537 - acc: 0.8963 - val_loss: 0.5803 - val_acc: 0.8667\n","Epoch 176/500\n","135/135 [==============================] - 0s 173us/step - loss: 0.5505 - acc: 0.8963 - val_loss: 0.5778 - val_acc: 0.8667\n","Epoch 177/500\n","135/135 [==============================] - 0s 152us/step - loss: 0.5473 - acc: 0.8963 - val_loss: 0.5753 - val_acc: 0.8667\n","Epoch 178/500\n","135/135 [==============================] - 0s 150us/step - loss: 0.5441 - acc: 0.8963 - val_loss: 0.5726 - val_acc: 0.8667\n","Epoch 179/500\n","135/135 [==============================] - 0s 151us/step - loss: 0.5409 - acc: 0.8963 - val_loss: 0.5700 - val_acc: 0.8667\n","Epoch 180/500\n","135/135 [==============================] - 0s 168us/step - loss: 0.5379 - acc: 0.8963 - val_loss: 0.5674 - val_acc: 0.8667\n","Epoch 181/500\n","135/135 [==============================] - 0s 142us/step - loss: 0.5348 - acc: 0.8963 - val_loss: 0.5647 - val_acc: 0.8667\n","Epoch 182/500\n","135/135 [==============================] - 0s 150us/step - loss: 0.5317 - acc: 0.8963 - val_loss: 0.5619 - val_acc: 0.8667\n","Epoch 183/500\n","135/135 [==============================] - 0s 156us/step - loss: 0.5286 - acc: 0.8963 - val_loss: 0.5592 - val_acc: 0.8667\n","Epoch 184/500\n","135/135 [==============================] - 0s 161us/step - loss: 0.5256 - acc: 0.8963 - val_loss: 0.5566 - val_acc: 0.8667\n","Epoch 185/500\n","135/135 [==============================] - 0s 155us/step - loss: 0.5227 - acc: 0.8963 - val_loss: 0.5538 - val_acc: 0.8667\n","Epoch 186/500\n","135/135 [==============================] - 0s 161us/step - loss: 0.5197 - acc: 0.8963 - val_loss: 0.5513 - val_acc: 0.8667\n","Epoch 187/500\n","135/135 [==============================] - 0s 149us/step - loss: 0.5168 - acc: 0.8963 - val_loss: 0.5489 - val_acc: 0.8667\n","Epoch 188/500\n","135/135 [==============================] - 0s 147us/step - loss: 0.5139 - acc: 0.8963 - val_loss: 0.5466 - val_acc: 0.8667\n","Epoch 189/500\n","135/135 [==============================] - 0s 165us/step - loss: 0.5112 - acc: 0.8963 - val_loss: 0.5445 - val_acc: 0.8667\n","Epoch 190/500\n","135/135 [==============================] - 0s 156us/step - loss: 0.5084 - acc: 0.8963 - val_loss: 0.5423 - val_acc: 0.8667\n","Epoch 191/500\n","135/135 [==============================] - 0s 193us/step - loss: 0.5056 - acc: 0.8963 - val_loss: 0.5399 - val_acc: 0.8667\n","Epoch 192/500\n","135/135 [==============================] - 0s 134us/step - loss: 0.5029 - acc: 0.8963 - val_loss: 0.5375 - val_acc: 0.8667\n","Epoch 193/500\n","135/135 [==============================] - 0s 152us/step - loss: 0.5002 - acc: 0.8963 - val_loss: 0.5352 - val_acc: 0.8667\n","Epoch 194/500\n","135/135 [==============================] - 0s 186us/step - loss: 0.4975 - acc: 0.8963 - val_loss: 0.5331 - val_acc: 0.8667\n","Epoch 195/500\n","135/135 [==============================] - 0s 161us/step - loss: 0.4947 - acc: 0.8963 - val_loss: 0.5310 - val_acc: 0.8667\n","Epoch 196/500\n","135/135 [==============================] - 0s 210us/step - loss: 0.4920 - acc: 0.8963 - val_loss: 0.5288 - val_acc: 0.8667\n","Epoch 197/500\n","135/135 [==============================] - 0s 184us/step - loss: 0.4893 - acc: 0.8963 - val_loss: 0.5265 - val_acc: 0.8667\n","Epoch 198/500\n","135/135 [==============================] - 0s 130us/step - loss: 0.4866 - acc: 0.8963 - val_loss: 0.5242 - val_acc: 0.8667\n","Epoch 199/500\n","135/135 [==============================] - 0s 147us/step - loss: 0.4839 - acc: 0.8963 - val_loss: 0.5219 - val_acc: 0.8667\n","Epoch 200/500\n","135/135 [==============================] - 0s 153us/step - loss: 0.4814 - acc: 0.9037 - val_loss: 0.5198 - val_acc: 0.8667\n","Epoch 201/500\n","135/135 [==============================] - 0s 159us/step - loss: 0.4788 - acc: 0.9037 - val_loss: 0.5179 - val_acc: 0.8667\n","Epoch 202/500\n","135/135 [==============================] - 0s 236us/step - loss: 0.4763 - acc: 0.9037 - val_loss: 0.5161 - val_acc: 0.8667\n","Epoch 203/500\n","135/135 [==============================] - 0s 163us/step - loss: 0.4737 - acc: 0.9037 - val_loss: 0.5142 - val_acc: 0.8667\n","Epoch 204/500\n","135/135 [==============================] - 0s 181us/step - loss: 0.4714 - acc: 0.9037 - val_loss: 0.5121 - val_acc: 0.8667\n","Epoch 205/500\n","135/135 [==============================] - 0s 125us/step - loss: 0.4689 - acc: 0.9037 - val_loss: 0.5102 - val_acc: 0.8667\n","Epoch 206/500\n","135/135 [==============================] - 0s 139us/step - loss: 0.4666 - acc: 0.9037 - val_loss: 0.5082 - val_acc: 0.8667\n","Epoch 207/500\n","135/135 [==============================] - 0s 146us/step - loss: 0.4643 - acc: 0.9037 - val_loss: 0.5064 - val_acc: 0.8667\n","Epoch 208/500\n","135/135 [==============================] - 0s 136us/step - loss: 0.4620 - acc: 0.9037 - val_loss: 0.5045 - val_acc: 0.8667\n","Epoch 209/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.4597 - acc: 0.9037 - val_loss: 0.5027 - val_acc: 0.8667\n","Epoch 210/500\n","135/135 [==============================] - 0s 129us/step - loss: 0.4576 - acc: 0.9037 - val_loss: 0.5008 - val_acc: 0.8667\n","Epoch 211/500\n","135/135 [==============================] - 0s 155us/step - loss: 0.4554 - acc: 0.9037 - val_loss: 0.4993 - val_acc: 0.8667\n","Epoch 212/500\n","135/135 [==============================] - 0s 133us/step - loss: 0.4533 - acc: 0.9037 - val_loss: 0.4978 - val_acc: 0.8667\n","Epoch 213/500\n","135/135 [==============================] - 0s 142us/step - loss: 0.4511 - acc: 0.9037 - val_loss: 0.4964 - val_acc: 0.8667\n","Epoch 214/500\n","135/135 [==============================] - 0s 139us/step - loss: 0.4490 - acc: 0.8963 - val_loss: 0.4949 - val_acc: 0.8667\n","Epoch 215/500\n","135/135 [==============================] - 0s 193us/step - loss: 0.4469 - acc: 0.8963 - val_loss: 0.4936 - val_acc: 0.8667\n","Epoch 216/500\n","135/135 [==============================] - 0s 161us/step - loss: 0.4448 - acc: 0.8963 - val_loss: 0.4920 - val_acc: 0.8667\n","Epoch 217/500\n","135/135 [==============================] - 0s 151us/step - loss: 0.4428 - acc: 0.8963 - val_loss: 0.4902 - val_acc: 0.8667\n","Epoch 218/500\n","135/135 [==============================] - 0s 165us/step - loss: 0.4406 - acc: 0.9037 - val_loss: 0.4887 - val_acc: 0.8667\n","Epoch 219/500\n","135/135 [==============================] - 0s 156us/step - loss: 0.4386 - acc: 0.9037 - val_loss: 0.4873 - val_acc: 0.8667\n","Epoch 220/500\n","135/135 [==============================] - 0s 179us/step - loss: 0.4366 - acc: 0.9037 - val_loss: 0.4859 - val_acc: 0.8667\n","Epoch 221/500\n","135/135 [==============================] - 0s 161us/step - loss: 0.4346 - acc: 0.9037 - val_loss: 0.4846 - val_acc: 0.8667\n","Epoch 222/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.4326 - acc: 0.8963 - val_loss: 0.4833 - val_acc: 0.8667\n","Epoch 223/500\n","135/135 [==============================] - 0s 142us/step - loss: 0.4307 - acc: 0.8963 - val_loss: 0.4820 - val_acc: 0.8667\n","Epoch 224/500\n","135/135 [==============================] - 0s 167us/step - loss: 0.4288 - acc: 0.8963 - val_loss: 0.4806 - val_acc: 0.8667\n","Epoch 225/500\n","135/135 [==============================] - 0s 164us/step - loss: 0.4270 - acc: 0.9037 - val_loss: 0.4792 - val_acc: 0.8667\n","Epoch 226/500\n","135/135 [==============================] - 0s 138us/step - loss: 0.4250 - acc: 0.9037 - val_loss: 0.4780 - val_acc: 0.8667\n","Epoch 227/500\n","135/135 [==============================] - 0s 199us/step - loss: 0.4233 - acc: 0.9037 - val_loss: 0.4767 - val_acc: 0.8667\n","Epoch 228/500\n","135/135 [==============================] - 0s 167us/step - loss: 0.4214 - acc: 0.9037 - val_loss: 0.4753 - val_acc: 0.8667\n","Epoch 229/500\n","135/135 [==============================] - 0s 159us/step - loss: 0.4196 - acc: 0.9037 - val_loss: 0.4740 - val_acc: 0.8667\n","Epoch 230/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.4177 - acc: 0.9037 - val_loss: 0.4726 - val_acc: 0.8667\n","Epoch 231/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.4159 - acc: 0.9111 - val_loss: 0.4712 - val_acc: 0.8667\n","Epoch 232/500\n","135/135 [==============================] - 0s 139us/step - loss: 0.4141 - acc: 0.9111 - val_loss: 0.4699 - val_acc: 0.8667\n","Epoch 233/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.4123 - acc: 0.9111 - val_loss: 0.4689 - val_acc: 0.8667\n","Epoch 234/500\n","135/135 [==============================] - 0s 166us/step - loss: 0.4107 - acc: 0.9111 - val_loss: 0.4680 - val_acc: 0.8667\n","Epoch 235/500\n","135/135 [==============================] - 0s 194us/step - loss: 0.4089 - acc: 0.9111 - val_loss: 0.4667 - val_acc: 0.8667\n","Epoch 236/500\n","135/135 [==============================] - 0s 177us/step - loss: 0.4073 - acc: 0.9111 - val_loss: 0.4653 - val_acc: 0.8667\n","Epoch 237/500\n","135/135 [==============================] - 0s 137us/step - loss: 0.4056 - acc: 0.9111 - val_loss: 0.4640 - val_acc: 0.8667\n","Epoch 238/500\n","135/135 [==============================] - 0s 169us/step - loss: 0.4040 - acc: 0.9111 - val_loss: 0.4629 - val_acc: 0.8667\n","Epoch 239/500\n","135/135 [==============================] - 0s 178us/step - loss: 0.4023 - acc: 0.9111 - val_loss: 0.4618 - val_acc: 0.8667\n","Epoch 240/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.4007 - acc: 0.9111 - val_loss: 0.4608 - val_acc: 0.8667\n","Epoch 241/500\n","135/135 [==============================] - 0s 139us/step - loss: 0.3991 - acc: 0.9111 - val_loss: 0.4598 - val_acc: 0.8667\n","Epoch 242/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.3975 - acc: 0.9111 - val_loss: 0.4587 - val_acc: 0.8667\n","Epoch 243/500\n","135/135 [==============================] - 0s 233us/step - loss: 0.3960 - acc: 0.9111 - val_loss: 0.4578 - val_acc: 0.8667\n","Epoch 244/500\n","135/135 [==============================] - 0s 192us/step - loss: 0.3946 - acc: 0.9111 - val_loss: 0.4572 - val_acc: 0.8667\n","Epoch 245/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.3930 - acc: 0.9111 - val_loss: 0.4565 - val_acc: 0.8667\n","Epoch 246/500\n","135/135 [==============================] - 0s 167us/step - loss: 0.3917 - acc: 0.9111 - val_loss: 0.4560 - val_acc: 0.8667\n","Epoch 247/500\n","135/135 [==============================] - 0s 144us/step - loss: 0.3902 - acc: 0.9111 - val_loss: 0.4554 - val_acc: 0.8667\n","Epoch 248/500\n","135/135 [==============================] - 0s 137us/step - loss: 0.3887 - acc: 0.9111 - val_loss: 0.4546 - val_acc: 0.8667\n","Epoch 249/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.3872 - acc: 0.9111 - val_loss: 0.4537 - val_acc: 0.8667\n","Epoch 250/500\n","135/135 [==============================] - 0s 143us/step - loss: 0.3857 - acc: 0.9111 - val_loss: 0.4529 - val_acc: 0.8667\n","Epoch 251/500\n","135/135 [==============================] - 0s 144us/step - loss: 0.3843 - acc: 0.9111 - val_loss: 0.4519 - val_acc: 0.8667\n","Epoch 252/500\n","135/135 [==============================] - 0s 139us/step - loss: 0.3829 - acc: 0.9111 - val_loss: 0.4506 - val_acc: 0.8667\n","Epoch 253/500\n","135/135 [==============================] - 0s 139us/step - loss: 0.3815 - acc: 0.9111 - val_loss: 0.4495 - val_acc: 0.8667\n","Epoch 254/500\n","135/135 [==============================] - 0s 161us/step - loss: 0.3801 - acc: 0.9111 - val_loss: 0.4484 - val_acc: 0.8667\n","Epoch 255/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.3787 - acc: 0.9111 - val_loss: 0.4475 - val_acc: 0.8667\n","Epoch 256/500\n","135/135 [==============================] - 0s 169us/step - loss: 0.3773 - acc: 0.9111 - val_loss: 0.4464 - val_acc: 0.8667\n","Epoch 257/500\n","135/135 [==============================] - 0s 180us/step - loss: 0.3760 - acc: 0.9111 - val_loss: 0.4454 - val_acc: 0.8667\n","Epoch 258/500\n","135/135 [==============================] - 0s 247us/step - loss: 0.3748 - acc: 0.9111 - val_loss: 0.4444 - val_acc: 0.8667\n","Epoch 259/500\n","135/135 [==============================] - 0s 190us/step - loss: 0.3735 - acc: 0.9111 - val_loss: 0.4436 - val_acc: 0.8667\n","Epoch 260/500\n","135/135 [==============================] - 0s 159us/step - loss: 0.3722 - acc: 0.9037 - val_loss: 0.4430 - val_acc: 0.8667\n","Epoch 261/500\n","135/135 [==============================] - 0s 164us/step - loss: 0.3710 - acc: 0.9037 - val_loss: 0.4421 - val_acc: 0.8667\n","Epoch 262/500\n","135/135 [==============================] - 0s 190us/step - loss: 0.3697 - acc: 0.9037 - val_loss: 0.4413 - val_acc: 0.8667\n","Epoch 263/500\n","135/135 [==============================] - 0s 198us/step - loss: 0.3686 - acc: 0.9037 - val_loss: 0.4404 - val_acc: 0.8667\n","Epoch 264/500\n","135/135 [==============================] - 0s 168us/step - loss: 0.3673 - acc: 0.9037 - val_loss: 0.4397 - val_acc: 0.8667\n","Epoch 265/500\n","135/135 [==============================] - 0s 154us/step - loss: 0.3661 - acc: 0.9037 - val_loss: 0.4389 - val_acc: 0.8667\n","Epoch 266/500\n","135/135 [==============================] - 0s 139us/step - loss: 0.3650 - acc: 0.9037 - val_loss: 0.4382 - val_acc: 0.8667\n","Epoch 267/500\n","135/135 [==============================] - 0s 143us/step - loss: 0.3638 - acc: 0.9111 - val_loss: 0.4374 - val_acc: 0.8667\n","Epoch 268/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.3627 - acc: 0.9111 - val_loss: 0.4368 - val_acc: 0.8667\n","Epoch 269/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.3616 - acc: 0.9111 - val_loss: 0.4360 - val_acc: 0.8667\n","Epoch 270/500\n","135/135 [==============================] - 0s 157us/step - loss: 0.3605 - acc: 0.9111 - val_loss: 0.4352 - val_acc: 0.8667\n","Epoch 271/500\n","135/135 [==============================] - 0s 182us/step - loss: 0.3594 - acc: 0.9111 - val_loss: 0.4346 - val_acc: 0.8667\n","Epoch 272/500\n","135/135 [==============================] - 0s 162us/step - loss: 0.3583 - acc: 0.9111 - val_loss: 0.4339 - val_acc: 0.8667\n","Epoch 273/500\n","135/135 [==============================] - 0s 157us/step - loss: 0.3573 - acc: 0.9111 - val_loss: 0.4335 - val_acc: 0.8667\n","Epoch 274/500\n","135/135 [==============================] - 0s 171us/step - loss: 0.3562 - acc: 0.9111 - val_loss: 0.4331 - val_acc: 0.8667\n","Epoch 275/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.3552 - acc: 0.9111 - val_loss: 0.4326 - val_acc: 0.8667\n","Epoch 276/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.3541 - acc: 0.9111 - val_loss: 0.4321 - val_acc: 0.8667\n","Epoch 277/500\n","135/135 [==============================] - 0s 154us/step - loss: 0.3531 - acc: 0.9111 - val_loss: 0.4316 - val_acc: 0.8667\n","Epoch 278/500\n","135/135 [==============================] - 0s 165us/step - loss: 0.3521 - acc: 0.9111 - val_loss: 0.4311 - val_acc: 0.8667\n","Epoch 279/500\n","135/135 [==============================] - 0s 151us/step - loss: 0.3511 - acc: 0.9111 - val_loss: 0.4307 - val_acc: 0.8667\n","Epoch 280/500\n","135/135 [==============================] - 0s 132us/step - loss: 0.3501 - acc: 0.9111 - val_loss: 0.4301 - val_acc: 0.8667\n","Epoch 281/500\n","135/135 [==============================] - 0s 151us/step - loss: 0.3491 - acc: 0.9111 - val_loss: 0.4296 - val_acc: 0.8667\n","Epoch 282/500\n","135/135 [==============================] - 0s 157us/step - loss: 0.3482 - acc: 0.9111 - val_loss: 0.4292 - val_acc: 0.8667\n","Epoch 283/500\n","135/135 [==============================] - 0s 155us/step - loss: 0.3472 - acc: 0.9111 - val_loss: 0.4287 - val_acc: 0.8667\n","Epoch 284/500\n","135/135 [==============================] - 0s 163us/step - loss: 0.3463 - acc: 0.9111 - val_loss: 0.4281 - val_acc: 0.8667\n","Epoch 285/500\n","135/135 [==============================] - 0s 169us/step - loss: 0.3454 - acc: 0.9111 - val_loss: 0.4276 - val_acc: 0.8667\n","Epoch 286/500\n","135/135 [==============================] - 0s 156us/step - loss: 0.3445 - acc: 0.9111 - val_loss: 0.4270 - val_acc: 0.8667\n","Epoch 287/500\n","135/135 [==============================] - 0s 183us/step - loss: 0.3436 - acc: 0.9111 - val_loss: 0.4265 - val_acc: 0.8667\n","Epoch 288/500\n","135/135 [==============================] - 0s 219us/step - loss: 0.3427 - acc: 0.9111 - val_loss: 0.4263 - val_acc: 0.8667\n","Epoch 289/500\n","135/135 [==============================] - 0s 217us/step - loss: 0.3419 - acc: 0.9111 - val_loss: 0.4261 - val_acc: 0.8667\n","Epoch 290/500\n","135/135 [==============================] - 0s 178us/step - loss: 0.3409 - acc: 0.9111 - val_loss: 0.4257 - val_acc: 0.8667\n","Epoch 291/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.3401 - acc: 0.9111 - val_loss: 0.4254 - val_acc: 0.8667\n","Epoch 292/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.3392 - acc: 0.9111 - val_loss: 0.4250 - val_acc: 0.8667\n","Epoch 293/500\n","135/135 [==============================] - 0s 169us/step - loss: 0.3383 - acc: 0.9111 - val_loss: 0.4244 - val_acc: 0.8667\n","Epoch 294/500\n","135/135 [==============================] - 0s 166us/step - loss: 0.3374 - acc: 0.9111 - val_loss: 0.4237 - val_acc: 0.8667\n","Epoch 295/500\n","135/135 [==============================] - 0s 160us/step - loss: 0.3366 - acc: 0.9111 - val_loss: 0.4231 - val_acc: 0.8667\n","Epoch 296/500\n","135/135 [==============================] - 0s 211us/step - loss: 0.3358 - acc: 0.9111 - val_loss: 0.4230 - val_acc: 0.8667\n","Epoch 297/500\n","135/135 [==============================] - 0s 212us/step - loss: 0.3349 - acc: 0.9111 - val_loss: 0.4227 - val_acc: 0.8667\n","Epoch 298/500\n","135/135 [==============================] - 0s 165us/step - loss: 0.3340 - acc: 0.9111 - val_loss: 0.4221 - val_acc: 0.8667\n","Epoch 299/500\n","135/135 [==============================] - 0s 135us/step - loss: 0.3333 - acc: 0.9111 - val_loss: 0.4218 - val_acc: 0.8667\n","Epoch 300/500\n","135/135 [==============================] - 0s 193us/step - loss: 0.3324 - acc: 0.9111 - val_loss: 0.4215 - val_acc: 0.8667\n","Epoch 301/500\n","135/135 [==============================] - 0s 149us/step - loss: 0.3317 - acc: 0.9111 - val_loss: 0.4212 - val_acc: 0.8667\n","Epoch 302/500\n","135/135 [==============================] - 0s 164us/step - loss: 0.3308 - acc: 0.9111 - val_loss: 0.4206 - val_acc: 0.8667\n","Epoch 303/500\n","135/135 [==============================] - 0s 173us/step - loss: 0.3300 - acc: 0.9111 - val_loss: 0.4202 - val_acc: 0.8667\n","Epoch 304/500\n","135/135 [==============================] - 0s 172us/step - loss: 0.3292 - acc: 0.9111 - val_loss: 0.4197 - val_acc: 0.8667\n","Epoch 305/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.3285 - acc: 0.9111 - val_loss: 0.4193 - val_acc: 0.8667\n","Epoch 306/500\n","135/135 [==============================] - 0s 163us/step - loss: 0.3278 - acc: 0.9111 - val_loss: 0.4191 - val_acc: 0.8667\n","Epoch 307/500\n","135/135 [==============================] - 0s 146us/step - loss: 0.3270 - acc: 0.9111 - val_loss: 0.4186 - val_acc: 0.8667\n","Epoch 308/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.3262 - acc: 0.9111 - val_loss: 0.4179 - val_acc: 0.8667\n","Epoch 309/500\n","135/135 [==============================] - 0s 131us/step - loss: 0.3255 - acc: 0.9111 - val_loss: 0.4174 - val_acc: 0.8667\n","Epoch 310/500\n","135/135 [==============================] - 0s 159us/step - loss: 0.3247 - acc: 0.9111 - val_loss: 0.4169 - val_acc: 0.8667\n","Epoch 311/500\n","135/135 [==============================] - 0s 166us/step - loss: 0.3240 - acc: 0.9111 - val_loss: 0.4163 - val_acc: 0.8667\n","Epoch 312/500\n","135/135 [==============================] - 0s 168us/step - loss: 0.3233 - acc: 0.9111 - val_loss: 0.4160 - val_acc: 0.8667\n","Epoch 313/500\n","135/135 [==============================] - 0s 162us/step - loss: 0.3227 - acc: 0.9111 - val_loss: 0.4156 - val_acc: 0.8667\n","Epoch 314/500\n","135/135 [==============================] - 0s 171us/step - loss: 0.3221 - acc: 0.9111 - val_loss: 0.4154 - val_acc: 0.8667\n","Epoch 315/500\n","135/135 [==============================] - 0s 163us/step - loss: 0.3214 - acc: 0.9111 - val_loss: 0.4152 - val_acc: 0.8667\n","Epoch 316/500\n","135/135 [==============================] - 0s 161us/step - loss: 0.3208 - acc: 0.9111 - val_loss: 0.4148 - val_acc: 0.8667\n","Epoch 317/500\n","135/135 [==============================] - 0s 144us/step - loss: 0.3201 - acc: 0.9111 - val_loss: 0.4143 - val_acc: 0.8667\n","Epoch 318/500\n","135/135 [==============================] - 0s 159us/step - loss: 0.3193 - acc: 0.9111 - val_loss: 0.4137 - val_acc: 0.8667\n","Epoch 319/500\n","135/135 [==============================] - 0s 143us/step - loss: 0.3186 - acc: 0.9111 - val_loss: 0.4134 - val_acc: 0.8667\n","Epoch 320/500\n","135/135 [==============================] - 0s 166us/step - loss: 0.3179 - acc: 0.9111 - val_loss: 0.4132 - val_acc: 0.8667\n","Epoch 321/500\n","135/135 [==============================] - 0s 163us/step - loss: 0.3173 - acc: 0.9111 - val_loss: 0.4131 - val_acc: 0.8667\n","Epoch 322/500\n","135/135 [==============================] - 0s 170us/step - loss: 0.3166 - acc: 0.9111 - val_loss: 0.4128 - val_acc: 0.8667\n","Epoch 323/500\n","135/135 [==============================] - 0s 169us/step - loss: 0.3160 - acc: 0.9111 - val_loss: 0.4125 - val_acc: 0.8667\n","Epoch 324/500\n","135/135 [==============================] - 0s 152us/step - loss: 0.3153 - acc: 0.9111 - val_loss: 0.4122 - val_acc: 0.8667\n","Epoch 325/500\n","135/135 [==============================] - 0s 188us/step - loss: 0.3147 - acc: 0.9111 - val_loss: 0.4118 - val_acc: 0.8667\n","Epoch 326/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.3141 - acc: 0.9111 - val_loss: 0.4113 - val_acc: 0.8667\n","Epoch 327/500\n","135/135 [==============================] - 0s 133us/step - loss: 0.3135 - acc: 0.9111 - val_loss: 0.4108 - val_acc: 0.8667\n","Epoch 328/500\n","135/135 [==============================] - 0s 154us/step - loss: 0.3129 - acc: 0.9111 - val_loss: 0.4103 - val_acc: 0.8667\n","Epoch 329/500\n","135/135 [==============================] - 0s 138us/step - loss: 0.3123 - acc: 0.9111 - val_loss: 0.4097 - val_acc: 0.8667\n","Epoch 330/500\n","135/135 [==============================] - 0s 180us/step - loss: 0.3117 - acc: 0.9111 - val_loss: 0.4094 - val_acc: 0.8667\n","Epoch 331/500\n","135/135 [==============================] - 0s 158us/step - loss: 0.3111 - acc: 0.9111 - val_loss: 0.4093 - val_acc: 0.8667\n","Epoch 332/500\n","135/135 [==============================] - 0s 152us/step - loss: 0.3105 - acc: 0.9111 - val_loss: 0.4092 - val_acc: 0.8667\n","Epoch 333/500\n","135/135 [==============================] - 0s 246us/step - loss: 0.3100 - acc: 0.9111 - val_loss: 0.4090 - val_acc: 0.8667\n","Epoch 334/500\n","135/135 [==============================] - 0s 206us/step - loss: 0.3094 - acc: 0.9111 - val_loss: 0.4090 - val_acc: 0.8667\n","Epoch 335/500\n","135/135 [==============================] - 0s 160us/step - loss: 0.3088 - acc: 0.9111 - val_loss: 0.4089 - val_acc: 0.8667\n","Epoch 336/500\n","135/135 [==============================] - 0s 161us/step - loss: 0.3083 - acc: 0.9111 - val_loss: 0.4087 - val_acc: 0.8667\n","Epoch 337/500\n","135/135 [==============================] - 0s 156us/step - loss: 0.3077 - acc: 0.9111 - val_loss: 0.4085 - val_acc: 0.8667\n","Epoch 338/500\n","135/135 [==============================] - 0s 136us/step - loss: 0.3072 - acc: 0.9111 - val_loss: 0.4085 - val_acc: 0.8667\n","Epoch 339/500\n","135/135 [==============================] - 0s 165us/step - loss: 0.3068 - acc: 0.9111 - val_loss: 0.4083 - val_acc: 0.8667\n","Epoch 340/500\n","135/135 [==============================] - 0s 156us/step - loss: 0.3062 - acc: 0.9111 - val_loss: 0.4081 - val_acc: 0.8667\n","Epoch 341/500\n","135/135 [==============================] - 0s 166us/step - loss: 0.3056 - acc: 0.9111 - val_loss: 0.4077 - val_acc: 0.8667\n","Epoch 342/500\n","135/135 [==============================] - 0s 138us/step - loss: 0.3051 - acc: 0.9111 - val_loss: 0.4074 - val_acc: 0.8667\n","Epoch 343/500\n","135/135 [==============================] - 0s 154us/step - loss: 0.3046 - acc: 0.9111 - val_loss: 0.4073 - val_acc: 0.8667\n","Epoch 344/500\n","135/135 [==============================] - 0s 165us/step - loss: 0.3040 - acc: 0.9111 - val_loss: 0.4070 - val_acc: 0.8667\n","Epoch 345/500\n","135/135 [==============================] - 0s 186us/step - loss: 0.3036 - acc: 0.9111 - val_loss: 0.4069 - val_acc: 0.8667\n","Epoch 346/500\n","135/135 [==============================] - 0s 158us/step - loss: 0.3030 - acc: 0.9111 - val_loss: 0.4066 - val_acc: 0.8667\n","Epoch 347/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.3025 - acc: 0.9111 - val_loss: 0.4065 - val_acc: 0.8667\n","Epoch 348/500\n","135/135 [==============================] - 0s 164us/step - loss: 0.3020 - acc: 0.9111 - val_loss: 0.4061 - val_acc: 0.8667\n","Epoch 349/500\n","135/135 [==============================] - 0s 159us/step - loss: 0.3014 - acc: 0.9111 - val_loss: 0.4059 - val_acc: 0.8667\n","Epoch 350/500\n","135/135 [==============================] - 0s 154us/step - loss: 0.3009 - acc: 0.9111 - val_loss: 0.4058 - val_acc: 0.8667\n","Epoch 351/500\n","135/135 [==============================] - 0s 131us/step - loss: 0.3004 - acc: 0.9111 - val_loss: 0.4058 - val_acc: 0.8667\n","Epoch 352/500\n","135/135 [==============================] - 0s 142us/step - loss: 0.2999 - acc: 0.9111 - val_loss: 0.4057 - val_acc: 0.8667\n","Epoch 353/500\n","135/135 [==============================] - 0s 155us/step - loss: 0.2993 - acc: 0.9111 - val_loss: 0.4056 - val_acc: 0.8667\n","Epoch 354/500\n","135/135 [==============================] - 0s 157us/step - loss: 0.2989 - acc: 0.9111 - val_loss: 0.4054 - val_acc: 0.8667\n","Epoch 355/500\n","135/135 [==============================] - 0s 196us/step - loss: 0.2985 - acc: 0.9111 - val_loss: 0.4051 - val_acc: 0.8667\n","Epoch 356/500\n","135/135 [==============================] - 0s 172us/step - loss: 0.2980 - acc: 0.9111 - val_loss: 0.4050 - val_acc: 0.8667\n","Epoch 357/500\n","135/135 [==============================] - 0s 167us/step - loss: 0.2975 - acc: 0.9111 - val_loss: 0.4048 - val_acc: 0.8667\n","Epoch 358/500\n","135/135 [==============================] - 0s 169us/step - loss: 0.2970 - acc: 0.9111 - val_loss: 0.4045 - val_acc: 0.8667\n","Epoch 359/500\n","135/135 [==============================] - 0s 146us/step - loss: 0.2966 - acc: 0.9111 - val_loss: 0.4042 - val_acc: 0.8667\n","Epoch 360/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.2961 - acc: 0.9111 - val_loss: 0.4041 - val_acc: 0.8667\n","Epoch 361/500\n","135/135 [==============================] - 0s 147us/step - loss: 0.2957 - acc: 0.9111 - val_loss: 0.4041 - val_acc: 0.8667\n","Epoch 362/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.2952 - acc: 0.9111 - val_loss: 0.4040 - val_acc: 0.8667\n","Epoch 363/500\n","135/135 [==============================] - 0s 153us/step - loss: 0.2948 - acc: 0.9111 - val_loss: 0.4038 - val_acc: 0.8667\n","Epoch 364/500\n","135/135 [==============================] - 0s 174us/step - loss: 0.2944 - acc: 0.9111 - val_loss: 0.4036 - val_acc: 0.8667\n","Epoch 365/500\n","135/135 [==============================] - 0s 143us/step - loss: 0.2940 - acc: 0.9111 - val_loss: 0.4034 - val_acc: 0.8667\n","Epoch 366/500\n","135/135 [==============================] - 0s 158us/step - loss: 0.2935 - acc: 0.9111 - val_loss: 0.4031 - val_acc: 0.8667\n","Epoch 367/500\n","135/135 [==============================] - 0s 170us/step - loss: 0.2932 - acc: 0.9111 - val_loss: 0.4026 - val_acc: 0.8667\n","Epoch 368/500\n","135/135 [==============================] - 0s 162us/step - loss: 0.2927 - acc: 0.9111 - val_loss: 0.4023 - val_acc: 0.8667\n","Epoch 369/500\n","135/135 [==============================] - 0s 154us/step - loss: 0.2923 - acc: 0.9111 - val_loss: 0.4023 - val_acc: 0.8667\n","Epoch 370/500\n","135/135 [==============================] - 0s 146us/step - loss: 0.2919 - acc: 0.9111 - val_loss: 0.4023 - val_acc: 0.8667\n","Epoch 371/500\n","135/135 [==============================] - 0s 160us/step - loss: 0.2915 - acc: 0.9111 - val_loss: 0.4023 - val_acc: 0.8667\n","Epoch 372/500\n","135/135 [==============================] - 0s 133us/step - loss: 0.2911 - acc: 0.9111 - val_loss: 0.4022 - val_acc: 0.8667\n","Epoch 373/500\n","135/135 [==============================] - 0s 134us/step - loss: 0.2907 - acc: 0.9111 - val_loss: 0.4019 - val_acc: 0.8667\n","Epoch 374/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.2904 - acc: 0.9111 - val_loss: 0.4015 - val_acc: 0.8667\n","Epoch 375/500\n","135/135 [==============================] - 0s 169us/step - loss: 0.2900 - acc: 0.9111 - val_loss: 0.4013 - val_acc: 0.8667\n","Epoch 376/500\n","135/135 [==============================] - 0s 173us/step - loss: 0.2896 - acc: 0.9111 - val_loss: 0.4012 - val_acc: 0.8667\n","Epoch 377/500\n","135/135 [==============================] - 0s 147us/step - loss: 0.2892 - acc: 0.9111 - val_loss: 0.4009 - val_acc: 0.8667\n","Epoch 378/500\n","135/135 [==============================] - 0s 152us/step - loss: 0.2887 - acc: 0.9111 - val_loss: 0.4002 - val_acc: 0.8667\n","Epoch 379/500\n","135/135 [==============================] - 0s 195us/step - loss: 0.2884 - acc: 0.9111 - val_loss: 0.3994 - val_acc: 0.8667\n","Epoch 380/500\n","135/135 [==============================] - 0s 259us/step - loss: 0.2880 - acc: 0.9037 - val_loss: 0.3990 - val_acc: 0.8667\n","Epoch 381/500\n","135/135 [==============================] - 0s 164us/step - loss: 0.2877 - acc: 0.9037 - val_loss: 0.3985 - val_acc: 0.8667\n","Epoch 382/500\n","135/135 [==============================] - 0s 184us/step - loss: 0.2872 - acc: 0.9037 - val_loss: 0.3982 - val_acc: 0.8667\n","Epoch 383/500\n","135/135 [==============================] - 0s 163us/step - loss: 0.2868 - acc: 0.9037 - val_loss: 0.3980 - val_acc: 0.8667\n","Epoch 384/500\n","135/135 [==============================] - 0s 162us/step - loss: 0.2865 - acc: 0.9037 - val_loss: 0.3978 - val_acc: 0.8667\n","Epoch 385/500\n","135/135 [==============================] - 0s 166us/step - loss: 0.2861 - acc: 0.9037 - val_loss: 0.3970 - val_acc: 0.8667\n","Epoch 386/500\n","135/135 [==============================] - 0s 177us/step - loss: 0.2857 - acc: 0.9037 - val_loss: 0.3965 - val_acc: 0.8667\n","Epoch 387/500\n","135/135 [==============================] - 0s 150us/step - loss: 0.2854 - acc: 0.9037 - val_loss: 0.3961 - val_acc: 0.8667\n","Epoch 388/500\n","135/135 [==============================] - 0s 163us/step - loss: 0.2850 - acc: 0.9037 - val_loss: 0.3958 - val_acc: 0.8667\n","Epoch 389/500\n","135/135 [==============================] - 0s 167us/step - loss: 0.2846 - acc: 0.9037 - val_loss: 0.3955 - val_acc: 0.8667\n","Epoch 390/500\n","135/135 [==============================] - 0s 133us/step - loss: 0.2843 - acc: 0.9037 - val_loss: 0.3952 - val_acc: 0.8667\n","Epoch 391/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.2839 - acc: 0.9037 - val_loss: 0.3949 - val_acc: 0.8667\n","Epoch 392/500\n","135/135 [==============================] - 0s 149us/step - loss: 0.2835 - acc: 0.9037 - val_loss: 0.3945 - val_acc: 0.8667\n","Epoch 393/500\n","135/135 [==============================] - 0s 171us/step - loss: 0.2832 - acc: 0.9037 - val_loss: 0.3944 - val_acc: 0.8667\n","Epoch 394/500\n","135/135 [==============================] - 0s 142us/step - loss: 0.2828 - acc: 0.9037 - val_loss: 0.3943 - val_acc: 0.8667\n","Epoch 395/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.2824 - acc: 0.9037 - val_loss: 0.3942 - val_acc: 0.8667\n","Epoch 396/500\n","135/135 [==============================] - 0s 153us/step - loss: 0.2821 - acc: 0.9037 - val_loss: 0.3944 - val_acc: 0.8667\n","Epoch 397/500\n","135/135 [==============================] - 0s 161us/step - loss: 0.2817 - acc: 0.9037 - val_loss: 0.3945 - val_acc: 0.8667\n","Epoch 398/500\n","135/135 [==============================] - 0s 156us/step - loss: 0.2814 - acc: 0.9037 - val_loss: 0.3944 - val_acc: 0.8667\n","Epoch 399/500\n","135/135 [==============================] - 0s 143us/step - loss: 0.2810 - acc: 0.9037 - val_loss: 0.3943 - val_acc: 0.8667\n","Epoch 400/500\n","135/135 [==============================] - 0s 159us/step - loss: 0.2807 - acc: 0.9037 - val_loss: 0.3940 - val_acc: 0.8667\n","Epoch 401/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.2803 - acc: 0.9037 - val_loss: 0.3940 - val_acc: 0.8667\n","Epoch 402/500\n","135/135 [==============================] - 0s 156us/step - loss: 0.2800 - acc: 0.9037 - val_loss: 0.3940 - val_acc: 0.8667\n","Epoch 403/500\n","135/135 [==============================] - 0s 142us/step - loss: 0.2798 - acc: 0.9037 - val_loss: 0.3942 - val_acc: 0.8667\n","Epoch 404/500\n","135/135 [==============================] - 0s 124us/step - loss: 0.2794 - acc: 0.9037 - val_loss: 0.3941 - val_acc: 0.8667\n","Epoch 405/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.2790 - acc: 0.9037 - val_loss: 0.3939 - val_acc: 0.8667\n","Epoch 406/500\n","135/135 [==============================] - 0s 154us/step - loss: 0.2787 - acc: 0.9037 - val_loss: 0.3935 - val_acc: 0.8667\n","Epoch 407/500\n","135/135 [==============================] - 0s 160us/step - loss: 0.2784 - acc: 0.9037 - val_loss: 0.3934 - val_acc: 0.8667\n","Epoch 408/500\n","135/135 [==============================] - 0s 152us/step - loss: 0.2780 - acc: 0.9037 - val_loss: 0.3936 - val_acc: 0.8667\n","Epoch 409/500\n","135/135 [==============================] - 0s 151us/step - loss: 0.2777 - acc: 0.9037 - val_loss: 0.3938 - val_acc: 0.8667\n","Epoch 410/500\n","135/135 [==============================] - 0s 138us/step - loss: 0.2774 - acc: 0.9037 - val_loss: 0.3937 - val_acc: 0.8667\n","Epoch 411/500\n","135/135 [==============================] - 0s 164us/step - loss: 0.2771 - acc: 0.9037 - val_loss: 0.3937 - val_acc: 0.8667\n","Epoch 412/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.2767 - acc: 0.9037 - val_loss: 0.3937 - val_acc: 0.8667\n","Epoch 413/500\n","135/135 [==============================] - 0s 137us/step - loss: 0.2764 - acc: 0.9037 - val_loss: 0.3939 - val_acc: 0.8667\n","Epoch 414/500\n","135/135 [==============================] - 0s 152us/step - loss: 0.2761 - acc: 0.9037 - val_loss: 0.3938 - val_acc: 0.8667\n","Epoch 415/500\n","135/135 [==============================] - 0s 160us/step - loss: 0.2758 - acc: 0.9037 - val_loss: 0.3937 - val_acc: 0.8667\n","Epoch 416/500\n","135/135 [==============================] - 0s 146us/step - loss: 0.2755 - acc: 0.9037 - val_loss: 0.3932 - val_acc: 0.8667\n","Epoch 417/500\n","135/135 [==============================] - 0s 170us/step - loss: 0.2752 - acc: 0.9037 - val_loss: 0.3930 - val_acc: 0.8667\n","Epoch 418/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.2749 - acc: 0.9037 - val_loss: 0.3928 - val_acc: 0.8667\n","Epoch 419/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.2746 - acc: 0.9037 - val_loss: 0.3926 - val_acc: 0.8667\n","Epoch 420/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.2743 - acc: 0.9037 - val_loss: 0.3922 - val_acc: 0.8667\n","Epoch 421/500\n","135/135 [==============================] - 0s 144us/step - loss: 0.2740 - acc: 0.9037 - val_loss: 0.3921 - val_acc: 0.8667\n","Epoch 422/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.2737 - acc: 0.9037 - val_loss: 0.3919 - val_acc: 0.8667\n","Epoch 423/500\n","135/135 [==============================] - 0s 137us/step - loss: 0.2735 - acc: 0.9037 - val_loss: 0.3918 - val_acc: 0.8667\n","Epoch 424/500\n","135/135 [==============================] - 0s 131us/step - loss: 0.2732 - acc: 0.9037 - val_loss: 0.3916 - val_acc: 0.8667\n","Epoch 425/500\n","135/135 [==============================] - 0s 131us/step - loss: 0.2729 - acc: 0.9037 - val_loss: 0.3915 - val_acc: 0.8667\n","Epoch 426/500\n","135/135 [==============================] - 0s 143us/step - loss: 0.2726 - acc: 0.9037 - val_loss: 0.3913 - val_acc: 0.9333\n","Epoch 427/500\n","135/135 [==============================] - 0s 219us/step - loss: 0.2723 - acc: 0.9037 - val_loss: 0.3912 - val_acc: 0.9333\n","Epoch 428/500\n","135/135 [==============================] - 0s 166us/step - loss: 0.2720 - acc: 0.9037 - val_loss: 0.3914 - val_acc: 0.9333\n","Epoch 429/500\n","135/135 [==============================] - 0s 160us/step - loss: 0.2717 - acc: 0.9037 - val_loss: 0.3915 - val_acc: 0.9333\n","Epoch 430/500\n","135/135 [==============================] - 0s 136us/step - loss: 0.2714 - acc: 0.9037 - val_loss: 0.3916 - val_acc: 0.9333\n","Epoch 431/500\n","135/135 [==============================] - 0s 132us/step - loss: 0.2712 - acc: 0.9037 - val_loss: 0.3918 - val_acc: 0.9333\n","Epoch 432/500\n","135/135 [==============================] - 0s 150us/step - loss: 0.2709 - acc: 0.9037 - val_loss: 0.3918 - val_acc: 0.9333\n","Epoch 433/500\n","135/135 [==============================] - 0s 161us/step - loss: 0.2706 - acc: 0.9037 - val_loss: 0.3916 - val_acc: 0.9333\n","Epoch 434/500\n","135/135 [==============================] - 0s 160us/step - loss: 0.2704 - acc: 0.9037 - val_loss: 0.3916 - val_acc: 0.9333\n","Epoch 435/500\n","135/135 [==============================] - 0s 143us/step - loss: 0.2701 - acc: 0.9037 - val_loss: 0.3922 - val_acc: 0.9333\n","Epoch 436/500\n","135/135 [==============================] - 0s 191us/step - loss: 0.2699 - acc: 0.9037 - val_loss: 0.3929 - val_acc: 0.9333\n","Epoch 437/500\n","135/135 [==============================] - 0s 157us/step - loss: 0.2696 - acc: 0.9037 - val_loss: 0.3932 - val_acc: 0.9333\n","Epoch 438/500\n","135/135 [==============================] - 0s 156us/step - loss: 0.2694 - acc: 0.9037 - val_loss: 0.3935 - val_acc: 0.9333\n","Epoch 439/500\n","135/135 [==============================] - 0s 155us/step - loss: 0.2691 - acc: 0.9037 - val_loss: 0.3934 - val_acc: 0.9333\n","Epoch 440/500\n","135/135 [==============================] - 0s 188us/step - loss: 0.2688 - acc: 0.9037 - val_loss: 0.3932 - val_acc: 0.9333\n","Epoch 441/500\n","135/135 [==============================] - 0s 129us/step - loss: 0.2686 - acc: 0.9037 - val_loss: 0.3928 - val_acc: 0.9333\n","Epoch 442/500\n","135/135 [==============================] - 0s 139us/step - loss: 0.2683 - acc: 0.9037 - val_loss: 0.3926 - val_acc: 0.9333\n","Epoch 443/500\n","135/135 [==============================] - 0s 130us/step - loss: 0.2680 - acc: 0.9037 - val_loss: 0.3928 - val_acc: 0.9333\n","Epoch 444/500\n","135/135 [==============================] - 0s 159us/step - loss: 0.2678 - acc: 0.9037 - val_loss: 0.3930 - val_acc: 0.9333\n","Epoch 445/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.2676 - acc: 0.9037 - val_loss: 0.3929 - val_acc: 0.9333\n","Epoch 446/500\n","135/135 [==============================] - 0s 130us/step - loss: 0.2673 - acc: 0.9037 - val_loss: 0.3929 - val_acc: 0.9333\n","Epoch 447/500\n","135/135 [==============================] - 0s 127us/step - loss: 0.2671 - acc: 0.9037 - val_loss: 0.3926 - val_acc: 0.9333\n","Epoch 448/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.2668 - acc: 0.9037 - val_loss: 0.3924 - val_acc: 0.9333\n","Epoch 449/500\n","135/135 [==============================] - 0s 157us/step - loss: 0.2665 - acc: 0.9037 - val_loss: 0.3921 - val_acc: 0.9333\n","Epoch 450/500\n","135/135 [==============================] - 0s 138us/step - loss: 0.2663 - acc: 0.9037 - val_loss: 0.3918 - val_acc: 0.9333\n","Epoch 451/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.2661 - acc: 0.9037 - val_loss: 0.3917 - val_acc: 0.9333\n","Epoch 452/500\n","135/135 [==============================] - 0s 128us/step - loss: 0.2658 - acc: 0.9037 - val_loss: 0.3915 - val_acc: 0.9333\n","Epoch 453/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.2655 - acc: 0.9037 - val_loss: 0.3915 - val_acc: 0.9333\n","Epoch 454/500\n","135/135 [==============================] - 0s 146us/step - loss: 0.2653 - acc: 0.9037 - val_loss: 0.3913 - val_acc: 0.9333\n","Epoch 455/500\n","135/135 [==============================] - 0s 178us/step - loss: 0.2651 - acc: 0.9037 - val_loss: 0.3912 - val_acc: 0.9333\n","Epoch 456/500\n","135/135 [==============================] - 0s 138us/step - loss: 0.2649 - acc: 0.9037 - val_loss: 0.3910 - val_acc: 0.9333\n","Epoch 457/500\n","135/135 [==============================] - 0s 144us/step - loss: 0.2646 - acc: 0.9037 - val_loss: 0.3907 - val_acc: 0.9333\n","Epoch 458/500\n","135/135 [==============================] - 0s 139us/step - loss: 0.2644 - acc: 0.9037 - val_loss: 0.3904 - val_acc: 0.9333\n","Epoch 459/500\n","135/135 [==============================] - 0s 159us/step - loss: 0.2642 - acc: 0.9037 - val_loss: 0.3900 - val_acc: 0.9333\n","Epoch 460/500\n","135/135 [==============================] - 0s 128us/step - loss: 0.2640 - acc: 0.9037 - val_loss: 0.3899 - val_acc: 0.9333\n","Epoch 461/500\n","135/135 [==============================] - 0s 127us/step - loss: 0.2637 - acc: 0.9037 - val_loss: 0.3897 - val_acc: 0.9333\n","Epoch 462/500\n","135/135 [==============================] - 0s 146us/step - loss: 0.2635 - acc: 0.9037 - val_loss: 0.3897 - val_acc: 0.9333\n","Epoch 463/500\n","135/135 [==============================] - 0s 127us/step - loss: 0.2632 - acc: 0.9037 - val_loss: 0.3899 - val_acc: 0.9333\n","Epoch 464/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.2632 - acc: 0.9037 - val_loss: 0.3902 - val_acc: 0.9333\n","Epoch 465/500\n","135/135 [==============================] - 0s 160us/step - loss: 0.2629 - acc: 0.9037 - val_loss: 0.3905 - val_acc: 0.9333\n","Epoch 466/500\n","135/135 [==============================] - 0s 163us/step - loss: 0.2627 - acc: 0.9037 - val_loss: 0.3905 - val_acc: 0.9333\n","Epoch 467/500\n","135/135 [==============================] - 0s 133us/step - loss: 0.2625 - acc: 0.9037 - val_loss: 0.3903 - val_acc: 0.9333\n","Epoch 468/500\n","135/135 [==============================] - 0s 136us/step - loss: 0.2622 - acc: 0.9037 - val_loss: 0.3901 - val_acc: 0.9333\n","Epoch 469/500\n","135/135 [==============================] - 0s 154us/step - loss: 0.2621 - acc: 0.9037 - val_loss: 0.3892 - val_acc: 0.9333\n","Epoch 470/500\n","135/135 [==============================] - 0s 159us/step - loss: 0.2617 - acc: 0.9037 - val_loss: 0.3888 - val_acc: 0.9333\n","Epoch 471/500\n","135/135 [==============================] - 0s 145us/step - loss: 0.2615 - acc: 0.9037 - val_loss: 0.3882 - val_acc: 0.9333\n","Epoch 472/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.2613 - acc: 0.9037 - val_loss: 0.3878 - val_acc: 0.9333\n","Epoch 473/500\n","135/135 [==============================] - 0s 132us/step - loss: 0.2611 - acc: 0.9037 - val_loss: 0.3876 - val_acc: 0.9333\n","Epoch 474/500\n","135/135 [==============================] - 0s 148us/step - loss: 0.2610 - acc: 0.9037 - val_loss: 0.3871 - val_acc: 0.9333\n","Epoch 475/500\n","135/135 [==============================] - 0s 150us/step - loss: 0.2607 - acc: 0.9037 - val_loss: 0.3869 - val_acc: 0.9333\n","Epoch 476/500\n","135/135 [==============================] - 0s 234us/step - loss: 0.2606 - acc: 0.9037 - val_loss: 0.3867 - val_acc: 0.9333\n","Epoch 477/500\n","135/135 [==============================] - 0s 190us/step - loss: 0.2603 - acc: 0.9037 - val_loss: 0.3867 - val_acc: 0.9333\n","Epoch 478/500\n","135/135 [==============================] - 0s 140us/step - loss: 0.2601 - acc: 0.9037 - val_loss: 0.3861 - val_acc: 0.9333\n","Epoch 479/500\n","135/135 [==============================] - 0s 138us/step - loss: 0.2599 - acc: 0.9037 - val_loss: 0.3859 - val_acc: 0.9333\n","Epoch 480/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.2597 - acc: 0.9037 - val_loss: 0.3856 - val_acc: 0.9333\n","Epoch 481/500\n","135/135 [==============================] - 0s 152us/step - loss: 0.2595 - acc: 0.9037 - val_loss: 0.3854 - val_acc: 0.9333\n","Epoch 482/500\n","135/135 [==============================] - 0s 146us/step - loss: 0.2593 - acc: 0.9037 - val_loss: 0.3851 - val_acc: 0.9333\n","Epoch 483/500\n","135/135 [==============================] - 0s 156us/step - loss: 0.2591 - acc: 0.9037 - val_loss: 0.3851 - val_acc: 0.9333\n","Epoch 484/500\n","135/135 [==============================] - 0s 136us/step - loss: 0.2589 - acc: 0.9037 - val_loss: 0.3851 - val_acc: 0.9333\n","Epoch 485/500\n","135/135 [==============================] - 0s 147us/step - loss: 0.2587 - acc: 0.9037 - val_loss: 0.3852 - val_acc: 0.9333\n","Epoch 486/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.2585 - acc: 0.9037 - val_loss: 0.3850 - val_acc: 0.9333\n","Epoch 487/500\n","135/135 [==============================] - 0s 137us/step - loss: 0.2583 - acc: 0.9037 - val_loss: 0.3850 - val_acc: 0.9333\n","Epoch 488/500\n","135/135 [==============================] - 0s 143us/step - loss: 0.2581 - acc: 0.9037 - val_loss: 0.3840 - val_acc: 0.9333\n","Epoch 489/500\n","135/135 [==============================] - 0s 139us/step - loss: 0.2579 - acc: 0.9037 - val_loss: 0.3833 - val_acc: 0.9333\n","Epoch 490/500\n","135/135 [==============================] - 0s 138us/step - loss: 0.2577 - acc: 0.9037 - val_loss: 0.3829 - val_acc: 0.9333\n","Epoch 491/500\n","135/135 [==============================] - 0s 144us/step - loss: 0.2574 - acc: 0.9037 - val_loss: 0.3828 - val_acc: 0.9333\n","Epoch 492/500\n","135/135 [==============================] - 0s 146us/step - loss: 0.2573 - acc: 0.9037 - val_loss: 0.3829 - val_acc: 0.9333\n","Epoch 493/500\n","135/135 [==============================] - 0s 135us/step - loss: 0.2571 - acc: 0.9037 - val_loss: 0.3828 - val_acc: 0.9333\n","Epoch 494/500\n","135/135 [==============================] - 0s 144us/step - loss: 0.2568 - acc: 0.9037 - val_loss: 0.3822 - val_acc: 0.9333\n","Epoch 495/500\n","135/135 [==============================] - 0s 141us/step - loss: 0.2568 - acc: 0.9037 - val_loss: 0.3818 - val_acc: 0.9333\n","Epoch 496/500\n","135/135 [==============================] - 0s 163us/step - loss: 0.2566 - acc: 0.9037 - val_loss: 0.3818 - val_acc: 0.9333\n","Epoch 497/500\n","135/135 [==============================] - 0s 158us/step - loss: 0.2564 - acc: 0.9037 - val_loss: 0.3817 - val_acc: 0.9333\n","Epoch 498/500\n","135/135 [==============================] - 0s 131us/step - loss: 0.2561 - acc: 0.9037 - val_loss: 0.3812 - val_acc: 0.9333\n","Epoch 499/500\n","135/135 [==============================] - 0s 124us/step - loss: 0.2560 - acc: 0.9037 - val_loss: 0.3809 - val_acc: 0.9333\n","Epoch 500/500\n","135/135 [==============================] - 0s 136us/step - loss: 0.2559 - acc: 0.9037 - val_loss: 0.3804 - val_acc: 0.9333\n","Accuracy :0.900\n","[[ 8  0  2]\n"," [ 0 10  0]\n"," [ 0  1  9]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1WdkjiA2AZYj"},"source":["# ADVANCED QUESTIONS\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OFBjZFLnA100"},"source":["### Effect of changing Nh\n","### Effect of changing Nepochs\n","### Effect of changing N, no. of training samples\n","\n","Can you observe overfitting? \n","\n","Can you do hyperparameter tuning here? \n","\n","To normalize test data, why do we use the mean and stddev of training data?\n"]},{"cell_type":"code","metadata":{"id":"Bp-K75kvB_fV","colab_type":"code","outputId":"ae68b5ff-fe73-494b-fe32-2e4a780edc07","executionInfo":{"status":"ok","timestamp":1560421146291,"user_tz":-330,"elapsed":71342,"user":{"displayName":"Soham Tiwari","photoUrl":"https://lh4.googleusercontent.com/-XiZ5rEdluPQ/AAAAAAAAAAI/AAAAAAAAH24/lvfjFM0g8Uw/s64/photo.jpg","userId":"06949155908663757402"}},"colab":{"base_uri":"https://localhost:8080/","height":43294}},"source":["Nh=[50,100]\n","Nepochs=[100,200]\n","N=[100,200]\n","accuracy=[]\n","k=0\n","for nh in Nh:\n","  for ne in Nepochs:\n","    for n in N:\n","      print(\"NH =\",nh)\n","      print(\"Nepochs =\",ne)\n","      print(\"N =\",n)\n","      model, mean_train, stddev_train = trainModel(n, nh, ne)\n","      acc, CM = testModel(model, 10, mean_train, stddev_train)\n","      accuracy+=[acc]\n","      k+=1\n","\n","      print(\"---------------------------------------------------------------------------------------------------------------------------\")"],"execution_count":19,"outputs":[{"output_type":"stream","text":["NH = 50\n","Nepochs = 100\n","N = 100\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         (None, 2)                 0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 50)                150       \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 3)                 153       \n","=================================================================\n","Total params: 303\n","Trainable params: 303\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 270 samples, validate on 30 samples\n","Epoch 1/100\n","270/270 [==============================] - 0s 712us/step - loss: 1.0971 - acc: 0.3667 - val_loss: 1.0960 - val_acc: 0.3000\n","Epoch 2/100\n","270/270 [==============================] - 0s 113us/step - loss: 1.0948 - acc: 0.3963 - val_loss: 1.0934 - val_acc: 0.3667\n","Epoch 3/100\n","270/270 [==============================] - 0s 120us/step - loss: 1.0925 - acc: 0.4259 - val_loss: 1.0912 - val_acc: 0.3667\n","Epoch 4/100\n","270/270 [==============================] - 0s 108us/step - loss: 1.0902 - acc: 0.4815 - val_loss: 1.0886 - val_acc: 0.6000\n","Epoch 5/100\n","270/270 [==============================] - 0s 129us/step - loss: 1.0879 - acc: 0.6000 - val_loss: 1.0863 - val_acc: 0.6333\n","Epoch 6/100\n","270/270 [==============================] - 0s 111us/step - loss: 1.0856 - acc: 0.6111 - val_loss: 1.0840 - val_acc: 0.6000\n","Epoch 7/100\n","270/270 [==============================] - 0s 113us/step - loss: 1.0832 - acc: 0.6148 - val_loss: 1.0813 - val_acc: 0.7000\n","Epoch 8/100\n","270/270 [==============================] - 0s 110us/step - loss: 1.0808 - acc: 0.6593 - val_loss: 1.0787 - val_acc: 0.7333\n","Epoch 9/100\n","270/270 [==============================] - 0s 113us/step - loss: 1.0782 - acc: 0.7111 - val_loss: 1.0758 - val_acc: 0.8000\n","Epoch 10/100\n","270/270 [==============================] - 0s 138us/step - loss: 1.0756 - acc: 0.7407 - val_loss: 1.0728 - val_acc: 0.8667\n","Epoch 11/100\n","270/270 [==============================] - 0s 120us/step - loss: 1.0729 - acc: 0.7778 - val_loss: 1.0698 - val_acc: 0.8667\n","Epoch 12/100\n","270/270 [==============================] - 0s 122us/step - loss: 1.0702 - acc: 0.8000 - val_loss: 1.0664 - val_acc: 0.9333\n","Epoch 13/100\n","270/270 [==============================] - 0s 134us/step - loss: 1.0674 - acc: 0.8185 - val_loss: 1.0631 - val_acc: 0.9333\n","Epoch 14/100\n","270/270 [==============================] - 0s 122us/step - loss: 1.0643 - acc: 0.8296 - val_loss: 1.0596 - val_acc: 0.9333\n","Epoch 15/100\n","270/270 [==============================] - 0s 116us/step - loss: 1.0611 - acc: 0.8444 - val_loss: 1.0562 - val_acc: 0.9333\n","Epoch 16/100\n","270/270 [==============================] - 0s 113us/step - loss: 1.0579 - acc: 0.8481 - val_loss: 1.0527 - val_acc: 0.9333\n","Epoch 17/100\n","270/270 [==============================] - 0s 139us/step - loss: 1.0545 - acc: 0.8481 - val_loss: 1.0490 - val_acc: 0.9667\n","Epoch 18/100\n","270/270 [==============================] - 0s 121us/step - loss: 1.0508 - acc: 0.8407 - val_loss: 1.0449 - val_acc: 0.9333\n","Epoch 19/100\n","270/270 [==============================] - 0s 121us/step - loss: 1.0471 - acc: 0.8519 - val_loss: 1.0407 - val_acc: 0.9333\n","Epoch 20/100\n","270/270 [==============================] - 0s 113us/step - loss: 1.0431 - acc: 0.8630 - val_loss: 1.0363 - val_acc: 0.9333\n","Epoch 21/100\n","270/270 [==============================] - 0s 118us/step - loss: 1.0391 - acc: 0.8667 - val_loss: 1.0315 - val_acc: 0.9333\n","Epoch 22/100\n","270/270 [==============================] - 0s 113us/step - loss: 1.0348 - acc: 0.8741 - val_loss: 1.0266 - val_acc: 0.9333\n","Epoch 23/100\n","270/270 [==============================] - 0s 123us/step - loss: 1.0306 - acc: 0.8630 - val_loss: 1.0215 - val_acc: 0.9333\n","Epoch 24/100\n","270/270 [==============================] - 0s 178us/step - loss: 1.0259 - acc: 0.8741 - val_loss: 1.0163 - val_acc: 0.9333\n","Epoch 25/100\n","270/270 [==============================] - 0s 112us/step - loss: 1.0212 - acc: 0.8704 - val_loss: 1.0111 - val_acc: 0.9333\n","Epoch 26/100\n","270/270 [==============================] - 0s 123us/step - loss: 1.0163 - acc: 0.8741 - val_loss: 1.0054 - val_acc: 0.9333\n","Epoch 27/100\n","270/270 [==============================] - 0s 111us/step - loss: 1.0113 - acc: 0.8741 - val_loss: 0.9996 - val_acc: 0.9333\n","Epoch 28/100\n","270/270 [==============================] - 0s 108us/step - loss: 1.0060 - acc: 0.8741 - val_loss: 0.9935 - val_acc: 0.9333\n","Epoch 29/100\n","270/270 [==============================] - 0s 114us/step - loss: 1.0005 - acc: 0.8815 - val_loss: 0.9873 - val_acc: 0.9333\n","Epoch 30/100\n","270/270 [==============================] - 0s 117us/step - loss: 0.9949 - acc: 0.8815 - val_loss: 0.9808 - val_acc: 0.9333\n","Epoch 31/100\n","270/270 [==============================] - 0s 106us/step - loss: 0.9893 - acc: 0.8815 - val_loss: 0.9742 - val_acc: 0.9333\n","Epoch 32/100\n","270/270 [==============================] - 0s 113us/step - loss: 0.9832 - acc: 0.8815 - val_loss: 0.9671 - val_acc: 0.9333\n","Epoch 33/100\n","270/270 [==============================] - 0s 115us/step - loss: 0.9770 - acc: 0.8852 - val_loss: 0.9598 - val_acc: 0.9333\n","Epoch 34/100\n","270/270 [==============================] - 0s 108us/step - loss: 0.9707 - acc: 0.8852 - val_loss: 0.9526 - val_acc: 0.9667\n","Epoch 35/100\n","270/270 [==============================] - 0s 125us/step - loss: 0.9644 - acc: 0.8889 - val_loss: 0.9451 - val_acc: 0.9667\n","Epoch 36/100\n","270/270 [==============================] - 0s 114us/step - loss: 0.9578 - acc: 0.8889 - val_loss: 0.9378 - val_acc: 0.9667\n","Epoch 37/100\n","270/270 [==============================] - 0s 110us/step - loss: 0.9512 - acc: 0.8852 - val_loss: 0.9300 - val_acc: 0.9667\n","Epoch 38/100\n","270/270 [==============================] - 0s 117us/step - loss: 0.9445 - acc: 0.8889 - val_loss: 0.9220 - val_acc: 0.9667\n","Epoch 39/100\n","270/270 [==============================] - 0s 116us/step - loss: 0.9375 - acc: 0.8852 - val_loss: 0.9139 - val_acc: 0.9667\n","Epoch 40/100\n","270/270 [==============================] - 0s 118us/step - loss: 0.9307 - acc: 0.8889 - val_loss: 0.9055 - val_acc: 0.9667\n","Epoch 41/100\n","270/270 [==============================] - 0s 113us/step - loss: 0.9235 - acc: 0.8889 - val_loss: 0.8971 - val_acc: 0.9667\n","Epoch 42/100\n","270/270 [==============================] - 0s 111us/step - loss: 0.9163 - acc: 0.8889 - val_loss: 0.8887 - val_acc: 0.9667\n","Epoch 43/100\n","270/270 [==============================] - 0s 120us/step - loss: 0.9090 - acc: 0.8889 - val_loss: 0.8803 - val_acc: 0.9667\n","Epoch 44/100\n","270/270 [==============================] - 0s 129us/step - loss: 0.9016 - acc: 0.8926 - val_loss: 0.8716 - val_acc: 0.9667\n","Epoch 45/100\n","270/270 [==============================] - 0s 121us/step - loss: 0.8942 - acc: 0.8926 - val_loss: 0.8626 - val_acc: 0.9667\n","Epoch 46/100\n","270/270 [==============================] - 0s 123us/step - loss: 0.8866 - acc: 0.8926 - val_loss: 0.8536 - val_acc: 0.9667\n","Epoch 47/100\n","270/270 [==============================] - 0s 108us/step - loss: 0.8789 - acc: 0.8926 - val_loss: 0.8447 - val_acc: 0.9667\n","Epoch 48/100\n","270/270 [==============================] - 0s 117us/step - loss: 0.8715 - acc: 0.8926 - val_loss: 0.8357 - val_acc: 0.9667\n","Epoch 49/100\n","270/270 [==============================] - 0s 114us/step - loss: 0.8637 - acc: 0.8926 - val_loss: 0.8267 - val_acc: 0.9667\n","Epoch 50/100\n","270/270 [==============================] - 0s 119us/step - loss: 0.8562 - acc: 0.8889 - val_loss: 0.8175 - val_acc: 0.9667\n","Epoch 51/100\n","270/270 [==============================] - 0s 124us/step - loss: 0.8484 - acc: 0.8889 - val_loss: 0.8085 - val_acc: 0.9667\n","Epoch 52/100\n","270/270 [==============================] - 0s 124us/step - loss: 0.8408 - acc: 0.8889 - val_loss: 0.7997 - val_acc: 0.9667\n","Epoch 53/100\n","270/270 [==============================] - 0s 126us/step - loss: 0.8331 - acc: 0.8815 - val_loss: 0.7904 - val_acc: 0.9667\n","Epoch 54/100\n","270/270 [==============================] - 0s 118us/step - loss: 0.8254 - acc: 0.8815 - val_loss: 0.7810 - val_acc: 0.9667\n","Epoch 55/100\n","270/270 [==============================] - 0s 139us/step - loss: 0.8176 - acc: 0.8815 - val_loss: 0.7721 - val_acc: 0.9667\n","Epoch 56/100\n","270/270 [==============================] - 0s 158us/step - loss: 0.8100 - acc: 0.8778 - val_loss: 0.7627 - val_acc: 0.9667\n","Epoch 57/100\n","270/270 [==============================] - 0s 116us/step - loss: 0.8024 - acc: 0.8778 - val_loss: 0.7534 - val_acc: 0.9667\n","Epoch 58/100\n","270/270 [==============================] - 0s 113us/step - loss: 0.7946 - acc: 0.8778 - val_loss: 0.7443 - val_acc: 0.9667\n","Epoch 59/100\n","270/270 [==============================] - 0s 102us/step - loss: 0.7871 - acc: 0.8778 - val_loss: 0.7353 - val_acc: 0.9667\n","Epoch 60/100\n","270/270 [==============================] - 0s 111us/step - loss: 0.7796 - acc: 0.8778 - val_loss: 0.7262 - val_acc: 0.9667\n","Epoch 61/100\n","270/270 [==============================] - 0s 113us/step - loss: 0.7720 - acc: 0.8778 - val_loss: 0.7171 - val_acc: 0.9667\n","Epoch 62/100\n","270/270 [==============================] - 0s 132us/step - loss: 0.7645 - acc: 0.8778 - val_loss: 0.7082 - val_acc: 0.9667\n","Epoch 63/100\n","270/270 [==============================] - 0s 119us/step - loss: 0.7571 - acc: 0.8778 - val_loss: 0.6990 - val_acc: 0.9667\n","Epoch 64/100\n","270/270 [==============================] - 0s 119us/step - loss: 0.7496 - acc: 0.8778 - val_loss: 0.6902 - val_acc: 0.9667\n","Epoch 65/100\n","270/270 [==============================] - 0s 124us/step - loss: 0.7425 - acc: 0.8778 - val_loss: 0.6814 - val_acc: 0.9667\n","Epoch 66/100\n","270/270 [==============================] - 0s 114us/step - loss: 0.7352 - acc: 0.8778 - val_loss: 0.6724 - val_acc: 0.9667\n","Epoch 67/100\n","270/270 [==============================] - 0s 116us/step - loss: 0.7278 - acc: 0.8778 - val_loss: 0.6638 - val_acc: 0.9667\n","Epoch 68/100\n","270/270 [==============================] - 0s 121us/step - loss: 0.7208 - acc: 0.8778 - val_loss: 0.6550 - val_acc: 0.9667\n","Epoch 69/100\n","270/270 [==============================] - 0s 120us/step - loss: 0.7137 - acc: 0.8778 - val_loss: 0.6465 - val_acc: 0.9667\n","Epoch 70/100\n","270/270 [==============================] - 0s 119us/step - loss: 0.7067 - acc: 0.8778 - val_loss: 0.6381 - val_acc: 0.9667\n","Epoch 71/100\n","270/270 [==============================] - 0s 137us/step - loss: 0.6998 - acc: 0.8741 - val_loss: 0.6297 - val_acc: 0.9667\n","Epoch 72/100\n","270/270 [==============================] - 0s 118us/step - loss: 0.6931 - acc: 0.8741 - val_loss: 0.6215 - val_acc: 0.9667\n","Epoch 73/100\n","270/270 [==============================] - 0s 115us/step - loss: 0.6864 - acc: 0.8778 - val_loss: 0.6132 - val_acc: 0.9667\n","Epoch 74/100\n","270/270 [==============================] - 0s 116us/step - loss: 0.6797 - acc: 0.8741 - val_loss: 0.6051 - val_acc: 0.9667\n","Epoch 75/100\n","270/270 [==============================] - 0s 112us/step - loss: 0.6733 - acc: 0.8741 - val_loss: 0.5970 - val_acc: 0.9667\n","Epoch 76/100\n","270/270 [==============================] - 0s 121us/step - loss: 0.6668 - acc: 0.8704 - val_loss: 0.5890 - val_acc: 0.9667\n","Epoch 77/100\n","270/270 [==============================] - 0s 141us/step - loss: 0.6605 - acc: 0.8778 - val_loss: 0.5815 - val_acc: 0.9667\n","Epoch 78/100\n","270/270 [==============================] - 0s 127us/step - loss: 0.6542 - acc: 0.8704 - val_loss: 0.5737 - val_acc: 0.9667\n","Epoch 79/100\n","270/270 [==============================] - 0s 105us/step - loss: 0.6481 - acc: 0.8704 - val_loss: 0.5661 - val_acc: 0.9667\n","Epoch 80/100\n","270/270 [==============================] - 0s 119us/step - loss: 0.6421 - acc: 0.8704 - val_loss: 0.5585 - val_acc: 0.9667\n","Epoch 81/100\n","270/270 [==============================] - 0s 113us/step - loss: 0.6361 - acc: 0.8704 - val_loss: 0.5510 - val_acc: 0.9667\n","Epoch 82/100\n","270/270 [==============================] - 0s 134us/step - loss: 0.6302 - acc: 0.8704 - val_loss: 0.5438 - val_acc: 0.9667\n","Epoch 83/100\n","270/270 [==============================] - 0s 123us/step - loss: 0.6245 - acc: 0.8704 - val_loss: 0.5367 - val_acc: 0.9667\n","Epoch 84/100\n","270/270 [==============================] - 0s 122us/step - loss: 0.6188 - acc: 0.8704 - val_loss: 0.5296 - val_acc: 0.9667\n","Epoch 85/100\n","270/270 [==============================] - 0s 110us/step - loss: 0.6133 - acc: 0.8704 - val_loss: 0.5227 - val_acc: 0.9667\n","Epoch 86/100\n","270/270 [==============================] - 0s 175us/step - loss: 0.6078 - acc: 0.8704 - val_loss: 0.5161 - val_acc: 0.9667\n","Epoch 87/100\n","270/270 [==============================] - 0s 127us/step - loss: 0.6027 - acc: 0.8704 - val_loss: 0.5092 - val_acc: 0.9667\n","Epoch 88/100\n","270/270 [==============================] - 0s 115us/step - loss: 0.5976 - acc: 0.8704 - val_loss: 0.5026 - val_acc: 0.9667\n","Epoch 89/100\n","270/270 [==============================] - 0s 119us/step - loss: 0.5922 - acc: 0.8704 - val_loss: 0.4962 - val_acc: 0.9667\n","Epoch 90/100\n","270/270 [==============================] - 0s 125us/step - loss: 0.5871 - acc: 0.8704 - val_loss: 0.4899 - val_acc: 0.9667\n","Epoch 91/100\n","270/270 [==============================] - 0s 118us/step - loss: 0.5822 - acc: 0.8704 - val_loss: 0.4834 - val_acc: 0.9667\n","Epoch 92/100\n","270/270 [==============================] - 0s 113us/step - loss: 0.5772 - acc: 0.8704 - val_loss: 0.4771 - val_acc: 0.9667\n","Epoch 93/100\n","270/270 [==============================] - 0s 120us/step - loss: 0.5723 - acc: 0.8704 - val_loss: 0.4711 - val_acc: 0.9667\n","Epoch 94/100\n","270/270 [==============================] - 0s 122us/step - loss: 0.5675 - acc: 0.8704 - val_loss: 0.4650 - val_acc: 0.9667\n","Epoch 95/100\n","270/270 [==============================] - 0s 118us/step - loss: 0.5630 - acc: 0.8704 - val_loss: 0.4591 - val_acc: 0.9667\n","Epoch 96/100\n","270/270 [==============================] - 0s 120us/step - loss: 0.5583 - acc: 0.8704 - val_loss: 0.4532 - val_acc: 0.9667\n","Epoch 97/100\n","270/270 [==============================] - 0s 129us/step - loss: 0.5539 - acc: 0.8741 - val_loss: 0.4475 - val_acc: 0.9667\n","Epoch 98/100\n","270/270 [==============================] - 0s 125us/step - loss: 0.5494 - acc: 0.8741 - val_loss: 0.4418 - val_acc: 0.9667\n","Epoch 99/100\n","270/270 [==============================] - 0s 138us/step - loss: 0.5451 - acc: 0.8741 - val_loss: 0.4364 - val_acc: 0.9667\n","Epoch 100/100\n","270/270 [==============================] - 0s 122us/step - loss: 0.5409 - acc: 0.8741 - val_loss: 0.4309 - val_acc: 0.9667\n","Accuracy :0.933\n","[[ 9  0  1]\n"," [ 0 10  0]\n"," [ 1  0  9]]\n","---------------------------------------------------------------------------------------------------------------------------\n","NH = 50\n","Nepochs = 100\n","N = 200\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_3 (InputLayer)         (None, 2)                 0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 50)                150       \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 3)                 153       \n","=================================================================\n","Total params: 303\n","Trainable params: 303\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 540 samples, validate on 60 samples\n","Epoch 1/100\n","540/540 [==============================] - 0s 475us/step - loss: 1.0994 - acc: 0.3407 - val_loss: 1.1006 - val_acc: 0.2667\n","Epoch 2/100\n","540/540 [==============================] - 0s 91us/step - loss: 1.0949 - acc: 0.3407 - val_loss: 1.0964 - val_acc: 0.2667\n","Epoch 3/100\n","540/540 [==============================] - 0s 91us/step - loss: 1.0903 - acc: 0.3704 - val_loss: 1.0911 - val_acc: 0.3667\n","Epoch 4/100\n","540/540 [==============================] - 0s 104us/step - loss: 1.0858 - acc: 0.4630 - val_loss: 1.0862 - val_acc: 0.4667\n","Epoch 5/100\n","540/540 [==============================] - 0s 92us/step - loss: 1.0812 - acc: 0.5130 - val_loss: 1.0815 - val_acc: 0.5500\n","Epoch 6/100\n","540/540 [==============================] - 0s 89us/step - loss: 1.0762 - acc: 0.6167 - val_loss: 1.0761 - val_acc: 0.6333\n","Epoch 7/100\n","540/540 [==============================] - 0s 91us/step - loss: 1.0709 - acc: 0.6759 - val_loss: 1.0700 - val_acc: 0.6833\n","Epoch 8/100\n","540/540 [==============================] - 0s 89us/step - loss: 1.0653 - acc: 0.7426 - val_loss: 1.0638 - val_acc: 0.7333\n","Epoch 9/100\n","540/540 [==============================] - 0s 94us/step - loss: 1.0592 - acc: 0.7981 - val_loss: 1.0568 - val_acc: 0.7667\n","Epoch 10/100\n","540/540 [==============================] - 0s 98us/step - loss: 1.0525 - acc: 0.8315 - val_loss: 1.0492 - val_acc: 0.8000\n","Epoch 11/100\n","540/540 [==============================] - 0s 104us/step - loss: 1.0450 - acc: 0.8481 - val_loss: 1.0414 - val_acc: 0.8167\n","Epoch 12/100\n","540/540 [==============================] - 0s 92us/step - loss: 1.0371 - acc: 0.8556 - val_loss: 1.0329 - val_acc: 0.8000\n","Epoch 13/100\n","540/540 [==============================] - 0s 89us/step - loss: 1.0286 - acc: 0.8611 - val_loss: 1.0235 - val_acc: 0.8333\n","Epoch 14/100\n","540/540 [==============================] - 0s 90us/step - loss: 1.0194 - acc: 0.8630 - val_loss: 1.0133 - val_acc: 0.8500\n","Epoch 15/100\n","540/540 [==============================] - 0s 88us/step - loss: 1.0096 - acc: 0.8630 - val_loss: 1.0026 - val_acc: 0.8333\n","Epoch 16/100\n","540/540 [==============================] - 0s 92us/step - loss: 0.9992 - acc: 0.8759 - val_loss: 0.9911 - val_acc: 0.8667\n","Epoch 17/100\n","540/540 [==============================] - 0s 99us/step - loss: 0.9881 - acc: 0.8759 - val_loss: 0.9797 - val_acc: 0.8667\n","Epoch 18/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.9765 - acc: 0.8722 - val_loss: 0.9676 - val_acc: 0.8667\n","Epoch 19/100\n","540/540 [==============================] - 0s 136us/step - loss: 0.9643 - acc: 0.8796 - val_loss: 0.9543 - val_acc: 0.8667\n","Epoch 20/100\n","540/540 [==============================] - 0s 86us/step - loss: 0.9517 - acc: 0.8889 - val_loss: 0.9406 - val_acc: 0.8833\n","Epoch 21/100\n","540/540 [==============================] - 0s 88us/step - loss: 0.9386 - acc: 0.8889 - val_loss: 0.9266 - val_acc: 0.9000\n","Epoch 22/100\n","540/540 [==============================] - 0s 91us/step - loss: 0.9252 - acc: 0.8889 - val_loss: 0.9123 - val_acc: 0.9000\n","Epoch 23/100\n","540/540 [==============================] - 0s 86us/step - loss: 0.9111 - acc: 0.8889 - val_loss: 0.8974 - val_acc: 0.9167\n","Epoch 24/100\n","540/540 [==============================] - 0s 98us/step - loss: 0.8970 - acc: 0.8907 - val_loss: 0.8824 - val_acc: 0.9167\n","Epoch 25/100\n","540/540 [==============================] - 0s 98us/step - loss: 0.8827 - acc: 0.8852 - val_loss: 0.8671 - val_acc: 0.9167\n","Epoch 26/100\n","540/540 [==============================] - 0s 89us/step - loss: 0.8679 - acc: 0.8852 - val_loss: 0.8516 - val_acc: 0.9167\n","Epoch 27/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.8532 - acc: 0.8870 - val_loss: 0.8363 - val_acc: 0.9167\n","Epoch 28/100\n","540/540 [==============================] - 0s 97us/step - loss: 0.8384 - acc: 0.8870 - val_loss: 0.8209 - val_acc: 0.9167\n","Epoch 29/100\n","540/540 [==============================] - 0s 96us/step - loss: 0.8235 - acc: 0.8870 - val_loss: 0.8054 - val_acc: 0.9333\n","Epoch 30/100\n","540/540 [==============================] - 0s 91us/step - loss: 0.8087 - acc: 0.8889 - val_loss: 0.7896 - val_acc: 0.9333\n","Epoch 31/100\n","540/540 [==============================] - 0s 92us/step - loss: 0.7939 - acc: 0.8889 - val_loss: 0.7744 - val_acc: 0.9333\n","Epoch 32/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.7791 - acc: 0.8889 - val_loss: 0.7590 - val_acc: 0.9333\n","Epoch 33/100\n","540/540 [==============================] - 0s 92us/step - loss: 0.7646 - acc: 0.8889 - val_loss: 0.7438 - val_acc: 0.9333\n","Epoch 34/100\n","540/540 [==============================] - 0s 90us/step - loss: 0.7500 - acc: 0.8870 - val_loss: 0.7288 - val_acc: 0.9333\n","Epoch 35/100\n","540/540 [==============================] - 0s 90us/step - loss: 0.7358 - acc: 0.8870 - val_loss: 0.7140 - val_acc: 0.9333\n","Epoch 36/100\n","540/540 [==============================] - 0s 103us/step - loss: 0.7217 - acc: 0.8870 - val_loss: 0.6996 - val_acc: 0.9333\n","Epoch 37/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.7077 - acc: 0.8870 - val_loss: 0.6854 - val_acc: 0.9500\n","Epoch 38/100\n","540/540 [==============================] - 0s 90us/step - loss: 0.6941 - acc: 0.8852 - val_loss: 0.6719 - val_acc: 0.9333\n","Epoch 39/100\n","540/540 [==============================] - 0s 138us/step - loss: 0.6809 - acc: 0.8852 - val_loss: 0.6578 - val_acc: 0.9333\n","Epoch 40/100\n","540/540 [==============================] - 0s 91us/step - loss: 0.6677 - acc: 0.8870 - val_loss: 0.6443 - val_acc: 0.9500\n","Epoch 41/100\n","540/540 [==============================] - 0s 88us/step - loss: 0.6548 - acc: 0.8852 - val_loss: 0.6315 - val_acc: 0.9500\n","Epoch 42/100\n","540/540 [==============================] - 0s 90us/step - loss: 0.6424 - acc: 0.8870 - val_loss: 0.6188 - val_acc: 0.9500\n","Epoch 43/100\n","540/540 [==============================] - 0s 90us/step - loss: 0.6302 - acc: 0.8852 - val_loss: 0.6063 - val_acc: 0.9500\n","Epoch 44/100\n","540/540 [==============================] - 0s 96us/step - loss: 0.6182 - acc: 0.8870 - val_loss: 0.5945 - val_acc: 0.9500\n","Epoch 45/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.6068 - acc: 0.8870 - val_loss: 0.5828 - val_acc: 0.9500\n","Epoch 46/100\n","540/540 [==============================] - 0s 91us/step - loss: 0.5954 - acc: 0.8870 - val_loss: 0.5714 - val_acc: 0.9500\n","Epoch 47/100\n","540/540 [==============================] - 0s 89us/step - loss: 0.5845 - acc: 0.8852 - val_loss: 0.5601 - val_acc: 0.9500\n","Epoch 48/100\n","540/540 [==============================] - 0s 105us/step - loss: 0.5739 - acc: 0.8852 - val_loss: 0.5494 - val_acc: 0.9500\n","Epoch 49/100\n","540/540 [==============================] - 0s 96us/step - loss: 0.5637 - acc: 0.8833 - val_loss: 0.5388 - val_acc: 0.9333\n","Epoch 50/100\n","540/540 [==============================] - 0s 90us/step - loss: 0.5535 - acc: 0.8833 - val_loss: 0.5289 - val_acc: 0.9333\n","Epoch 51/100\n","540/540 [==============================] - 0s 90us/step - loss: 0.5440 - acc: 0.8852 - val_loss: 0.5193 - val_acc: 0.9333\n","Epoch 52/100\n","540/540 [==============================] - 0s 90us/step - loss: 0.5345 - acc: 0.8833 - val_loss: 0.5096 - val_acc: 0.9333\n","Epoch 53/100\n","540/540 [==============================] - 0s 91us/step - loss: 0.5253 - acc: 0.8833 - val_loss: 0.5003 - val_acc: 0.9333\n","Epoch 54/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.5165 - acc: 0.8833 - val_loss: 0.4913 - val_acc: 0.9333\n","Epoch 55/100\n","540/540 [==============================] - 0s 92us/step - loss: 0.5080 - acc: 0.8833 - val_loss: 0.4827 - val_acc: 0.9333\n","Epoch 56/100\n","540/540 [==============================] - 0s 91us/step - loss: 0.4998 - acc: 0.8852 - val_loss: 0.4741 - val_acc: 0.9333\n","Epoch 57/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.4917 - acc: 0.8852 - val_loss: 0.4662 - val_acc: 0.9333\n","Epoch 58/100\n","540/540 [==============================] - 0s 92us/step - loss: 0.4839 - acc: 0.8870 - val_loss: 0.4585 - val_acc: 0.9333\n","Epoch 59/100\n","540/540 [==============================] - 0s 134us/step - loss: 0.4766 - acc: 0.8833 - val_loss: 0.4510 - val_acc: 0.9333\n","Epoch 60/100\n","540/540 [==============================] - 0s 91us/step - loss: 0.4692 - acc: 0.8833 - val_loss: 0.4434 - val_acc: 0.9333\n","Epoch 61/100\n","540/540 [==============================] - 0s 88us/step - loss: 0.4621 - acc: 0.8870 - val_loss: 0.4363 - val_acc: 0.9333\n","Epoch 62/100\n","540/540 [==============================] - 0s 101us/step - loss: 0.4554 - acc: 0.8870 - val_loss: 0.4295 - val_acc: 0.9333\n","Epoch 63/100\n","540/540 [==============================] - 0s 97us/step - loss: 0.4488 - acc: 0.8870 - val_loss: 0.4230 - val_acc: 0.9333\n","Epoch 64/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.4425 - acc: 0.8870 - val_loss: 0.4165 - val_acc: 0.9333\n","Epoch 65/100\n","540/540 [==============================] - 0s 92us/step - loss: 0.4365 - acc: 0.8870 - val_loss: 0.4102 - val_acc: 0.9333\n","Epoch 66/100\n","540/540 [==============================] - 0s 91us/step - loss: 0.4304 - acc: 0.8870 - val_loss: 0.4042 - val_acc: 0.9333\n","Epoch 67/100\n","540/540 [==============================] - 0s 87us/step - loss: 0.4248 - acc: 0.8870 - val_loss: 0.3982 - val_acc: 0.9333\n","Epoch 68/100\n","540/540 [==============================] - 0s 96us/step - loss: 0.4193 - acc: 0.8870 - val_loss: 0.3923 - val_acc: 0.9333\n","Epoch 69/100\n","540/540 [==============================] - 0s 92us/step - loss: 0.4138 - acc: 0.8870 - val_loss: 0.3871 - val_acc: 0.9333\n","Epoch 70/100\n","540/540 [==============================] - 0s 98us/step - loss: 0.4088 - acc: 0.8870 - val_loss: 0.3817 - val_acc: 0.9333\n","Epoch 71/100\n","540/540 [==============================] - 0s 89us/step - loss: 0.4038 - acc: 0.8870 - val_loss: 0.3767 - val_acc: 0.9333\n","Epoch 72/100\n","540/540 [==============================] - 0s 97us/step - loss: 0.3989 - acc: 0.8870 - val_loss: 0.3719 - val_acc: 0.9333\n","Epoch 73/100\n","540/540 [==============================] - 0s 88us/step - loss: 0.3945 - acc: 0.8870 - val_loss: 0.3668 - val_acc: 0.9333\n","Epoch 74/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.3900 - acc: 0.8889 - val_loss: 0.3623 - val_acc: 0.9333\n","Epoch 75/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.3856 - acc: 0.8870 - val_loss: 0.3581 - val_acc: 0.9333\n","Epoch 76/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.3816 - acc: 0.8870 - val_loss: 0.3536 - val_acc: 0.9333\n","Epoch 77/100\n","540/540 [==============================] - 0s 92us/step - loss: 0.3775 - acc: 0.8870 - val_loss: 0.3496 - val_acc: 0.9333\n","Epoch 78/100\n","540/540 [==============================] - 0s 90us/step - loss: 0.3736 - acc: 0.8852 - val_loss: 0.3455 - val_acc: 0.9333\n","Epoch 79/100\n","540/540 [==============================] - 0s 136us/step - loss: 0.3700 - acc: 0.8870 - val_loss: 0.3419 - val_acc: 0.9333\n","Epoch 80/100\n","540/540 [==============================] - 0s 90us/step - loss: 0.3664 - acc: 0.8870 - val_loss: 0.3382 - val_acc: 0.9333\n","Epoch 81/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.3629 - acc: 0.8852 - val_loss: 0.3343 - val_acc: 0.9333\n","Epoch 82/100\n","540/540 [==============================] - 0s 96us/step - loss: 0.3596 - acc: 0.8852 - val_loss: 0.3310 - val_acc: 0.9333\n","Epoch 83/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.3564 - acc: 0.8852 - val_loss: 0.3276 - val_acc: 0.9333\n","Epoch 84/100\n","540/540 [==============================] - 0s 89us/step - loss: 0.3533 - acc: 0.8852 - val_loss: 0.3246 - val_acc: 0.9333\n","Epoch 85/100\n","540/540 [==============================] - 0s 94us/step - loss: 0.3503 - acc: 0.8852 - val_loss: 0.3215 - val_acc: 0.9333\n","Epoch 86/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.3475 - acc: 0.8852 - val_loss: 0.3185 - val_acc: 0.9333\n","Epoch 87/100\n","540/540 [==============================] - 0s 90us/step - loss: 0.3449 - acc: 0.8852 - val_loss: 0.3159 - val_acc: 0.9333\n","Epoch 88/100\n","540/540 [==============================] - 0s 97us/step - loss: 0.3421 - acc: 0.8852 - val_loss: 0.3129 - val_acc: 0.9333\n","Epoch 89/100\n","540/540 [==============================] - 0s 91us/step - loss: 0.3396 - acc: 0.8852 - val_loss: 0.3104 - val_acc: 0.9333\n","Epoch 90/100\n","540/540 [==============================] - 0s 88us/step - loss: 0.3371 - acc: 0.8833 - val_loss: 0.3078 - val_acc: 0.9333\n","Epoch 91/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.3348 - acc: 0.8833 - val_loss: 0.3054 - val_acc: 0.9333\n","Epoch 92/100\n","540/540 [==============================] - 0s 94us/step - loss: 0.3325 - acc: 0.8852 - val_loss: 0.3031 - val_acc: 0.9333\n","Epoch 93/100\n","540/540 [==============================] - 0s 97us/step - loss: 0.3304 - acc: 0.8852 - val_loss: 0.3008 - val_acc: 0.9333\n","Epoch 94/100\n","540/540 [==============================] - 0s 90us/step - loss: 0.3283 - acc: 0.8852 - val_loss: 0.2987 - val_acc: 0.9333\n","Epoch 95/100\n","540/540 [==============================] - 0s 90us/step - loss: 0.3263 - acc: 0.8889 - val_loss: 0.2966 - val_acc: 0.9333\n","Epoch 96/100\n","540/540 [==============================] - 0s 89us/step - loss: 0.3243 - acc: 0.8889 - val_loss: 0.2946 - val_acc: 0.9333\n","Epoch 97/100\n","540/540 [==============================] - 0s 91us/step - loss: 0.3225 - acc: 0.8889 - val_loss: 0.2926 - val_acc: 0.9333\n","Epoch 98/100\n","540/540 [==============================] - 0s 121us/step - loss: 0.3207 - acc: 0.8889 - val_loss: 0.2910 - val_acc: 0.9333\n","Epoch 99/100\n","540/540 [==============================] - 0s 110us/step - loss: 0.3189 - acc: 0.8889 - val_loss: 0.2892 - val_acc: 0.9333\n","Epoch 100/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.3173 - acc: 0.8889 - val_loss: 0.2874 - val_acc: 0.9333\n","Accuracy :0.900\n","[[ 8  1  1]\n"," [ 0 10  0]\n"," [ 1  0  9]]\n","---------------------------------------------------------------------------------------------------------------------------\n","NH = 50\n","Nepochs = 200\n","N = 100\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         (None, 2)                 0         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 50)                150       \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 3)                 153       \n","=================================================================\n","Total params: 303\n","Trainable params: 303\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 270 samples, validate on 30 samples\n","Epoch 1/200\n","270/270 [==============================] - 0s 877us/step - loss: 1.0956 - acc: 0.3407 - val_loss: 1.1003 - val_acc: 0.2667\n","Epoch 2/200\n","270/270 [==============================] - 0s 110us/step - loss: 1.0934 - acc: 0.3407 - val_loss: 1.0976 - val_acc: 0.2667\n","Epoch 3/200\n","270/270 [==============================] - 0s 122us/step - loss: 1.0912 - acc: 0.3407 - val_loss: 1.0953 - val_acc: 0.3000\n","Epoch 4/200\n","270/270 [==============================] - 0s 105us/step - loss: 1.0890 - acc: 0.3556 - val_loss: 1.0933 - val_acc: 0.3000\n","Epoch 5/200\n","270/270 [==============================] - 0s 108us/step - loss: 1.0870 - acc: 0.3778 - val_loss: 1.0913 - val_acc: 0.3667\n","Epoch 6/200\n","270/270 [==============================] - 0s 107us/step - loss: 1.0848 - acc: 0.3926 - val_loss: 1.0892 - val_acc: 0.3667\n","Epoch 7/200\n","270/270 [==============================] - 0s 109us/step - loss: 1.0827 - acc: 0.4296 - val_loss: 1.0867 - val_acc: 0.4333\n","Epoch 8/200\n","270/270 [==============================] - 0s 108us/step - loss: 1.0804 - acc: 0.4593 - val_loss: 1.0844 - val_acc: 0.4333\n","Epoch 9/200\n","270/270 [==============================] - 0s 111us/step - loss: 1.0780 - acc: 0.5037 - val_loss: 1.0818 - val_acc: 0.4333\n","Epoch 10/200\n","270/270 [==============================] - 0s 111us/step - loss: 1.0758 - acc: 0.5593 - val_loss: 1.0791 - val_acc: 0.5333\n","Epoch 11/200\n","270/270 [==============================] - 0s 109us/step - loss: 1.0733 - acc: 0.6074 - val_loss: 1.0763 - val_acc: 0.5667\n","Epoch 12/200\n","270/270 [==============================] - 0s 110us/step - loss: 1.0707 - acc: 0.6407 - val_loss: 1.0740 - val_acc: 0.5667\n","Epoch 13/200\n","270/270 [==============================] - 0s 158us/step - loss: 1.0680 - acc: 0.6519 - val_loss: 1.0713 - val_acc: 0.5667\n","Epoch 14/200\n","270/270 [==============================] - 0s 136us/step - loss: 1.0652 - acc: 0.6778 - val_loss: 1.0682 - val_acc: 0.6000\n","Epoch 15/200\n","270/270 [==============================] - 0s 114us/step - loss: 1.0623 - acc: 0.7111 - val_loss: 1.0653 - val_acc: 0.6000\n","Epoch 16/200\n","270/270 [==============================] - 0s 107us/step - loss: 1.0592 - acc: 0.7333 - val_loss: 1.0620 - val_acc: 0.6000\n","Epoch 17/200\n","270/270 [==============================] - 0s 108us/step - loss: 1.0563 - acc: 0.7370 - val_loss: 1.0590 - val_acc: 0.6000\n","Epoch 18/200\n","270/270 [==============================] - 0s 111us/step - loss: 1.0529 - acc: 0.7556 - val_loss: 1.0550 - val_acc: 0.7333\n","Epoch 19/200\n","270/270 [==============================] - 0s 108us/step - loss: 1.0494 - acc: 0.7630 - val_loss: 1.0511 - val_acc: 0.7667\n","Epoch 20/200\n","270/270 [==============================] - 0s 105us/step - loss: 1.0458 - acc: 0.7778 - val_loss: 1.0471 - val_acc: 0.8000\n","Epoch 21/200\n","270/270 [==============================] - 0s 105us/step - loss: 1.0421 - acc: 0.8074 - val_loss: 1.0430 - val_acc: 0.8000\n","Epoch 22/200\n","270/270 [==============================] - 0s 118us/step - loss: 1.0381 - acc: 0.8074 - val_loss: 1.0389 - val_acc: 0.8333\n","Epoch 23/200\n","270/270 [==============================] - 0s 109us/step - loss: 1.0340 - acc: 0.8222 - val_loss: 1.0345 - val_acc: 0.8333\n","Epoch 24/200\n","270/270 [==============================] - 0s 107us/step - loss: 1.0298 - acc: 0.8222 - val_loss: 1.0300 - val_acc: 0.8333\n","Epoch 25/200\n","270/270 [==============================] - 0s 114us/step - loss: 1.0254 - acc: 0.8296 - val_loss: 1.0253 - val_acc: 0.8333\n","Epoch 26/200\n","270/270 [==============================] - 0s 110us/step - loss: 1.0209 - acc: 0.8370 - val_loss: 1.0206 - val_acc: 0.8333\n","Epoch 27/200\n","270/270 [==============================] - 0s 133us/step - loss: 1.0162 - acc: 0.8407 - val_loss: 1.0159 - val_acc: 0.8333\n","Epoch 28/200\n","270/270 [==============================] - 0s 122us/step - loss: 1.0112 - acc: 0.8444 - val_loss: 1.0108 - val_acc: 0.8333\n","Epoch 29/200\n","270/270 [==============================] - 0s 110us/step - loss: 1.0062 - acc: 0.8444 - val_loss: 1.0052 - val_acc: 0.8000\n","Epoch 30/200\n","270/270 [==============================] - 0s 112us/step - loss: 1.0010 - acc: 0.8481 - val_loss: 0.9999 - val_acc: 0.8000\n","Epoch 31/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.9955 - acc: 0.8481 - val_loss: 0.9945 - val_acc: 0.8000\n","Epoch 32/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.9900 - acc: 0.8593 - val_loss: 0.9886 - val_acc: 0.8000\n","Epoch 33/200\n","270/270 [==============================] - 0s 104us/step - loss: 0.9843 - acc: 0.8630 - val_loss: 0.9827 - val_acc: 0.8000\n","Epoch 34/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.9784 - acc: 0.8630 - val_loss: 0.9767 - val_acc: 0.8000\n","Epoch 35/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.9724 - acc: 0.8630 - val_loss: 0.9708 - val_acc: 0.8000\n","Epoch 36/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.9663 - acc: 0.8593 - val_loss: 0.9644 - val_acc: 0.8000\n","Epoch 37/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.9600 - acc: 0.8593 - val_loss: 0.9580 - val_acc: 0.8000\n","Epoch 38/200\n","270/270 [==============================] - 0s 103us/step - loss: 0.9537 - acc: 0.8556 - val_loss: 0.9515 - val_acc: 0.8000\n","Epoch 39/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.9471 - acc: 0.8630 - val_loss: 0.9445 - val_acc: 0.8000\n","Epoch 40/200\n","270/270 [==============================] - 0s 104us/step - loss: 0.9404 - acc: 0.8630 - val_loss: 0.9377 - val_acc: 0.8000\n","Epoch 41/200\n","270/270 [==============================] - 0s 103us/step - loss: 0.9336 - acc: 0.8630 - val_loss: 0.9306 - val_acc: 0.8000\n","Epoch 42/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.9270 - acc: 0.8667 - val_loss: 0.9233 - val_acc: 0.8000\n","Epoch 43/200\n","270/270 [==============================] - 0s 118us/step - loss: 0.9198 - acc: 0.8667 - val_loss: 0.9167 - val_acc: 0.8000\n","Epoch 44/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.9127 - acc: 0.8667 - val_loss: 0.9096 - val_acc: 0.8000\n","Epoch 45/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.9056 - acc: 0.8630 - val_loss: 0.9022 - val_acc: 0.8000\n","Epoch 46/200\n","270/270 [==============================] - 0s 153us/step - loss: 0.8983 - acc: 0.8630 - val_loss: 0.8951 - val_acc: 0.8000\n","Epoch 47/200\n","270/270 [==============================] - 0s 131us/step - loss: 0.8910 - acc: 0.8630 - val_loss: 0.8878 - val_acc: 0.8000\n","Epoch 48/200\n","270/270 [==============================] - 0s 103us/step - loss: 0.8836 - acc: 0.8630 - val_loss: 0.8806 - val_acc: 0.8000\n","Epoch 49/200\n","270/270 [==============================] - 0s 105us/step - loss: 0.8764 - acc: 0.8630 - val_loss: 0.8731 - val_acc: 0.8000\n","Epoch 50/200\n","270/270 [==============================] - 0s 124us/step - loss: 0.8687 - acc: 0.8556 - val_loss: 0.8658 - val_acc: 0.8000\n","Epoch 51/200\n","270/270 [==============================] - 0s 106us/step - loss: 0.8612 - acc: 0.8593 - val_loss: 0.8587 - val_acc: 0.8000\n","Epoch 52/200\n","270/270 [==============================] - 0s 106us/step - loss: 0.8539 - acc: 0.8556 - val_loss: 0.8511 - val_acc: 0.8000\n","Epoch 53/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.8463 - acc: 0.8593 - val_loss: 0.8436 - val_acc: 0.8000\n","Epoch 54/200\n","270/270 [==============================] - 0s 117us/step - loss: 0.8387 - acc: 0.8593 - val_loss: 0.8363 - val_acc: 0.8000\n","Epoch 55/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.8311 - acc: 0.8630 - val_loss: 0.8288 - val_acc: 0.8000\n","Epoch 56/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.8233 - acc: 0.8630 - val_loss: 0.8212 - val_acc: 0.8000\n","Epoch 57/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.8157 - acc: 0.8593 - val_loss: 0.8136 - val_acc: 0.8000\n","Epoch 58/200\n","270/270 [==============================] - 0s 106us/step - loss: 0.8082 - acc: 0.8593 - val_loss: 0.8063 - val_acc: 0.8000\n","Epoch 59/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.8005 - acc: 0.8593 - val_loss: 0.7988 - val_acc: 0.8000\n","Epoch 60/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.7929 - acc: 0.8593 - val_loss: 0.7915 - val_acc: 0.8000\n","Epoch 61/200\n","270/270 [==============================] - 0s 104us/step - loss: 0.7853 - acc: 0.8593 - val_loss: 0.7841 - val_acc: 0.8000\n","Epoch 62/200\n","270/270 [==============================] - 0s 106us/step - loss: 0.7780 - acc: 0.8593 - val_loss: 0.7766 - val_acc: 0.8000\n","Epoch 63/200\n","270/270 [==============================] - 0s 105us/step - loss: 0.7703 - acc: 0.8593 - val_loss: 0.7697 - val_acc: 0.8000\n","Epoch 64/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.7629 - acc: 0.8593 - val_loss: 0.7623 - val_acc: 0.8000\n","Epoch 65/200\n","270/270 [==============================] - 0s 119us/step - loss: 0.7555 - acc: 0.8593 - val_loss: 0.7553 - val_acc: 0.8000\n","Epoch 66/200\n","270/270 [==============================] - 0s 104us/step - loss: 0.7482 - acc: 0.8593 - val_loss: 0.7490 - val_acc: 0.8000\n","Epoch 67/200\n","270/270 [==============================] - 0s 118us/step - loss: 0.7410 - acc: 0.8593 - val_loss: 0.7419 - val_acc: 0.8000\n","Epoch 68/200\n","270/270 [==============================] - 0s 105us/step - loss: 0.7337 - acc: 0.8593 - val_loss: 0.7350 - val_acc: 0.8000\n","Epoch 69/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.7264 - acc: 0.8593 - val_loss: 0.7283 - val_acc: 0.8000\n","Epoch 70/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.7196 - acc: 0.8593 - val_loss: 0.7218 - val_acc: 0.8000\n","Epoch 71/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.7126 - acc: 0.8593 - val_loss: 0.7155 - val_acc: 0.8000\n","Epoch 72/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.7055 - acc: 0.8593 - val_loss: 0.7089 - val_acc: 0.8000\n","Epoch 73/200\n","270/270 [==============================] - 0s 103us/step - loss: 0.6987 - acc: 0.8593 - val_loss: 0.7023 - val_acc: 0.8000\n","Epoch 74/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.6918 - acc: 0.8593 - val_loss: 0.6959 - val_acc: 0.8000\n","Epoch 75/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.6852 - acc: 0.8593 - val_loss: 0.6896 - val_acc: 0.8000\n","Epoch 76/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.6785 - acc: 0.8593 - val_loss: 0.6833 - val_acc: 0.8000\n","Epoch 77/200\n","270/270 [==============================] - 0s 104us/step - loss: 0.6718 - acc: 0.8593 - val_loss: 0.6770 - val_acc: 0.8000\n","Epoch 78/200\n","270/270 [==============================] - 0s 121us/step - loss: 0.6653 - acc: 0.8556 - val_loss: 0.6709 - val_acc: 0.8000\n","Epoch 79/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.6588 - acc: 0.8556 - val_loss: 0.6649 - val_acc: 0.8000\n","Epoch 80/200\n","270/270 [==============================] - 0s 179us/step - loss: 0.6525 - acc: 0.8556 - val_loss: 0.6590 - val_acc: 0.8000\n","Epoch 81/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.6463 - acc: 0.8556 - val_loss: 0.6532 - val_acc: 0.8000\n","Epoch 82/200\n","270/270 [==============================] - 0s 105us/step - loss: 0.6400 - acc: 0.8556 - val_loss: 0.6473 - val_acc: 0.8000\n","Epoch 83/200\n","270/270 [==============================] - 0s 117us/step - loss: 0.6339 - acc: 0.8593 - val_loss: 0.6415 - val_acc: 0.8000\n","Epoch 84/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.6279 - acc: 0.8593 - val_loss: 0.6359 - val_acc: 0.8000\n","Epoch 85/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.6219 - acc: 0.8593 - val_loss: 0.6306 - val_acc: 0.8000\n","Epoch 86/200\n","270/270 [==============================] - 0s 127us/step - loss: 0.6163 - acc: 0.8556 - val_loss: 0.6250 - val_acc: 0.8000\n","Epoch 87/200\n","270/270 [==============================] - 0s 104us/step - loss: 0.6106 - acc: 0.8556 - val_loss: 0.6200 - val_acc: 0.8000\n","Epoch 88/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.6049 - acc: 0.8593 - val_loss: 0.6150 - val_acc: 0.8000\n","Epoch 89/200\n","270/270 [==============================] - 0s 101us/step - loss: 0.5994 - acc: 0.8556 - val_loss: 0.6099 - val_acc: 0.8000\n","Epoch 90/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.5938 - acc: 0.8556 - val_loss: 0.6050 - val_acc: 0.8000\n","Epoch 91/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.5885 - acc: 0.8556 - val_loss: 0.6001 - val_acc: 0.8000\n","Epoch 92/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.5831 - acc: 0.8556 - val_loss: 0.5955 - val_acc: 0.8000\n","Epoch 93/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.5780 - acc: 0.8556 - val_loss: 0.5904 - val_acc: 0.8000\n","Epoch 94/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.5727 - acc: 0.8556 - val_loss: 0.5859 - val_acc: 0.8000\n","Epoch 95/200\n","270/270 [==============================] - 0s 113us/step - loss: 0.5677 - acc: 0.8593 - val_loss: 0.5818 - val_acc: 0.8000\n","Epoch 96/200\n","270/270 [==============================] - 0s 116us/step - loss: 0.5628 - acc: 0.8593 - val_loss: 0.5774 - val_acc: 0.8000\n","Epoch 97/200\n","270/270 [==============================] - 0s 106us/step - loss: 0.5580 - acc: 0.8593 - val_loss: 0.5730 - val_acc: 0.8000\n","Epoch 98/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.5531 - acc: 0.8593 - val_loss: 0.5691 - val_acc: 0.8000\n","Epoch 99/200\n","270/270 [==============================] - 0s 124us/step - loss: 0.5486 - acc: 0.8593 - val_loss: 0.5649 - val_acc: 0.8000\n","Epoch 100/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.5438 - acc: 0.8593 - val_loss: 0.5610 - val_acc: 0.8000\n","Epoch 101/200\n","270/270 [==============================] - 0s 105us/step - loss: 0.5394 - acc: 0.8593 - val_loss: 0.5569 - val_acc: 0.8000\n","Epoch 102/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.5349 - acc: 0.8593 - val_loss: 0.5530 - val_acc: 0.8000\n","Epoch 103/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.5304 - acc: 0.8593 - val_loss: 0.5493 - val_acc: 0.8000\n","Epoch 104/200\n","270/270 [==============================] - 0s 102us/step - loss: 0.5261 - acc: 0.8593 - val_loss: 0.5455 - val_acc: 0.8000\n","Epoch 105/200\n","270/270 [==============================] - 0s 106us/step - loss: 0.5219 - acc: 0.8593 - val_loss: 0.5417 - val_acc: 0.8000\n","Epoch 106/200\n","270/270 [==============================] - 0s 117us/step - loss: 0.5177 - acc: 0.8593 - val_loss: 0.5382 - val_acc: 0.8000\n","Epoch 107/200\n","270/270 [==============================] - 0s 101us/step - loss: 0.5136 - acc: 0.8593 - val_loss: 0.5345 - val_acc: 0.8000\n","Epoch 108/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.5096 - acc: 0.8556 - val_loss: 0.5306 - val_acc: 0.8000\n","Epoch 109/200\n","270/270 [==============================] - 0s 99us/step - loss: 0.5058 - acc: 0.8556 - val_loss: 0.5268 - val_acc: 0.8000\n","Epoch 110/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.5017 - acc: 0.8556 - val_loss: 0.5238 - val_acc: 0.8000\n","Epoch 111/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.4981 - acc: 0.8556 - val_loss: 0.5208 - val_acc: 0.8000\n","Epoch 112/200\n","270/270 [==============================] - 0s 106us/step - loss: 0.4944 - acc: 0.8556 - val_loss: 0.5177 - val_acc: 0.8000\n","Epoch 113/200\n","270/270 [==============================] - 0s 158us/step - loss: 0.4908 - acc: 0.8556 - val_loss: 0.5146 - val_acc: 0.8000\n","Epoch 114/200\n","270/270 [==============================] - 0s 148us/step - loss: 0.4873 - acc: 0.8556 - val_loss: 0.5117 - val_acc: 0.8000\n","Epoch 115/200\n","270/270 [==============================] - 0s 116us/step - loss: 0.4838 - acc: 0.8556 - val_loss: 0.5090 - val_acc: 0.8000\n","Epoch 116/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.4803 - acc: 0.8556 - val_loss: 0.5058 - val_acc: 0.8000\n","Epoch 117/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.4772 - acc: 0.8556 - val_loss: 0.5022 - val_acc: 0.8000\n","Epoch 118/200\n","270/270 [==============================] - 0s 115us/step - loss: 0.4739 - acc: 0.8556 - val_loss: 0.4994 - val_acc: 0.8000\n","Epoch 119/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.4706 - acc: 0.8556 - val_loss: 0.4966 - val_acc: 0.8000\n","Epoch 120/200\n","270/270 [==============================] - 0s 124us/step - loss: 0.4675 - acc: 0.8556 - val_loss: 0.4937 - val_acc: 0.8000\n","Epoch 121/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.4645 - acc: 0.8556 - val_loss: 0.4911 - val_acc: 0.8000\n","Epoch 122/200\n","270/270 [==============================] - 0s 106us/step - loss: 0.4615 - acc: 0.8556 - val_loss: 0.4889 - val_acc: 0.8000\n","Epoch 123/200\n","270/270 [==============================] - 0s 105us/step - loss: 0.4586 - acc: 0.8593 - val_loss: 0.4863 - val_acc: 0.8000\n","Epoch 124/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.4556 - acc: 0.8556 - val_loss: 0.4838 - val_acc: 0.8000\n","Epoch 125/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.4529 - acc: 0.8556 - val_loss: 0.4815 - val_acc: 0.8000\n","Epoch 126/200\n","270/270 [==============================] - 0s 133us/step - loss: 0.4501 - acc: 0.8593 - val_loss: 0.4790 - val_acc: 0.8000\n","Epoch 127/200\n","270/270 [==============================] - 0s 145us/step - loss: 0.4474 - acc: 0.8556 - val_loss: 0.4770 - val_acc: 0.8000\n","Epoch 128/200\n","270/270 [==============================] - 0s 113us/step - loss: 0.4448 - acc: 0.8556 - val_loss: 0.4746 - val_acc: 0.8000\n","Epoch 129/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.4422 - acc: 0.8593 - val_loss: 0.4722 - val_acc: 0.8333\n","Epoch 130/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.4396 - acc: 0.8593 - val_loss: 0.4699 - val_acc: 0.8333\n","Epoch 131/200\n","270/270 [==============================] - 0s 119us/step - loss: 0.4371 - acc: 0.8593 - val_loss: 0.4677 - val_acc: 0.8333\n","Epoch 132/200\n","270/270 [==============================] - 0s 106us/step - loss: 0.4347 - acc: 0.8593 - val_loss: 0.4655 - val_acc: 0.8333\n","Epoch 133/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.4324 - acc: 0.8593 - val_loss: 0.4637 - val_acc: 0.8333\n","Epoch 134/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.4300 - acc: 0.8593 - val_loss: 0.4615 - val_acc: 0.8333\n","Epoch 135/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.4277 - acc: 0.8593 - val_loss: 0.4595 - val_acc: 0.8333\n","Epoch 136/200\n","270/270 [==============================] - 0s 102us/step - loss: 0.4255 - acc: 0.8593 - val_loss: 0.4577 - val_acc: 0.8333\n","Epoch 137/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.4232 - acc: 0.8593 - val_loss: 0.4563 - val_acc: 0.8333\n","Epoch 138/200\n","270/270 [==============================] - 0s 102us/step - loss: 0.4212 - acc: 0.8593 - val_loss: 0.4547 - val_acc: 0.8333\n","Epoch 139/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.4191 - acc: 0.8593 - val_loss: 0.4530 - val_acc: 0.8333\n","Epoch 140/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.4170 - acc: 0.8556 - val_loss: 0.4514 - val_acc: 0.8333\n","Epoch 141/200\n","270/270 [==============================] - 0s 105us/step - loss: 0.4151 - acc: 0.8556 - val_loss: 0.4496 - val_acc: 0.8333\n","Epoch 142/200\n","270/270 [==============================] - 0s 118us/step - loss: 0.4132 - acc: 0.8556 - val_loss: 0.4483 - val_acc: 0.8333\n","Epoch 143/200\n","270/270 [==============================] - 0s 117us/step - loss: 0.4113 - acc: 0.8556 - val_loss: 0.4465 - val_acc: 0.8333\n","Epoch 144/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.4094 - acc: 0.8556 - val_loss: 0.4452 - val_acc: 0.8333\n","Epoch 145/200\n","270/270 [==============================] - 0s 104us/step - loss: 0.4077 - acc: 0.8593 - val_loss: 0.4435 - val_acc: 0.8333\n","Epoch 146/200\n","270/270 [==============================] - 0s 155us/step - loss: 0.4058 - acc: 0.8593 - val_loss: 0.4418 - val_acc: 0.8333\n","Epoch 147/200\n","270/270 [==============================] - 0s 130us/step - loss: 0.4041 - acc: 0.8593 - val_loss: 0.4402 - val_acc: 0.8333\n","Epoch 148/200\n","270/270 [==============================] - 0s 105us/step - loss: 0.4023 - acc: 0.8556 - val_loss: 0.4386 - val_acc: 0.8333\n","Epoch 149/200\n","270/270 [==============================] - 0s 117us/step - loss: 0.4006 - acc: 0.8556 - val_loss: 0.4369 - val_acc: 0.8333\n","Epoch 150/200\n","270/270 [==============================] - 0s 122us/step - loss: 0.3990 - acc: 0.8556 - val_loss: 0.4353 - val_acc: 0.8333\n","Epoch 151/200\n","270/270 [==============================] - 0s 104us/step - loss: 0.3974 - acc: 0.8556 - val_loss: 0.4340 - val_acc: 0.8333\n","Epoch 152/200\n","270/270 [==============================] - 0s 105us/step - loss: 0.3958 - acc: 0.8556 - val_loss: 0.4327 - val_acc: 0.8333\n","Epoch 153/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.3943 - acc: 0.8556 - val_loss: 0.4315 - val_acc: 0.8333\n","Epoch 154/200\n","270/270 [==============================] - 0s 117us/step - loss: 0.3928 - acc: 0.8556 - val_loss: 0.4301 - val_acc: 0.8333\n","Epoch 155/200\n","270/270 [==============================] - 0s 105us/step - loss: 0.3913 - acc: 0.8556 - val_loss: 0.4288 - val_acc: 0.8333\n","Epoch 156/200\n","270/270 [==============================] - 0s 113us/step - loss: 0.3899 - acc: 0.8593 - val_loss: 0.4276 - val_acc: 0.8333\n","Epoch 157/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.3884 - acc: 0.8593 - val_loss: 0.4264 - val_acc: 0.8333\n","Epoch 158/200\n","270/270 [==============================] - 0s 104us/step - loss: 0.3871 - acc: 0.8593 - val_loss: 0.4253 - val_acc: 0.8333\n","Epoch 159/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.3858 - acc: 0.8593 - val_loss: 0.4245 - val_acc: 0.8333\n","Epoch 160/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.3844 - acc: 0.8593 - val_loss: 0.4232 - val_acc: 0.8333\n","Epoch 161/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.3831 - acc: 0.8593 - val_loss: 0.4220 - val_acc: 0.8333\n","Epoch 162/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.3818 - acc: 0.8593 - val_loss: 0.4208 - val_acc: 0.8333\n","Epoch 163/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.3806 - acc: 0.8593 - val_loss: 0.4197 - val_acc: 0.8333\n","Epoch 164/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.3793 - acc: 0.8593 - val_loss: 0.4186 - val_acc: 0.8667\n","Epoch 165/200\n","270/270 [==============================] - 0s 131us/step - loss: 0.3781 - acc: 0.8593 - val_loss: 0.4176 - val_acc: 0.8667\n","Epoch 166/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.3771 - acc: 0.8593 - val_loss: 0.4166 - val_acc: 0.8667\n","Epoch 167/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.3759 - acc: 0.8593 - val_loss: 0.4155 - val_acc: 0.8667\n","Epoch 168/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.3748 - acc: 0.8593 - val_loss: 0.4150 - val_acc: 0.8667\n","Epoch 169/200\n","270/270 [==============================] - 0s 101us/step - loss: 0.3737 - acc: 0.8630 - val_loss: 0.4138 - val_acc: 0.8667\n","Epoch 170/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.3727 - acc: 0.8630 - val_loss: 0.4128 - val_acc: 0.8667\n","Epoch 171/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.3715 - acc: 0.8630 - val_loss: 0.4116 - val_acc: 0.8667\n","Epoch 172/200\n","270/270 [==============================] - 0s 114us/step - loss: 0.3704 - acc: 0.8630 - val_loss: 0.4107 - val_acc: 0.8667\n","Epoch 173/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.3695 - acc: 0.8630 - val_loss: 0.4097 - val_acc: 0.8667\n","Epoch 174/200\n","270/270 [==============================] - 0s 122us/step - loss: 0.3685 - acc: 0.8630 - val_loss: 0.4089 - val_acc: 0.8667\n","Epoch 175/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.3674 - acc: 0.8630 - val_loss: 0.4079 - val_acc: 0.8667\n","Epoch 176/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.3665 - acc: 0.8630 - val_loss: 0.4068 - val_acc: 0.8667\n","Epoch 177/200\n","270/270 [==============================] - 0s 105us/step - loss: 0.3656 - acc: 0.8667 - val_loss: 0.4058 - val_acc: 0.8667\n","Epoch 178/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.3646 - acc: 0.8667 - val_loss: 0.4053 - val_acc: 0.8667\n","Epoch 179/200\n","270/270 [==============================] - 0s 146us/step - loss: 0.3637 - acc: 0.8667 - val_loss: 0.4048 - val_acc: 0.8667\n","Epoch 180/200\n","270/270 [==============================] - 0s 126us/step - loss: 0.3628 - acc: 0.8667 - val_loss: 0.4037 - val_acc: 0.8667\n","Epoch 181/200\n","270/270 [==============================] - 0s 125us/step - loss: 0.3619 - acc: 0.8667 - val_loss: 0.4033 - val_acc: 0.8667\n","Epoch 182/200\n","270/270 [==============================] - 0s 113us/step - loss: 0.3610 - acc: 0.8667 - val_loss: 0.4026 - val_acc: 0.8667\n","Epoch 183/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.3601 - acc: 0.8667 - val_loss: 0.4020 - val_acc: 0.8667\n","Epoch 184/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.3593 - acc: 0.8667 - val_loss: 0.4014 - val_acc: 0.8667\n","Epoch 185/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.3585 - acc: 0.8667 - val_loss: 0.4008 - val_acc: 0.8667\n","Epoch 186/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.3577 - acc: 0.8667 - val_loss: 0.4006 - val_acc: 0.8667\n","Epoch 187/200\n","270/270 [==============================] - 0s 104us/step - loss: 0.3569 - acc: 0.8667 - val_loss: 0.4001 - val_acc: 0.8667\n","Epoch 188/200\n","270/270 [==============================] - 0s 106us/step - loss: 0.3561 - acc: 0.8667 - val_loss: 0.3993 - val_acc: 0.8667\n","Epoch 189/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.3553 - acc: 0.8667 - val_loss: 0.3989 - val_acc: 0.8667\n","Epoch 190/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.3545 - acc: 0.8667 - val_loss: 0.3986 - val_acc: 0.8667\n","Epoch 191/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.3539 - acc: 0.8667 - val_loss: 0.3983 - val_acc: 0.8667\n","Epoch 192/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.3532 - acc: 0.8667 - val_loss: 0.3982 - val_acc: 0.8667\n","Epoch 193/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.3525 - acc: 0.8667 - val_loss: 0.3973 - val_acc: 0.8667\n","Epoch 194/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.3518 - acc: 0.8667 - val_loss: 0.3968 - val_acc: 0.8667\n","Epoch 195/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.3510 - acc: 0.8667 - val_loss: 0.3963 - val_acc: 0.8667\n","Epoch 196/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.3504 - acc: 0.8667 - val_loss: 0.3958 - val_acc: 0.9000\n","Epoch 197/200\n","270/270 [==============================] - 0s 116us/step - loss: 0.3497 - acc: 0.8667 - val_loss: 0.3955 - val_acc: 0.9000\n","Epoch 198/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.3491 - acc: 0.8667 - val_loss: 0.3949 - val_acc: 0.9000\n","Epoch 199/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.3484 - acc: 0.8667 - val_loss: 0.3948 - val_acc: 0.9000\n","Epoch 200/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.3479 - acc: 0.8667 - val_loss: 0.3939 - val_acc: 0.9000\n","Accuracy :0.900\n","[[ 7  1  2]\n"," [ 0 10  0]\n"," [ 0  0 10]]\n","---------------------------------------------------------------------------------------------------------------------------\n","NH = 50\n","Nepochs = 200\n","N = 200\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_5 (InputLayer)         (None, 2)                 0         \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 50)                150       \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 3)                 153       \n","=================================================================\n","Total params: 303\n","Trainable params: 303\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 540 samples, validate on 60 samples\n","Epoch 1/200\n","540/540 [==============================] - 0s 564us/step - loss: 1.1017 - acc: 0.3222 - val_loss: 1.0949 - val_acc: 0.3667\n","Epoch 2/200\n","540/540 [==============================] - 0s 92us/step - loss: 1.0966 - acc: 0.3296 - val_loss: 1.0908 - val_acc: 0.3667\n","Epoch 3/200\n","540/540 [==============================] - 0s 91us/step - loss: 1.0918 - acc: 0.3296 - val_loss: 1.0867 - val_acc: 0.3667\n","Epoch 4/200\n","540/540 [==============================] - 0s 94us/step - loss: 1.0868 - acc: 0.4370 - val_loss: 1.0834 - val_acc: 0.5333\n","Epoch 5/200\n","540/540 [==============================] - 0s 94us/step - loss: 1.0815 - acc: 0.5833 - val_loss: 1.0786 - val_acc: 0.6000\n","Epoch 6/200\n","540/540 [==============================] - 0s 93us/step - loss: 1.0761 - acc: 0.7037 - val_loss: 1.0739 - val_acc: 0.6833\n","Epoch 7/200\n","540/540 [==============================] - 0s 95us/step - loss: 1.0703 - acc: 0.7685 - val_loss: 1.0685 - val_acc: 0.7167\n","Epoch 8/200\n","540/540 [==============================] - 0s 93us/step - loss: 1.0642 - acc: 0.8148 - val_loss: 1.0626 - val_acc: 0.8000\n","Epoch 9/200\n","540/540 [==============================] - 0s 101us/step - loss: 1.0575 - acc: 0.8352 - val_loss: 1.0563 - val_acc: 0.8167\n","Epoch 10/200\n","540/540 [==============================] - 0s 101us/step - loss: 1.0504 - acc: 0.8370 - val_loss: 1.0496 - val_acc: 0.8167\n","Epoch 11/200\n","540/540 [==============================] - 0s 95us/step - loss: 1.0428 - acc: 0.8333 - val_loss: 1.0422 - val_acc: 0.8167\n","Epoch 12/200\n","540/540 [==============================] - 0s 100us/step - loss: 1.0347 - acc: 0.8296 - val_loss: 1.0340 - val_acc: 0.8167\n","Epoch 13/200\n","540/540 [==============================] - 0s 92us/step - loss: 1.0259 - acc: 0.8370 - val_loss: 1.0247 - val_acc: 0.8333\n","Epoch 14/200\n","540/540 [==============================] - 0s 89us/step - loss: 1.0168 - acc: 0.8481 - val_loss: 1.0157 - val_acc: 0.8333\n","Epoch 15/200\n","540/540 [==============================] - 0s 94us/step - loss: 1.0069 - acc: 0.8463 - val_loss: 1.0055 - val_acc: 0.8333\n","Epoch 16/200\n","540/540 [==============================] - 0s 132us/step - loss: 0.9966 - acc: 0.8463 - val_loss: 0.9957 - val_acc: 0.8333\n","Epoch 17/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.9858 - acc: 0.8519 - val_loss: 0.9848 - val_acc: 0.8333\n","Epoch 18/200\n","540/540 [==============================] - 0s 90us/step - loss: 0.9746 - acc: 0.8519 - val_loss: 0.9736 - val_acc: 0.8333\n","Epoch 19/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.9629 - acc: 0.8519 - val_loss: 0.9611 - val_acc: 0.8333\n","Epoch 20/200\n","540/540 [==============================] - 0s 90us/step - loss: 0.9506 - acc: 0.8537 - val_loss: 0.9494 - val_acc: 0.8333\n","Epoch 21/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.9384 - acc: 0.8537 - val_loss: 0.9366 - val_acc: 0.8333\n","Epoch 22/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.9255 - acc: 0.8481 - val_loss: 0.9235 - val_acc: 0.8333\n","Epoch 23/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.9126 - acc: 0.8444 - val_loss: 0.9099 - val_acc: 0.8333\n","Epoch 24/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.8993 - acc: 0.8463 - val_loss: 0.8962 - val_acc: 0.8333\n","Epoch 25/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.8859 - acc: 0.8481 - val_loss: 0.8829 - val_acc: 0.8333\n","Epoch 26/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.8723 - acc: 0.8463 - val_loss: 0.8690 - val_acc: 0.8333\n","Epoch 27/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.8587 - acc: 0.8444 - val_loss: 0.8550 - val_acc: 0.8333\n","Epoch 28/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.8450 - acc: 0.8463 - val_loss: 0.8412 - val_acc: 0.8333\n","Epoch 29/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.8311 - acc: 0.8463 - val_loss: 0.8269 - val_acc: 0.8333\n","Epoch 30/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.8175 - acc: 0.8481 - val_loss: 0.8127 - val_acc: 0.8500\n","Epoch 31/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.8037 - acc: 0.8500 - val_loss: 0.7988 - val_acc: 0.8500\n","Epoch 32/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.7901 - acc: 0.8500 - val_loss: 0.7848 - val_acc: 0.8500\n","Epoch 33/200\n","540/540 [==============================] - 0s 103us/step - loss: 0.7768 - acc: 0.8481 - val_loss: 0.7713 - val_acc: 0.8500\n","Epoch 34/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.7636 - acc: 0.8463 - val_loss: 0.7579 - val_acc: 0.8500\n","Epoch 35/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.7503 - acc: 0.8463 - val_loss: 0.7442 - val_acc: 0.8500\n","Epoch 36/200\n","540/540 [==============================] - 0s 136us/step - loss: 0.7377 - acc: 0.8481 - val_loss: 0.7310 - val_acc: 0.8500\n","Epoch 37/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.7248 - acc: 0.8444 - val_loss: 0.7184 - val_acc: 0.8500\n","Epoch 38/200\n","540/540 [==============================] - 0s 105us/step - loss: 0.7124 - acc: 0.8426 - val_loss: 0.7053 - val_acc: 0.8500\n","Epoch 39/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.7004 - acc: 0.8389 - val_loss: 0.6926 - val_acc: 0.8500\n","Epoch 40/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.6883 - acc: 0.8426 - val_loss: 0.6810 - val_acc: 0.8500\n","Epoch 41/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.6767 - acc: 0.8426 - val_loss: 0.6694 - val_acc: 0.8500\n","Epoch 42/200\n","540/540 [==============================] - 0s 89us/step - loss: 0.6655 - acc: 0.8370 - val_loss: 0.6576 - val_acc: 0.8500\n","Epoch 43/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.6544 - acc: 0.8370 - val_loss: 0.6461 - val_acc: 0.8500\n","Epoch 44/200\n","540/540 [==============================] - 0s 104us/step - loss: 0.6437 - acc: 0.8370 - val_loss: 0.6354 - val_acc: 0.8500\n","Epoch 45/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.6333 - acc: 0.8370 - val_loss: 0.6245 - val_acc: 0.8500\n","Epoch 46/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.6231 - acc: 0.8370 - val_loss: 0.6147 - val_acc: 0.8500\n","Epoch 47/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.6134 - acc: 0.8370 - val_loss: 0.6048 - val_acc: 0.8500\n","Epoch 48/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.6039 - acc: 0.8370 - val_loss: 0.5949 - val_acc: 0.8500\n","Epoch 49/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.5946 - acc: 0.8370 - val_loss: 0.5855 - val_acc: 0.8500\n","Epoch 50/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.5858 - acc: 0.8370 - val_loss: 0.5764 - val_acc: 0.8500\n","Epoch 51/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.5772 - acc: 0.8370 - val_loss: 0.5672 - val_acc: 0.8500\n","Epoch 52/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.5687 - acc: 0.8370 - val_loss: 0.5590 - val_acc: 0.8500\n","Epoch 53/200\n","540/540 [==============================] - 0s 103us/step - loss: 0.5608 - acc: 0.8370 - val_loss: 0.5505 - val_acc: 0.8500\n","Epoch 54/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.5531 - acc: 0.8370 - val_loss: 0.5425 - val_acc: 0.8500\n","Epoch 55/200\n","540/540 [==============================] - 0s 113us/step - loss: 0.5456 - acc: 0.8370 - val_loss: 0.5351 - val_acc: 0.8500\n","Epoch 56/200\n","540/540 [==============================] - 0s 111us/step - loss: 0.5383 - acc: 0.8389 - val_loss: 0.5280 - val_acc: 0.8500\n","Epoch 57/200\n","540/540 [==============================] - 0s 102us/step - loss: 0.5314 - acc: 0.8370 - val_loss: 0.5204 - val_acc: 0.8500\n","Epoch 58/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.5247 - acc: 0.8407 - val_loss: 0.5139 - val_acc: 0.8500\n","Epoch 59/200\n","540/540 [==============================] - 0s 98us/step - loss: 0.5183 - acc: 0.8407 - val_loss: 0.5069 - val_acc: 0.8500\n","Epoch 60/200\n","540/540 [==============================] - 0s 106us/step - loss: 0.5120 - acc: 0.8407 - val_loss: 0.5006 - val_acc: 0.8500\n","Epoch 61/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.5061 - acc: 0.8444 - val_loss: 0.4948 - val_acc: 0.8500\n","Epoch 62/200\n","540/540 [==============================] - 0s 90us/step - loss: 0.5005 - acc: 0.8444 - val_loss: 0.4890 - val_acc: 0.8500\n","Epoch 63/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.4948 - acc: 0.8444 - val_loss: 0.4831 - val_acc: 0.8500\n","Epoch 64/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.4895 - acc: 0.8444 - val_loss: 0.4776 - val_acc: 0.8500\n","Epoch 65/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.4845 - acc: 0.8444 - val_loss: 0.4726 - val_acc: 0.8500\n","Epoch 66/200\n","540/540 [==============================] - 0s 100us/step - loss: 0.4795 - acc: 0.8444 - val_loss: 0.4673 - val_acc: 0.8500\n","Epoch 67/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.4749 - acc: 0.8444 - val_loss: 0.4627 - val_acc: 0.8500\n","Epoch 68/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.4704 - acc: 0.8444 - val_loss: 0.4579 - val_acc: 0.8500\n","Epoch 69/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.4661 - acc: 0.8444 - val_loss: 0.4536 - val_acc: 0.8500\n","Epoch 70/200\n","540/540 [==============================] - 0s 99us/step - loss: 0.4619 - acc: 0.8444 - val_loss: 0.4494 - val_acc: 0.8500\n","Epoch 71/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.4579 - acc: 0.8481 - val_loss: 0.4453 - val_acc: 0.8500\n","Epoch 72/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.4542 - acc: 0.8481 - val_loss: 0.4413 - val_acc: 0.8500\n","Epoch 73/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.4504 - acc: 0.8481 - val_loss: 0.4378 - val_acc: 0.8500\n","Epoch 74/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.4470 - acc: 0.8481 - val_loss: 0.4337 - val_acc: 0.8500\n","Epoch 75/200\n","540/540 [==============================] - 0s 134us/step - loss: 0.4435 - acc: 0.8481 - val_loss: 0.4305 - val_acc: 0.8500\n","Epoch 76/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.4404 - acc: 0.8481 - val_loss: 0.4274 - val_acc: 0.8500\n","Epoch 77/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.4372 - acc: 0.8500 - val_loss: 0.4240 - val_acc: 0.8500\n","Epoch 78/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.4342 - acc: 0.8500 - val_loss: 0.4211 - val_acc: 0.8500\n","Epoch 79/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.4313 - acc: 0.8500 - val_loss: 0.4179 - val_acc: 0.8500\n","Epoch 80/200\n","540/540 [==============================] - 0s 110us/step - loss: 0.4286 - acc: 0.8500 - val_loss: 0.4153 - val_acc: 0.8500\n","Epoch 81/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.4258 - acc: 0.8481 - val_loss: 0.4125 - val_acc: 0.8500\n","Epoch 82/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.4233 - acc: 0.8481 - val_loss: 0.4101 - val_acc: 0.8500\n","Epoch 83/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.4208 - acc: 0.8481 - val_loss: 0.4074 - val_acc: 0.8500\n","Epoch 84/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.4185 - acc: 0.8463 - val_loss: 0.4046 - val_acc: 0.8500\n","Epoch 85/200\n","540/540 [==============================] - 0s 90us/step - loss: 0.4162 - acc: 0.8481 - val_loss: 0.4026 - val_acc: 0.8500\n","Epoch 86/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.4140 - acc: 0.8500 - val_loss: 0.4003 - val_acc: 0.8500\n","Epoch 87/200\n","540/540 [==============================] - 0s 90us/step - loss: 0.4119 - acc: 0.8500 - val_loss: 0.3978 - val_acc: 0.8500\n","Epoch 88/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.4099 - acc: 0.8500 - val_loss: 0.3961 - val_acc: 0.8500\n","Epoch 89/200\n","540/540 [==============================] - 0s 98us/step - loss: 0.4079 - acc: 0.8500 - val_loss: 0.3937 - val_acc: 0.8500\n","Epoch 90/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.4060 - acc: 0.8500 - val_loss: 0.3915 - val_acc: 0.8500\n","Epoch 91/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.4041 - acc: 0.8500 - val_loss: 0.3896 - val_acc: 0.8500\n","Epoch 92/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.4024 - acc: 0.8500 - val_loss: 0.3874 - val_acc: 0.8500\n","Epoch 93/200\n","540/540 [==============================] - 0s 102us/step - loss: 0.4007 - acc: 0.8500 - val_loss: 0.3861 - val_acc: 0.8500\n","Epoch 94/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3990 - acc: 0.8519 - val_loss: 0.3844 - val_acc: 0.8500\n","Epoch 95/200\n","540/540 [==============================] - 0s 125us/step - loss: 0.3974 - acc: 0.8519 - val_loss: 0.3827 - val_acc: 0.8500\n","Epoch 96/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3958 - acc: 0.8537 - val_loss: 0.3809 - val_acc: 0.8500\n","Epoch 97/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3943 - acc: 0.8537 - val_loss: 0.3794 - val_acc: 0.8500\n","Epoch 98/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3929 - acc: 0.8537 - val_loss: 0.3777 - val_acc: 0.8500\n","Epoch 99/200\n","540/540 [==============================] - 0s 99us/step - loss: 0.3914 - acc: 0.8519 - val_loss: 0.3762 - val_acc: 0.8500\n","Epoch 100/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.3900 - acc: 0.8519 - val_loss: 0.3749 - val_acc: 0.8500\n","Epoch 101/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3887 - acc: 0.8519 - val_loss: 0.3735 - val_acc: 0.8500\n","Epoch 102/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3874 - acc: 0.8519 - val_loss: 0.3719 - val_acc: 0.8500\n","Epoch 103/200\n","540/540 [==============================] - 0s 89us/step - loss: 0.3862 - acc: 0.8519 - val_loss: 0.3705 - val_acc: 0.8500\n","Epoch 104/200\n","540/540 [==============================] - 0s 99us/step - loss: 0.3850 - acc: 0.8519 - val_loss: 0.3687 - val_acc: 0.8667\n","Epoch 105/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.3839 - acc: 0.8537 - val_loss: 0.3680 - val_acc: 0.8500\n","Epoch 106/200\n","540/540 [==============================] - 0s 98us/step - loss: 0.3826 - acc: 0.8556 - val_loss: 0.3663 - val_acc: 0.8833\n","Epoch 107/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3815 - acc: 0.8556 - val_loss: 0.3648 - val_acc: 0.8833\n","Epoch 108/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3805 - acc: 0.8556 - val_loss: 0.3643 - val_acc: 0.8833\n","Epoch 109/200\n","540/540 [==============================] - 0s 90us/step - loss: 0.3794 - acc: 0.8556 - val_loss: 0.3625 - val_acc: 0.8833\n","Epoch 110/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.3783 - acc: 0.8556 - val_loss: 0.3615 - val_acc: 0.8833\n","Epoch 111/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3773 - acc: 0.8556 - val_loss: 0.3602 - val_acc: 0.8833\n","Epoch 112/200\n","540/540 [==============================] - 0s 102us/step - loss: 0.3763 - acc: 0.8556 - val_loss: 0.3592 - val_acc: 0.8833\n","Epoch 113/200\n","540/540 [==============================] - 0s 90us/step - loss: 0.3754 - acc: 0.8556 - val_loss: 0.3579 - val_acc: 0.8833\n","Epoch 114/200\n","540/540 [==============================] - 0s 104us/step - loss: 0.3744 - acc: 0.8556 - val_loss: 0.3568 - val_acc: 0.8833\n","Epoch 115/200\n","540/540 [==============================] - 0s 110us/step - loss: 0.3736 - acc: 0.8556 - val_loss: 0.3558 - val_acc: 0.8833\n","Epoch 116/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3726 - acc: 0.8556 - val_loss: 0.3542 - val_acc: 0.8833\n","Epoch 117/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3718 - acc: 0.8556 - val_loss: 0.3530 - val_acc: 0.8833\n","Epoch 118/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3709 - acc: 0.8556 - val_loss: 0.3526 - val_acc: 0.8833\n","Epoch 119/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.3700 - acc: 0.8556 - val_loss: 0.3517 - val_acc: 0.8833\n","Epoch 120/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3692 - acc: 0.8556 - val_loss: 0.3506 - val_acc: 0.8833\n","Epoch 121/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3685 - acc: 0.8556 - val_loss: 0.3497 - val_acc: 0.8833\n","Epoch 122/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3677 - acc: 0.8556 - val_loss: 0.3485 - val_acc: 0.8833\n","Epoch 123/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3669 - acc: 0.8556 - val_loss: 0.3473 - val_acc: 0.8833\n","Epoch 124/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3663 - acc: 0.8556 - val_loss: 0.3468 - val_acc: 0.8833\n","Epoch 125/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3657 - acc: 0.8537 - val_loss: 0.3452 - val_acc: 0.8833\n","Epoch 126/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3648 - acc: 0.8537 - val_loss: 0.3450 - val_acc: 0.8833\n","Epoch 127/200\n","540/540 [==============================] - 0s 104us/step - loss: 0.3640 - acc: 0.8556 - val_loss: 0.3440 - val_acc: 0.8833\n","Epoch 128/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3633 - acc: 0.8556 - val_loss: 0.3433 - val_acc: 0.8833\n","Epoch 129/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3627 - acc: 0.8556 - val_loss: 0.3422 - val_acc: 0.8833\n","Epoch 130/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3620 - acc: 0.8556 - val_loss: 0.3415 - val_acc: 0.8833\n","Epoch 131/200\n","540/540 [==============================] - 0s 90us/step - loss: 0.3614 - acc: 0.8556 - val_loss: 0.3402 - val_acc: 0.8833\n","Epoch 132/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3607 - acc: 0.8556 - val_loss: 0.3393 - val_acc: 0.8833\n","Epoch 133/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3602 - acc: 0.8556 - val_loss: 0.3388 - val_acc: 0.8833\n","Epoch 134/200\n","540/540 [==============================] - 0s 111us/step - loss: 0.3595 - acc: 0.8556 - val_loss: 0.3380 - val_acc: 0.8833\n","Epoch 135/200\n","540/540 [==============================] - 0s 104us/step - loss: 0.3589 - acc: 0.8556 - val_loss: 0.3374 - val_acc: 0.8833\n","Epoch 136/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.3584 - acc: 0.8537 - val_loss: 0.3361 - val_acc: 0.8833\n","Epoch 137/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3578 - acc: 0.8556 - val_loss: 0.3355 - val_acc: 0.8833\n","Epoch 138/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.3573 - acc: 0.8537 - val_loss: 0.3344 - val_acc: 0.8833\n","Epoch 139/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3568 - acc: 0.8556 - val_loss: 0.3344 - val_acc: 0.8833\n","Epoch 140/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.3561 - acc: 0.8556 - val_loss: 0.3335 - val_acc: 0.8833\n","Epoch 141/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.3556 - acc: 0.8574 - val_loss: 0.3326 - val_acc: 0.8833\n","Epoch 142/200\n","540/540 [==============================] - 0s 125us/step - loss: 0.3551 - acc: 0.8574 - val_loss: 0.3318 - val_acc: 0.8833\n","Epoch 143/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3546 - acc: 0.8574 - val_loss: 0.3312 - val_acc: 0.8833\n","Epoch 144/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.3541 - acc: 0.8574 - val_loss: 0.3301 - val_acc: 0.8833\n","Epoch 145/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3536 - acc: 0.8574 - val_loss: 0.3291 - val_acc: 0.8833\n","Epoch 146/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3531 - acc: 0.8574 - val_loss: 0.3289 - val_acc: 0.8833\n","Epoch 147/200\n","540/540 [==============================] - 0s 101us/step - loss: 0.3527 - acc: 0.8574 - val_loss: 0.3288 - val_acc: 0.8833\n","Epoch 148/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3522 - acc: 0.8574 - val_loss: 0.3279 - val_acc: 0.8833\n","Epoch 149/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3518 - acc: 0.8574 - val_loss: 0.3277 - val_acc: 0.8833\n","Epoch 150/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3512 - acc: 0.8574 - val_loss: 0.3265 - val_acc: 0.8833\n","Epoch 151/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3508 - acc: 0.8574 - val_loss: 0.3257 - val_acc: 0.8833\n","Epoch 152/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3505 - acc: 0.8574 - val_loss: 0.3248 - val_acc: 0.8833\n","Epoch 153/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3500 - acc: 0.8574 - val_loss: 0.3248 - val_acc: 0.8833\n","Epoch 154/200\n","540/540 [==============================] - 0s 140us/step - loss: 0.3497 - acc: 0.8574 - val_loss: 0.3238 - val_acc: 0.9000\n","Epoch 155/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3491 - acc: 0.8593 - val_loss: 0.3233 - val_acc: 0.9000\n","Epoch 156/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3487 - acc: 0.8593 - val_loss: 0.3230 - val_acc: 0.9000\n","Epoch 157/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.3483 - acc: 0.8593 - val_loss: 0.3223 - val_acc: 0.9000\n","Epoch 158/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.3480 - acc: 0.8611 - val_loss: 0.3221 - val_acc: 0.9000\n","Epoch 159/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3477 - acc: 0.8611 - val_loss: 0.3211 - val_acc: 0.9000\n","Epoch 160/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3473 - acc: 0.8611 - val_loss: 0.3204 - val_acc: 0.9000\n","Epoch 161/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.3469 - acc: 0.8611 - val_loss: 0.3200 - val_acc: 0.9000\n","Epoch 162/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3465 - acc: 0.8611 - val_loss: 0.3193 - val_acc: 0.9000\n","Epoch 163/200\n","540/540 [==============================] - 0s 89us/step - loss: 0.3462 - acc: 0.8611 - val_loss: 0.3190 - val_acc: 0.9000\n","Epoch 164/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3458 - acc: 0.8611 - val_loss: 0.3186 - val_acc: 0.9000\n","Epoch 165/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.3456 - acc: 0.8556 - val_loss: 0.3180 - val_acc: 0.9000\n","Epoch 166/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3452 - acc: 0.8574 - val_loss: 0.3173 - val_acc: 0.9000\n","Epoch 167/200\n","540/540 [==============================] - 0s 104us/step - loss: 0.3448 - acc: 0.8593 - val_loss: 0.3169 - val_acc: 0.9000\n","Epoch 168/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.3446 - acc: 0.8593 - val_loss: 0.3170 - val_acc: 0.9167\n","Epoch 169/200\n","540/540 [==============================] - 0s 98us/step - loss: 0.3442 - acc: 0.8611 - val_loss: 0.3162 - val_acc: 0.9167\n","Epoch 170/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.3440 - acc: 0.8593 - val_loss: 0.3163 - val_acc: 0.9167\n","Epoch 171/200\n","540/540 [==============================] - 0s 90us/step - loss: 0.3436 - acc: 0.8611 - val_loss: 0.3150 - val_acc: 0.9167\n","Epoch 172/200\n","540/540 [==============================] - 0s 103us/step - loss: 0.3433 - acc: 0.8611 - val_loss: 0.3146 - val_acc: 0.9167\n","Epoch 173/200\n","540/540 [==============================] - 0s 141us/step - loss: 0.3431 - acc: 0.8630 - val_loss: 0.3137 - val_acc: 0.9167\n","Epoch 174/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3428 - acc: 0.8593 - val_loss: 0.3136 - val_acc: 0.9167\n","Epoch 175/200\n","540/540 [==============================] - 0s 90us/step - loss: 0.3426 - acc: 0.8630 - val_loss: 0.3124 - val_acc: 0.9167\n","Epoch 176/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.3422 - acc: 0.8630 - val_loss: 0.3127 - val_acc: 0.9167\n","Epoch 177/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3419 - acc: 0.8611 - val_loss: 0.3128 - val_acc: 0.9167\n","Epoch 178/200\n","540/540 [==============================] - 0s 90us/step - loss: 0.3416 - acc: 0.8611 - val_loss: 0.3120 - val_acc: 0.9167\n","Epoch 179/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3414 - acc: 0.8611 - val_loss: 0.3119 - val_acc: 0.9167\n","Epoch 180/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.3411 - acc: 0.8630 - val_loss: 0.3108 - val_acc: 0.9167\n","Epoch 181/200\n","540/540 [==============================] - 0s 98us/step - loss: 0.3409 - acc: 0.8630 - val_loss: 0.3108 - val_acc: 0.9167\n","Epoch 182/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.3405 - acc: 0.8630 - val_loss: 0.3103 - val_acc: 0.9167\n","Epoch 183/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3403 - acc: 0.8611 - val_loss: 0.3101 - val_acc: 0.9167\n","Epoch 184/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.3400 - acc: 0.8611 - val_loss: 0.3093 - val_acc: 0.9167\n","Epoch 185/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3399 - acc: 0.8611 - val_loss: 0.3094 - val_acc: 0.9167\n","Epoch 186/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3396 - acc: 0.8611 - val_loss: 0.3085 - val_acc: 0.9167\n","Epoch 187/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.3394 - acc: 0.8611 - val_loss: 0.3083 - val_acc: 0.9167\n","Epoch 188/200\n","540/540 [==============================] - 0s 100us/step - loss: 0.3392 - acc: 0.8611 - val_loss: 0.3079 - val_acc: 0.9167\n","Epoch 189/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3390 - acc: 0.8593 - val_loss: 0.3075 - val_acc: 0.9167\n","Epoch 190/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3387 - acc: 0.8574 - val_loss: 0.3076 - val_acc: 0.9167\n","Epoch 191/200\n","540/540 [==============================] - 0s 89us/step - loss: 0.3385 - acc: 0.8574 - val_loss: 0.3074 - val_acc: 0.9167\n","Epoch 192/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.3382 - acc: 0.8574 - val_loss: 0.3070 - val_acc: 0.9167\n","Epoch 193/200\n","540/540 [==============================] - 0s 142us/step - loss: 0.3380 - acc: 0.8574 - val_loss: 0.3067 - val_acc: 0.9167\n","Epoch 194/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.3379 - acc: 0.8574 - val_loss: 0.3061 - val_acc: 0.9167\n","Epoch 195/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3378 - acc: 0.8574 - val_loss: 0.3067 - val_acc: 0.9167\n","Epoch 196/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3374 - acc: 0.8574 - val_loss: 0.3058 - val_acc: 0.9167\n","Epoch 197/200\n","540/540 [==============================] - 0s 88us/step - loss: 0.3373 - acc: 0.8574 - val_loss: 0.3054 - val_acc: 0.9167\n","Epoch 198/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3371 - acc: 0.8593 - val_loss: 0.3050 - val_acc: 0.9167\n","Epoch 199/200\n","540/540 [==============================] - 0s 106us/step - loss: 0.3369 - acc: 0.8574 - val_loss: 0.3044 - val_acc: 0.9167\n","Epoch 200/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3367 - acc: 0.8574 - val_loss: 0.3042 - val_acc: 0.9167\n","Accuracy :0.967\n","[[10  0  0]\n"," [ 0 10  0]\n"," [ 1  0  9]]\n","---------------------------------------------------------------------------------------------------------------------------\n","NH = 100\n","Nepochs = 100\n","N = 100\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_6 (InputLayer)         (None, 2)                 0         \n","_________________________________________________________________\n","dense_11 (Dense)             (None, 100)               300       \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 3)                 303       \n","=================================================================\n","Total params: 603\n","Trainable params: 603\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 270 samples, validate on 30 samples\n","Epoch 1/100\n","270/270 [==============================] - 0s 1ms/step - loss: 1.0973 - acc: 0.3407 - val_loss: 1.0949 - val_acc: 0.4667\n","Epoch 2/100\n","270/270 [==============================] - 0s 116us/step - loss: 1.0954 - acc: 0.4963 - val_loss: 1.0932 - val_acc: 0.6667\n","Epoch 3/100\n","270/270 [==============================] - 0s 111us/step - loss: 1.0937 - acc: 0.6185 - val_loss: 1.0919 - val_acc: 0.8667\n","Epoch 4/100\n","270/270 [==============================] - 0s 110us/step - loss: 1.0920 - acc: 0.8741 - val_loss: 1.0904 - val_acc: 0.9000\n","Epoch 5/100\n","270/270 [==============================] - 0s 110us/step - loss: 1.0903 - acc: 0.9148 - val_loss: 1.0886 - val_acc: 0.9333\n","Epoch 6/100\n","270/270 [==============================] - 0s 117us/step - loss: 1.0886 - acc: 0.9037 - val_loss: 1.0868 - val_acc: 0.8333\n","Epoch 7/100\n","270/270 [==============================] - 0s 109us/step - loss: 1.0868 - acc: 0.8889 - val_loss: 1.0849 - val_acc: 0.8333\n","Epoch 8/100\n","270/270 [==============================] - 0s 111us/step - loss: 1.0848 - acc: 0.8926 - val_loss: 1.0829 - val_acc: 0.8333\n","Epoch 9/100\n","270/270 [==============================] - 0s 109us/step - loss: 1.0829 - acc: 0.8815 - val_loss: 1.0807 - val_acc: 0.8333\n","Epoch 10/100\n","270/270 [==============================] - 0s 110us/step - loss: 1.0808 - acc: 0.8741 - val_loss: 1.0783 - val_acc: 0.8333\n","Epoch 11/100\n","270/270 [==============================] - 0s 116us/step - loss: 1.0786 - acc: 0.8815 - val_loss: 1.0759 - val_acc: 0.8333\n","Epoch 12/100\n","270/270 [==============================] - 0s 117us/step - loss: 1.0763 - acc: 0.8889 - val_loss: 1.0734 - val_acc: 0.8333\n","Epoch 13/100\n","270/270 [==============================] - 0s 111us/step - loss: 1.0739 - acc: 0.8889 - val_loss: 1.0707 - val_acc: 0.8333\n","Epoch 14/100\n","270/270 [==============================] - 0s 109us/step - loss: 1.0713 - acc: 0.8778 - val_loss: 1.0679 - val_acc: 0.8333\n","Epoch 15/100\n","270/270 [==============================] - 0s 112us/step - loss: 1.0685 - acc: 0.8889 - val_loss: 1.0648 - val_acc: 0.8333\n","Epoch 16/100\n","270/270 [==============================] - 0s 111us/step - loss: 1.0657 - acc: 0.8926 - val_loss: 1.0615 - val_acc: 0.8333\n","Epoch 17/100\n","270/270 [==============================] - 0s 111us/step - loss: 1.0627 - acc: 0.9000 - val_loss: 1.0579 - val_acc: 0.9000\n","Epoch 18/100\n","270/270 [==============================] - 0s 115us/step - loss: 1.0594 - acc: 0.8963 - val_loss: 1.0546 - val_acc: 0.8333\n","Epoch 19/100\n","270/270 [==============================] - 0s 115us/step - loss: 1.0560 - acc: 0.9037 - val_loss: 1.0507 - val_acc: 0.8333\n","Epoch 20/100\n","270/270 [==============================] - 0s 109us/step - loss: 1.0525 - acc: 0.9074 - val_loss: 1.0470 - val_acc: 0.8667\n","Epoch 21/100\n","270/270 [==============================] - 0s 112us/step - loss: 1.0486 - acc: 0.8926 - val_loss: 1.0430 - val_acc: 0.8333\n","Epoch 22/100\n","270/270 [==============================] - 0s 109us/step - loss: 1.0447 - acc: 0.8926 - val_loss: 1.0383 - val_acc: 0.9000\n","Epoch 23/100\n","270/270 [==============================] - 0s 109us/step - loss: 1.0404 - acc: 0.8926 - val_loss: 1.0337 - val_acc: 0.9000\n","Epoch 24/100\n","270/270 [==============================] - 0s 107us/step - loss: 1.0361 - acc: 0.8926 - val_loss: 1.0292 - val_acc: 0.9333\n","Epoch 25/100\n","270/270 [==============================] - 0s 113us/step - loss: 1.0316 - acc: 0.8815 - val_loss: 1.0243 - val_acc: 0.9333\n","Epoch 26/100\n","270/270 [==============================] - 0s 112us/step - loss: 1.0269 - acc: 0.8852 - val_loss: 1.0190 - val_acc: 0.9333\n","Epoch 27/100\n","270/270 [==============================] - 0s 107us/step - loss: 1.0219 - acc: 0.8852 - val_loss: 1.0138 - val_acc: 0.9333\n","Epoch 28/100\n","270/270 [==============================] - 0s 125us/step - loss: 1.0168 - acc: 0.8889 - val_loss: 1.0083 - val_acc: 0.9333\n","Epoch 29/100\n","270/270 [==============================] - 0s 136us/step - loss: 1.0115 - acc: 0.8852 - val_loss: 1.0028 - val_acc: 0.9000\n","Epoch 30/100\n","270/270 [==============================] - 0s 149us/step - loss: 1.0060 - acc: 0.8926 - val_loss: 0.9967 - val_acc: 0.8333\n","Epoch 31/100\n","270/270 [==============================] - 0s 110us/step - loss: 1.0005 - acc: 0.8852 - val_loss: 0.9906 - val_acc: 0.8333\n","Epoch 32/100\n","270/270 [==============================] - 0s 113us/step - loss: 0.9946 - acc: 0.8926 - val_loss: 0.9842 - val_acc: 0.8333\n","Epoch 33/100\n","270/270 [==============================] - 0s 109us/step - loss: 0.9887 - acc: 0.8926 - val_loss: 0.9780 - val_acc: 0.8333\n","Epoch 34/100\n","270/270 [==============================] - 0s 113us/step - loss: 0.9827 - acc: 0.8889 - val_loss: 0.9715 - val_acc: 0.8667\n","Epoch 35/100\n","270/270 [==============================] - 0s 112us/step - loss: 0.9762 - acc: 0.8963 - val_loss: 0.9644 - val_acc: 0.9333\n","Epoch 36/100\n","270/270 [==============================] - 0s 124us/step - loss: 0.9698 - acc: 0.8926 - val_loss: 0.9573 - val_acc: 0.9333\n","Epoch 37/100\n","270/270 [==============================] - 0s 110us/step - loss: 0.9632 - acc: 0.8926 - val_loss: 0.9499 - val_acc: 0.9000\n","Epoch 38/100\n","270/270 [==============================] - 0s 109us/step - loss: 0.9565 - acc: 0.8963 - val_loss: 0.9425 - val_acc: 0.9000\n","Epoch 39/100\n","270/270 [==============================] - 0s 118us/step - loss: 0.9497 - acc: 0.8963 - val_loss: 0.9348 - val_acc: 0.9000\n","Epoch 40/100\n","270/270 [==============================] - 0s 117us/step - loss: 0.9427 - acc: 0.8963 - val_loss: 0.9271 - val_acc: 0.9000\n","Epoch 41/100\n","270/270 [==============================] - 0s 108us/step - loss: 0.9356 - acc: 0.8963 - val_loss: 0.9193 - val_acc: 0.9000\n","Epoch 42/100\n","270/270 [==============================] - 0s 112us/step - loss: 0.9284 - acc: 0.8926 - val_loss: 0.9114 - val_acc: 0.9000\n","Epoch 43/100\n","270/270 [==============================] - 0s 110us/step - loss: 0.9211 - acc: 0.8889 - val_loss: 0.9034 - val_acc: 0.9000\n","Epoch 44/100\n","270/270 [==============================] - 0s 109us/step - loss: 0.9136 - acc: 0.8926 - val_loss: 0.8957 - val_acc: 0.9000\n","Epoch 45/100\n","270/270 [==============================] - 0s 113us/step - loss: 0.9063 - acc: 0.8963 - val_loss: 0.8877 - val_acc: 0.9000\n","Epoch 46/100\n","270/270 [==============================] - 0s 110us/step - loss: 0.8985 - acc: 0.9000 - val_loss: 0.8799 - val_acc: 0.9000\n","Epoch 47/100\n","270/270 [==============================] - 0s 110us/step - loss: 0.8910 - acc: 0.9074 - val_loss: 0.8719 - val_acc: 0.9000\n","Epoch 48/100\n","270/270 [==============================] - 0s 107us/step - loss: 0.8832 - acc: 0.9074 - val_loss: 0.8635 - val_acc: 0.9000\n","Epoch 49/100\n","270/270 [==============================] - 0s 122us/step - loss: 0.8754 - acc: 0.9037 - val_loss: 0.8551 - val_acc: 0.9000\n","Epoch 50/100\n","270/270 [==============================] - 0s 117us/step - loss: 0.8676 - acc: 0.9037 - val_loss: 0.8469 - val_acc: 0.9000\n","Epoch 51/100\n","270/270 [==============================] - 0s 107us/step - loss: 0.8597 - acc: 0.9074 - val_loss: 0.8385 - val_acc: 0.9000\n","Epoch 52/100\n","270/270 [==============================] - 0s 105us/step - loss: 0.8516 - acc: 0.9037 - val_loss: 0.8298 - val_acc: 0.9000\n","Epoch 53/100\n","270/270 [==============================] - 0s 108us/step - loss: 0.8437 - acc: 0.9000 - val_loss: 0.8212 - val_acc: 0.9000\n","Epoch 54/100\n","270/270 [==============================] - 0s 109us/step - loss: 0.8355 - acc: 0.9000 - val_loss: 0.8127 - val_acc: 0.9000\n","Epoch 55/100\n","270/270 [==============================] - 0s 118us/step - loss: 0.8276 - acc: 0.9000 - val_loss: 0.8042 - val_acc: 0.9000\n","Epoch 56/100\n","270/270 [==============================] - 0s 114us/step - loss: 0.8196 - acc: 0.8963 - val_loss: 0.7954 - val_acc: 0.9000\n","Epoch 57/100\n","270/270 [==============================] - 0s 113us/step - loss: 0.8114 - acc: 0.9000 - val_loss: 0.7869 - val_acc: 0.9000\n","Epoch 58/100\n","270/270 [==============================] - 0s 108us/step - loss: 0.8034 - acc: 0.9000 - val_loss: 0.7783 - val_acc: 0.9000\n","Epoch 59/100\n","270/270 [==============================] - 0s 108us/step - loss: 0.7953 - acc: 0.9000 - val_loss: 0.7699 - val_acc: 0.9000\n","Epoch 60/100\n","270/270 [==============================] - 0s 111us/step - loss: 0.7873 - acc: 0.9000 - val_loss: 0.7612 - val_acc: 0.9000\n","Epoch 61/100\n","270/270 [==============================] - 0s 111us/step - loss: 0.7792 - acc: 0.8963 - val_loss: 0.7526 - val_acc: 0.9000\n","Epoch 62/100\n","270/270 [==============================] - 0s 137us/step - loss: 0.7714 - acc: 0.8926 - val_loss: 0.7438 - val_acc: 0.9000\n","Epoch 63/100\n","270/270 [==============================] - 0s 161us/step - loss: 0.7634 - acc: 0.8926 - val_loss: 0.7355 - val_acc: 0.9000\n","Epoch 64/100\n","270/270 [==============================] - 0s 110us/step - loss: 0.7554 - acc: 0.8926 - val_loss: 0.7274 - val_acc: 0.8667\n","Epoch 65/100\n","270/270 [==============================] - 0s 129us/step - loss: 0.7476 - acc: 0.8963 - val_loss: 0.7191 - val_acc: 0.8333\n","Epoch 66/100\n","270/270 [==============================] - 0s 110us/step - loss: 0.7397 - acc: 0.9000 - val_loss: 0.7111 - val_acc: 0.8333\n","Epoch 67/100\n","270/270 [==============================] - 0s 110us/step - loss: 0.7321 - acc: 0.9000 - val_loss: 0.7028 - val_acc: 0.8333\n","Epoch 68/100\n","270/270 [==============================] - 0s 108us/step - loss: 0.7242 - acc: 0.9000 - val_loss: 0.6946 - val_acc: 0.8333\n","Epoch 69/100\n","270/270 [==============================] - 0s 119us/step - loss: 0.7166 - acc: 0.9000 - val_loss: 0.6865 - val_acc: 0.8333\n","Epoch 70/100\n","270/270 [==============================] - 0s 103us/step - loss: 0.7088 - acc: 0.9000 - val_loss: 0.6786 - val_acc: 0.8333\n","Epoch 71/100\n","270/270 [==============================] - 0s 110us/step - loss: 0.7012 - acc: 0.8963 - val_loss: 0.6708 - val_acc: 0.8333\n","Epoch 72/100\n","270/270 [==============================] - 0s 116us/step - loss: 0.6937 - acc: 0.9000 - val_loss: 0.6628 - val_acc: 0.8333\n","Epoch 73/100\n","270/270 [==============================] - 0s 113us/step - loss: 0.6861 - acc: 0.9000 - val_loss: 0.6553 - val_acc: 0.8333\n","Epoch 74/100\n","270/270 [==============================] - 0s 108us/step - loss: 0.6788 - acc: 0.9000 - val_loss: 0.6475 - val_acc: 0.8333\n","Epoch 75/100\n","270/270 [==============================] - 0s 114us/step - loss: 0.6715 - acc: 0.9000 - val_loss: 0.6400 - val_acc: 0.8333\n","Epoch 76/100\n","270/270 [==============================] - 0s 110us/step - loss: 0.6644 - acc: 0.9000 - val_loss: 0.6323 - val_acc: 0.8333\n","Epoch 77/100\n","270/270 [==============================] - 0s 108us/step - loss: 0.6571 - acc: 0.9000 - val_loss: 0.6252 - val_acc: 0.8333\n","Epoch 78/100\n","270/270 [==============================] - 0s 113us/step - loss: 0.6503 - acc: 0.9000 - val_loss: 0.6181 - val_acc: 0.8333\n","Epoch 79/100\n","270/270 [==============================] - 0s 111us/step - loss: 0.6432 - acc: 0.9000 - val_loss: 0.6109 - val_acc: 0.8333\n","Epoch 80/100\n","270/270 [==============================] - 0s 104us/step - loss: 0.6364 - acc: 0.9000 - val_loss: 0.6039 - val_acc: 0.8333\n","Epoch 81/100\n","270/270 [==============================] - 0s 109us/step - loss: 0.6295 - acc: 0.9000 - val_loss: 0.5970 - val_acc: 0.8333\n","Epoch 82/100\n","270/270 [==============================] - 0s 108us/step - loss: 0.6228 - acc: 0.9000 - val_loss: 0.5900 - val_acc: 0.8333\n","Epoch 83/100\n","270/270 [==============================] - 0s 107us/step - loss: 0.6160 - acc: 0.9000 - val_loss: 0.5834 - val_acc: 0.8333\n","Epoch 84/100\n","270/270 [==============================] - 0s 110us/step - loss: 0.6096 - acc: 0.9000 - val_loss: 0.5769 - val_acc: 0.8333\n","Epoch 85/100\n","270/270 [==============================] - 0s 113us/step - loss: 0.6032 - acc: 0.9000 - val_loss: 0.5705 - val_acc: 0.8333\n","Epoch 86/100\n","270/270 [==============================] - 0s 109us/step - loss: 0.5969 - acc: 0.9000 - val_loss: 0.5643 - val_acc: 0.8333\n","Epoch 87/100\n","270/270 [==============================] - 0s 121us/step - loss: 0.5907 - acc: 0.9000 - val_loss: 0.5580 - val_acc: 0.8333\n","Epoch 88/100\n","270/270 [==============================] - 0s 115us/step - loss: 0.5844 - acc: 0.9000 - val_loss: 0.5519 - val_acc: 0.8333\n","Epoch 89/100\n","270/270 [==============================] - 0s 112us/step - loss: 0.5784 - acc: 0.9000 - val_loss: 0.5458 - val_acc: 0.8333\n","Epoch 90/100\n","270/270 [==============================] - 0s 109us/step - loss: 0.5724 - acc: 0.9000 - val_loss: 0.5398 - val_acc: 0.8333\n","Epoch 91/100\n","270/270 [==============================] - 0s 108us/step - loss: 0.5665 - acc: 0.9000 - val_loss: 0.5339 - val_acc: 0.8333\n","Epoch 92/100\n","270/270 [==============================] - 0s 106us/step - loss: 0.5607 - acc: 0.9000 - val_loss: 0.5282 - val_acc: 0.8333\n","Epoch 93/100\n","270/270 [==============================] - 0s 114us/step - loss: 0.5549 - acc: 0.9000 - val_loss: 0.5226 - val_acc: 0.8333\n","Epoch 94/100\n","270/270 [==============================] - 0s 110us/step - loss: 0.5494 - acc: 0.9000 - val_loss: 0.5169 - val_acc: 0.8333\n","Epoch 95/100\n","270/270 [==============================] - 0s 123us/step - loss: 0.5437 - acc: 0.9000 - val_loss: 0.5116 - val_acc: 0.8333\n","Epoch 96/100\n","270/270 [==============================] - 0s 145us/step - loss: 0.5385 - acc: 0.9000 - val_loss: 0.5065 - val_acc: 0.8333\n","Epoch 97/100\n","270/270 [==============================] - 0s 123us/step - loss: 0.5331 - acc: 0.9000 - val_loss: 0.5010 - val_acc: 0.8333\n","Epoch 98/100\n","270/270 [==============================] - 0s 116us/step - loss: 0.5279 - acc: 0.9000 - val_loss: 0.4960 - val_acc: 0.8333\n","Epoch 99/100\n","270/270 [==============================] - 0s 114us/step - loss: 0.5226 - acc: 0.9000 - val_loss: 0.4911 - val_acc: 0.8333\n","Epoch 100/100\n","270/270 [==============================] - 0s 113us/step - loss: 0.5176 - acc: 0.9000 - val_loss: 0.4862 - val_acc: 0.8333\n","Accuracy :0.900\n","[[9 1 0]\n"," [1 9 0]\n"," [1 0 9]]\n","---------------------------------------------------------------------------------------------------------------------------\n","NH = 100\n","Nepochs = 100\n","N = 200\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_7 (InputLayer)         (None, 2)                 0         \n","_________________________________________________________________\n","dense_13 (Dense)             (None, 100)               300       \n","_________________________________________________________________\n","dense_14 (Dense)             (None, 3)                 303       \n","=================================================================\n","Total params: 603\n","Trainable params: 603\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 540 samples, validate on 60 samples\n","Epoch 1/100\n","540/540 [==============================] - 0s 647us/step - loss: 1.0993 - acc: 0.3296 - val_loss: 1.0966 - val_acc: 0.3667\n","Epoch 2/100\n","540/540 [==============================] - 0s 90us/step - loss: 1.0966 - acc: 0.3556 - val_loss: 1.0939 - val_acc: 0.5167\n","Epoch 3/100\n","540/540 [==============================] - 0s 94us/step - loss: 1.0937 - acc: 0.5593 - val_loss: 1.0908 - val_acc: 0.6500\n","Epoch 4/100\n","540/540 [==============================] - 0s 109us/step - loss: 1.0906 - acc: 0.6574 - val_loss: 1.0877 - val_acc: 0.7667\n","Epoch 5/100\n","540/540 [==============================] - 0s 106us/step - loss: 1.0873 - acc: 0.7685 - val_loss: 1.0842 - val_acc: 0.9333\n","Epoch 6/100\n","540/540 [==============================] - 0s 97us/step - loss: 1.0837 - acc: 0.8611 - val_loss: 1.0804 - val_acc: 0.9500\n","Epoch 7/100\n","540/540 [==============================] - 0s 92us/step - loss: 1.0796 - acc: 0.8889 - val_loss: 1.0757 - val_acc: 0.9333\n","Epoch 8/100\n","540/540 [==============================] - 0s 102us/step - loss: 1.0750 - acc: 0.9019 - val_loss: 1.0707 - val_acc: 0.9167\n","Epoch 9/100\n","540/540 [==============================] - 0s 93us/step - loss: 1.0699 - acc: 0.8981 - val_loss: 1.0650 - val_acc: 0.9000\n","Epoch 10/100\n","540/540 [==============================] - 0s 93us/step - loss: 1.0641 - acc: 0.8944 - val_loss: 1.0586 - val_acc: 0.8833\n","Epoch 11/100\n","540/540 [==============================] - 0s 93us/step - loss: 1.0576 - acc: 0.8889 - val_loss: 1.0517 - val_acc: 0.9000\n","Epoch 12/100\n","540/540 [==============================] - 0s 98us/step - loss: 1.0505 - acc: 0.8870 - val_loss: 1.0439 - val_acc: 0.9000\n","Epoch 13/100\n","540/540 [==============================] - 0s 94us/step - loss: 1.0426 - acc: 0.8889 - val_loss: 1.0355 - val_acc: 0.9000\n","Epoch 14/100\n","540/540 [==============================] - 0s 90us/step - loss: 1.0341 - acc: 0.8833 - val_loss: 1.0262 - val_acc: 0.9000\n","Epoch 15/100\n","540/540 [==============================] - 0s 94us/step - loss: 1.0247 - acc: 0.8852 - val_loss: 1.0158 - val_acc: 0.9000\n","Epoch 16/100\n","540/540 [==============================] - 0s 91us/step - loss: 1.0147 - acc: 0.8944 - val_loss: 1.0049 - val_acc: 0.9000\n","Epoch 17/100\n","540/540 [==============================] - 0s 96us/step - loss: 1.0038 - acc: 0.8963 - val_loss: 0.9935 - val_acc: 0.9000\n","Epoch 18/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.9924 - acc: 0.8870 - val_loss: 0.9814 - val_acc: 0.9000\n","Epoch 19/100\n","540/540 [==============================] - 0s 96us/step - loss: 0.9802 - acc: 0.8926 - val_loss: 0.9682 - val_acc: 0.9000\n","Epoch 20/100\n","540/540 [==============================] - 0s 105us/step - loss: 0.9676 - acc: 0.8870 - val_loss: 0.9549 - val_acc: 0.9000\n","Epoch 21/100\n","540/540 [==============================] - 0s 91us/step - loss: 0.9541 - acc: 0.8926 - val_loss: 0.9407 - val_acc: 0.9000\n","Epoch 22/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.9402 - acc: 0.9000 - val_loss: 0.9259 - val_acc: 0.9000\n","Epoch 23/100\n","540/540 [==============================] - 0s 96us/step - loss: 0.9259 - acc: 0.8963 - val_loss: 0.9109 - val_acc: 0.9000\n","Epoch 24/100\n","540/540 [==============================] - 0s 109us/step - loss: 0.9112 - acc: 0.9000 - val_loss: 0.8953 - val_acc: 0.9000\n","Epoch 25/100\n","540/540 [==============================] - 0s 102us/step - loss: 0.8960 - acc: 0.9000 - val_loss: 0.8797 - val_acc: 0.9000\n","Epoch 26/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.8806 - acc: 0.8963 - val_loss: 0.8635 - val_acc: 0.9000\n","Epoch 27/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.8652 - acc: 0.8981 - val_loss: 0.8471 - val_acc: 0.9000\n","Epoch 28/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.8494 - acc: 0.9000 - val_loss: 0.8304 - val_acc: 0.9000\n","Epoch 29/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.8333 - acc: 0.9000 - val_loss: 0.8141 - val_acc: 0.9000\n","Epoch 30/100\n","540/540 [==============================] - 0s 92us/step - loss: 0.8174 - acc: 0.9000 - val_loss: 0.7973 - val_acc: 0.9000\n","Epoch 31/100\n","540/540 [==============================] - 0s 105us/step - loss: 0.8016 - acc: 0.9037 - val_loss: 0.7804 - val_acc: 0.9000\n","Epoch 32/100\n","540/540 [==============================] - 0s 90us/step - loss: 0.7856 - acc: 0.9019 - val_loss: 0.7638 - val_acc: 0.9000\n","Epoch 33/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.7699 - acc: 0.8981 - val_loss: 0.7473 - val_acc: 0.9000\n","Epoch 34/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.7541 - acc: 0.9037 - val_loss: 0.7309 - val_acc: 0.9000\n","Epoch 35/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.7386 - acc: 0.9037 - val_loss: 0.7149 - val_acc: 0.9000\n","Epoch 36/100\n","540/540 [==============================] - 0s 91us/step - loss: 0.7234 - acc: 0.9000 - val_loss: 0.6987 - val_acc: 0.9000\n","Epoch 37/100\n","540/540 [==============================] - 0s 99us/step - loss: 0.7080 - acc: 0.9037 - val_loss: 0.6831 - val_acc: 0.9000\n","Epoch 38/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.6934 - acc: 0.9019 - val_loss: 0.6675 - val_acc: 0.9000\n","Epoch 39/100\n","540/540 [==============================] - 0s 101us/step - loss: 0.6787 - acc: 0.9019 - val_loss: 0.6523 - val_acc: 0.9000\n","Epoch 40/100\n","540/540 [==============================] - 0s 92us/step - loss: 0.6645 - acc: 0.9019 - val_loss: 0.6373 - val_acc: 0.9000\n","Epoch 41/100\n","540/540 [==============================] - 0s 94us/step - loss: 0.6506 - acc: 0.9019 - val_loss: 0.6226 - val_acc: 0.9000\n","Epoch 42/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.6369 - acc: 0.9019 - val_loss: 0.6083 - val_acc: 0.9000\n","Epoch 43/100\n","540/540 [==============================] - 0s 91us/step - loss: 0.6237 - acc: 0.9037 - val_loss: 0.5944 - val_acc: 0.9000\n","Epoch 44/100\n","540/540 [==============================] - 0s 140us/step - loss: 0.6106 - acc: 0.9019 - val_loss: 0.5805 - val_acc: 0.9000\n","Epoch 45/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.5981 - acc: 0.9037 - val_loss: 0.5672 - val_acc: 0.9000\n","Epoch 46/100\n","540/540 [==============================] - 0s 98us/step - loss: 0.5859 - acc: 0.9037 - val_loss: 0.5550 - val_acc: 0.9000\n","Epoch 47/100\n","540/540 [==============================] - 0s 96us/step - loss: 0.5739 - acc: 0.9037 - val_loss: 0.5426 - val_acc: 0.9000\n","Epoch 48/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.5624 - acc: 0.9037 - val_loss: 0.5306 - val_acc: 0.9000\n","Epoch 49/100\n","540/540 [==============================] - 0s 94us/step - loss: 0.5513 - acc: 0.9037 - val_loss: 0.5189 - val_acc: 0.9000\n","Epoch 50/100\n","540/540 [==============================] - 0s 92us/step - loss: 0.5406 - acc: 0.9037 - val_loss: 0.5075 - val_acc: 0.9000\n","Epoch 51/100\n","540/540 [==============================] - 0s 91us/step - loss: 0.5302 - acc: 0.9037 - val_loss: 0.4969 - val_acc: 0.9000\n","Epoch 52/100\n","540/540 [==============================] - 0s 92us/step - loss: 0.5203 - acc: 0.9037 - val_loss: 0.4865 - val_acc: 0.9000\n","Epoch 53/100\n","540/540 [==============================] - 0s 103us/step - loss: 0.5105 - acc: 0.9037 - val_loss: 0.4766 - val_acc: 0.9000\n","Epoch 54/100\n","540/540 [==============================] - 0s 97us/step - loss: 0.5013 - acc: 0.9037 - val_loss: 0.4670 - val_acc: 0.9000\n","Epoch 55/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.4924 - acc: 0.9037 - val_loss: 0.4577 - val_acc: 0.9000\n","Epoch 56/100\n","540/540 [==============================] - 0s 135us/step - loss: 0.4837 - acc: 0.9037 - val_loss: 0.4489 - val_acc: 0.9000\n","Epoch 57/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.4756 - acc: 0.9037 - val_loss: 0.4399 - val_acc: 0.9000\n","Epoch 58/100\n","540/540 [==============================] - 0s 90us/step - loss: 0.4675 - acc: 0.9037 - val_loss: 0.4319 - val_acc: 0.9000\n","Epoch 59/100\n","540/540 [==============================] - 0s 94us/step - loss: 0.4600 - acc: 0.9037 - val_loss: 0.4235 - val_acc: 0.9000\n","Epoch 60/100\n","540/540 [==============================] - 0s 94us/step - loss: 0.4524 - acc: 0.9037 - val_loss: 0.4163 - val_acc: 0.9000\n","Epoch 61/100\n","540/540 [==============================] - 0s 98us/step - loss: 0.4455 - acc: 0.9037 - val_loss: 0.4091 - val_acc: 0.9000\n","Epoch 62/100\n","540/540 [==============================] - 0s 92us/step - loss: 0.4386 - acc: 0.9037 - val_loss: 0.4020 - val_acc: 0.9000\n","Epoch 63/100\n","540/540 [==============================] - 0s 135us/step - loss: 0.4321 - acc: 0.9037 - val_loss: 0.3954 - val_acc: 0.9000\n","Epoch 64/100\n","540/540 [==============================] - 0s 92us/step - loss: 0.4258 - acc: 0.9037 - val_loss: 0.3888 - val_acc: 0.9000\n","Epoch 65/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.4197 - acc: 0.9037 - val_loss: 0.3826 - val_acc: 0.9000\n","Epoch 66/100\n","540/540 [==============================] - 0s 94us/step - loss: 0.4139 - acc: 0.9037 - val_loss: 0.3766 - val_acc: 0.9000\n","Epoch 67/100\n","540/540 [==============================] - 0s 94us/step - loss: 0.4084 - acc: 0.9037 - val_loss: 0.3707 - val_acc: 0.9000\n","Epoch 68/100\n","540/540 [==============================] - 0s 91us/step - loss: 0.4030 - acc: 0.9037 - val_loss: 0.3652 - val_acc: 0.9000\n","Epoch 69/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.3979 - acc: 0.9037 - val_loss: 0.3596 - val_acc: 0.9000\n","Epoch 70/100\n","540/540 [==============================] - 0s 100us/step - loss: 0.3929 - acc: 0.9037 - val_loss: 0.3546 - val_acc: 0.9000\n","Epoch 71/100\n","540/540 [==============================] - 0s 91us/step - loss: 0.3882 - acc: 0.9056 - val_loss: 0.3497 - val_acc: 0.9000\n","Epoch 72/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.3836 - acc: 0.9056 - val_loss: 0.3452 - val_acc: 0.9000\n","Epoch 73/100\n","540/540 [==============================] - 0s 89us/step - loss: 0.3793 - acc: 0.9056 - val_loss: 0.3408 - val_acc: 0.9000\n","Epoch 74/100\n","540/540 [==============================] - 0s 94us/step - loss: 0.3751 - acc: 0.9056 - val_loss: 0.3367 - val_acc: 0.9000\n","Epoch 75/100\n","540/540 [==============================] - 0s 89us/step - loss: 0.3711 - acc: 0.9056 - val_loss: 0.3325 - val_acc: 0.9000\n","Epoch 76/100\n","540/540 [==============================] - 0s 97us/step - loss: 0.3672 - acc: 0.9056 - val_loss: 0.3284 - val_acc: 0.9000\n","Epoch 77/100\n","540/540 [==============================] - 0s 98us/step - loss: 0.3634 - acc: 0.9056 - val_loss: 0.3248 - val_acc: 0.9000\n","Epoch 78/100\n","540/540 [==============================] - 0s 95us/step - loss: 0.3599 - acc: 0.9056 - val_loss: 0.3209 - val_acc: 0.9000\n","Epoch 79/100\n","540/540 [==============================] - 0s 98us/step - loss: 0.3566 - acc: 0.9056 - val_loss: 0.3178 - val_acc: 0.9000\n","Epoch 80/100\n","540/540 [==============================] - 0s 94us/step - loss: 0.3532 - acc: 0.9056 - val_loss: 0.3140 - val_acc: 0.9000\n","Epoch 81/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.3500 - acc: 0.9056 - val_loss: 0.3107 - val_acc: 0.9000\n","Epoch 82/100\n","540/540 [==============================] - 0s 101us/step - loss: 0.3469 - acc: 0.9056 - val_loss: 0.3073 - val_acc: 0.9000\n","Epoch 83/100\n","540/540 [==============================] - 0s 133us/step - loss: 0.3440 - acc: 0.9056 - val_loss: 0.3046 - val_acc: 0.9000\n","Epoch 84/100\n","540/540 [==============================] - 0s 92us/step - loss: 0.3412 - acc: 0.9056 - val_loss: 0.3017 - val_acc: 0.9000\n","Epoch 85/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.3385 - acc: 0.9056 - val_loss: 0.2991 - val_acc: 0.9000\n","Epoch 86/100\n","540/540 [==============================] - 0s 92us/step - loss: 0.3358 - acc: 0.9056 - val_loss: 0.2961 - val_acc: 0.9000\n","Epoch 87/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.3333 - acc: 0.9056 - val_loss: 0.2932 - val_acc: 0.9000\n","Epoch 88/100\n","540/540 [==============================] - 0s 98us/step - loss: 0.3309 - acc: 0.9056 - val_loss: 0.2906 - val_acc: 0.9000\n","Epoch 89/100\n","540/540 [==============================] - 0s 107us/step - loss: 0.3286 - acc: 0.9074 - val_loss: 0.2884 - val_acc: 0.9000\n","Epoch 90/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.3263 - acc: 0.9074 - val_loss: 0.2857 - val_acc: 0.9000\n","Epoch 91/100\n","540/540 [==============================] - 0s 94us/step - loss: 0.3240 - acc: 0.9074 - val_loss: 0.2837 - val_acc: 0.9000\n","Epoch 92/100\n","540/540 [==============================] - 0s 94us/step - loss: 0.3221 - acc: 0.9074 - val_loss: 0.2810 - val_acc: 0.9000\n","Epoch 93/100\n","540/540 [==============================] - 0s 96us/step - loss: 0.3200 - acc: 0.9074 - val_loss: 0.2789 - val_acc: 0.9000\n","Epoch 94/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.3180 - acc: 0.9074 - val_loss: 0.2773 - val_acc: 0.9000\n","Epoch 95/100\n","540/540 [==============================] - 0s 94us/step - loss: 0.3162 - acc: 0.9074 - val_loss: 0.2751 - val_acc: 0.9000\n","Epoch 96/100\n","540/540 [==============================] - 0s 98us/step - loss: 0.3144 - acc: 0.9074 - val_loss: 0.2731 - val_acc: 0.9000\n","Epoch 97/100\n","540/540 [==============================] - 0s 97us/step - loss: 0.3126 - acc: 0.9074 - val_loss: 0.2712 - val_acc: 0.9000\n","Epoch 98/100\n","540/540 [==============================] - 0s 94us/step - loss: 0.3110 - acc: 0.9074 - val_loss: 0.2697 - val_acc: 0.9000\n","Epoch 99/100\n","540/540 [==============================] - 0s 93us/step - loss: 0.3093 - acc: 0.9074 - val_loss: 0.2678 - val_acc: 0.9000\n","Epoch 100/100\n","540/540 [==============================] - 0s 96us/step - loss: 0.3078 - acc: 0.9074 - val_loss: 0.2662 - val_acc: 0.9000\n","Accuracy :0.967\n","[[10  0  0]\n"," [ 1  9  0]\n"," [ 0  0 10]]\n","---------------------------------------------------------------------------------------------------------------------------\n","NH = 100\n","Nepochs = 200\n","N = 100\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_8 (InputLayer)         (None, 2)                 0         \n","_________________________________________________________________\n","dense_15 (Dense)             (None, 100)               300       \n","_________________________________________________________________\n","dense_16 (Dense)             (None, 3)                 303       \n","=================================================================\n","Total params: 603\n","Trainable params: 603\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 270 samples, validate on 30 samples\n","Epoch 1/200\n","270/270 [==============================] - 0s 1ms/step - loss: 1.0963 - acc: 0.5222 - val_loss: 1.0963 - val_acc: 0.5667\n","Epoch 2/200\n","270/270 [==============================] - 0s 119us/step - loss: 1.0943 - acc: 0.6333 - val_loss: 1.0957 - val_acc: 0.5667\n","Epoch 3/200\n","270/270 [==============================] - 0s 111us/step - loss: 1.0925 - acc: 0.6000 - val_loss: 1.0955 - val_acc: 0.4333\n","Epoch 4/200\n","270/270 [==============================] - 0s 109us/step - loss: 1.0907 - acc: 0.5704 - val_loss: 1.0942 - val_acc: 0.4000\n","Epoch 5/200\n","270/270 [==============================] - 0s 107us/step - loss: 1.0889 - acc: 0.5444 - val_loss: 1.0934 - val_acc: 0.3667\n","Epoch 6/200\n","270/270 [==============================] - 0s 103us/step - loss: 1.0869 - acc: 0.5370 - val_loss: 1.0920 - val_acc: 0.4000\n","Epoch 7/200\n","270/270 [==============================] - 0s 113us/step - loss: 1.0850 - acc: 0.5296 - val_loss: 1.0911 - val_acc: 0.4333\n","Epoch 8/200\n","270/270 [==============================] - 0s 124us/step - loss: 1.0828 - acc: 0.5333 - val_loss: 1.0896 - val_acc: 0.4000\n","Epoch 9/200\n","270/270 [==============================] - 0s 126us/step - loss: 1.0806 - acc: 0.5370 - val_loss: 1.0888 - val_acc: 0.4333\n","Epoch 10/200\n","270/270 [==============================] - 0s 125us/step - loss: 1.0784 - acc: 0.5444 - val_loss: 1.0867 - val_acc: 0.5333\n","Epoch 11/200\n","270/270 [==============================] - 0s 120us/step - loss: 1.0759 - acc: 0.5704 - val_loss: 1.0852 - val_acc: 0.5333\n","Epoch 12/200\n","270/270 [==============================] - 0s 126us/step - loss: 1.0733 - acc: 0.5630 - val_loss: 1.0839 - val_acc: 0.4667\n","Epoch 13/200\n","270/270 [==============================] - 0s 152us/step - loss: 1.0708 - acc: 0.5593 - val_loss: 1.0824 - val_acc: 0.5000\n","Epoch 14/200\n","270/270 [==============================] - 0s 126us/step - loss: 1.0678 - acc: 0.5778 - val_loss: 1.0800 - val_acc: 0.5333\n","Epoch 15/200\n","270/270 [==============================] - 0s 111us/step - loss: 1.0649 - acc: 0.5778 - val_loss: 1.0780 - val_acc: 0.5000\n","Epoch 16/200\n","270/270 [==============================] - 0s 112us/step - loss: 1.0617 - acc: 0.6000 - val_loss: 1.0754 - val_acc: 0.5333\n","Epoch 17/200\n","270/270 [==============================] - 0s 121us/step - loss: 1.0583 - acc: 0.6370 - val_loss: 1.0718 - val_acc: 0.5667\n","Epoch 18/200\n","270/270 [==============================] - 0s 121us/step - loss: 1.0549 - acc: 0.6667 - val_loss: 1.0680 - val_acc: 0.6000\n","Epoch 19/200\n","270/270 [==============================] - 0s 111us/step - loss: 1.0511 - acc: 0.6963 - val_loss: 1.0644 - val_acc: 0.6333\n","Epoch 20/200\n","270/270 [==============================] - 0s 122us/step - loss: 1.0473 - acc: 0.7259 - val_loss: 1.0610 - val_acc: 0.6333\n","Epoch 21/200\n","270/270 [==============================] - 0s 138us/step - loss: 1.0430 - acc: 0.7296 - val_loss: 1.0573 - val_acc: 0.6333\n","Epoch 22/200\n","270/270 [==============================] - 0s 120us/step - loss: 1.0387 - acc: 0.7519 - val_loss: 1.0527 - val_acc: 0.6667\n","Epoch 23/200\n","270/270 [==============================] - 0s 110us/step - loss: 1.0341 - acc: 0.7593 - val_loss: 1.0488 - val_acc: 0.6667\n","Epoch 24/200\n","270/270 [==============================] - 0s 128us/step - loss: 1.0295 - acc: 0.7667 - val_loss: 1.0452 - val_acc: 0.6667\n","Epoch 25/200\n","270/270 [==============================] - 0s 114us/step - loss: 1.0245 - acc: 0.7741 - val_loss: 1.0403 - val_acc: 0.7667\n","Epoch 26/200\n","270/270 [==============================] - 0s 107us/step - loss: 1.0193 - acc: 0.8000 - val_loss: 1.0354 - val_acc: 0.8000\n","Epoch 27/200\n","270/270 [==============================] - 0s 118us/step - loss: 1.0140 - acc: 0.8111 - val_loss: 1.0305 - val_acc: 0.8333\n","Epoch 28/200\n","270/270 [==============================] - 0s 122us/step - loss: 1.0085 - acc: 0.8148 - val_loss: 1.0258 - val_acc: 0.8000\n","Epoch 29/200\n","270/270 [==============================] - 0s 124us/step - loss: 1.0028 - acc: 0.8185 - val_loss: 1.0210 - val_acc: 0.8667\n","Epoch 30/200\n","270/270 [==============================] - 0s 123us/step - loss: 0.9969 - acc: 0.8222 - val_loss: 1.0152 - val_acc: 0.8333\n","Epoch 31/200\n","270/270 [==============================] - 0s 120us/step - loss: 0.9907 - acc: 0.8296 - val_loss: 1.0095 - val_acc: 0.8333\n","Epoch 32/200\n","270/270 [==============================] - 0s 128us/step - loss: 0.9847 - acc: 0.8296 - val_loss: 1.0031 - val_acc: 0.8667\n","Epoch 33/200\n","270/270 [==============================] - 0s 124us/step - loss: 0.9783 - acc: 0.8444 - val_loss: 0.9971 - val_acc: 0.8667\n","Epoch 34/200\n","270/270 [==============================] - 0s 116us/step - loss: 0.9717 - acc: 0.8519 - val_loss: 0.9904 - val_acc: 0.8667\n","Epoch 35/200\n","270/270 [==============================] - 0s 114us/step - loss: 0.9650 - acc: 0.8519 - val_loss: 0.9836 - val_acc: 0.8667\n","Epoch 36/200\n","270/270 [==============================] - 0s 106us/step - loss: 0.9584 - acc: 0.8556 - val_loss: 0.9764 - val_acc: 0.8667\n","Epoch 37/200\n","270/270 [==============================] - 0s 117us/step - loss: 0.9514 - acc: 0.8556 - val_loss: 0.9706 - val_acc: 0.8667\n","Epoch 38/200\n","270/270 [==============================] - 0s 121us/step - loss: 0.9444 - acc: 0.8556 - val_loss: 0.9634 - val_acc: 0.8667\n","Epoch 39/200\n","270/270 [==============================] - 0s 131us/step - loss: 0.9372 - acc: 0.8556 - val_loss: 0.9560 - val_acc: 0.8667\n","Epoch 40/200\n","270/270 [==============================] - 0s 131us/step - loss: 0.9301 - acc: 0.8556 - val_loss: 0.9493 - val_acc: 0.8667\n","Epoch 41/200\n","270/270 [==============================] - 0s 132us/step - loss: 0.9227 - acc: 0.8556 - val_loss: 0.9423 - val_acc: 0.8667\n","Epoch 42/200\n","270/270 [==============================] - 0s 114us/step - loss: 0.9153 - acc: 0.8630 - val_loss: 0.9358 - val_acc: 0.8333\n","Epoch 43/200\n","270/270 [==============================] - 0s 117us/step - loss: 0.9079 - acc: 0.8667 - val_loss: 0.9284 - val_acc: 0.8333\n","Epoch 44/200\n","270/270 [==============================] - 0s 154us/step - loss: 0.9002 - acc: 0.8741 - val_loss: 0.9209 - val_acc: 0.8667\n","Epoch 45/200\n","270/270 [==============================] - 0s 120us/step - loss: 0.8926 - acc: 0.8630 - val_loss: 0.9140 - val_acc: 0.8667\n","Epoch 46/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.8850 - acc: 0.8630 - val_loss: 0.9070 - val_acc: 0.8667\n","Epoch 47/200\n","270/270 [==============================] - 0s 125us/step - loss: 0.8773 - acc: 0.8667 - val_loss: 0.8991 - val_acc: 0.9000\n","Epoch 48/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.8695 - acc: 0.8630 - val_loss: 0.8920 - val_acc: 0.9000\n","Epoch 49/200\n","270/270 [==============================] - 0s 103us/step - loss: 0.8617 - acc: 0.8667 - val_loss: 0.8846 - val_acc: 0.9333\n","Epoch 50/200\n","270/270 [==============================] - 0s 114us/step - loss: 0.8539 - acc: 0.8704 - val_loss: 0.8769 - val_acc: 0.9333\n","Epoch 51/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.8460 - acc: 0.8704 - val_loss: 0.8688 - val_acc: 0.9333\n","Epoch 52/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.8382 - acc: 0.8704 - val_loss: 0.8615 - val_acc: 0.9333\n","Epoch 53/200\n","270/270 [==============================] - 0s 106us/step - loss: 0.8302 - acc: 0.8704 - val_loss: 0.8536 - val_acc: 0.9667\n","Epoch 54/200\n","270/270 [==============================] - 0s 113us/step - loss: 0.8224 - acc: 0.8815 - val_loss: 0.8456 - val_acc: 0.9667\n","Epoch 55/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.8144 - acc: 0.8852 - val_loss: 0.8381 - val_acc: 0.9667\n","Epoch 56/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.8065 - acc: 0.8815 - val_loss: 0.8298 - val_acc: 0.9667\n","Epoch 57/200\n","270/270 [==============================] - 0s 121us/step - loss: 0.7987 - acc: 0.8815 - val_loss: 0.8222 - val_acc: 0.9667\n","Epoch 58/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.7907 - acc: 0.8852 - val_loss: 0.8138 - val_acc: 0.9667\n","Epoch 59/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.7828 - acc: 0.8852 - val_loss: 0.8057 - val_acc: 0.9667\n","Epoch 60/200\n","270/270 [==============================] - 0s 124us/step - loss: 0.7749 - acc: 0.8852 - val_loss: 0.7981 - val_acc: 0.9667\n","Epoch 61/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.7670 - acc: 0.8852 - val_loss: 0.7901 - val_acc: 0.9667\n","Epoch 62/200\n","270/270 [==============================] - 0s 120us/step - loss: 0.7591 - acc: 0.8852 - val_loss: 0.7821 - val_acc: 0.9667\n","Epoch 63/200\n","270/270 [==============================] - 0s 118us/step - loss: 0.7514 - acc: 0.8815 - val_loss: 0.7744 - val_acc: 0.9667\n","Epoch 64/200\n","270/270 [==============================] - 0s 123us/step - loss: 0.7436 - acc: 0.8815 - val_loss: 0.7669 - val_acc: 0.9667\n","Epoch 65/200\n","270/270 [==============================] - 0s 121us/step - loss: 0.7358 - acc: 0.8815 - val_loss: 0.7591 - val_acc: 0.9667\n","Epoch 66/200\n","270/270 [==============================] - 0s 116us/step - loss: 0.7282 - acc: 0.8815 - val_loss: 0.7512 - val_acc: 0.9667\n","Epoch 67/200\n","270/270 [==============================] - 0s 115us/step - loss: 0.7206 - acc: 0.8815 - val_loss: 0.7433 - val_acc: 0.9667\n","Epoch 68/200\n","270/270 [==============================] - 0s 119us/step - loss: 0.7131 - acc: 0.8815 - val_loss: 0.7355 - val_acc: 0.9667\n","Epoch 69/200\n","270/270 [==============================] - 0s 133us/step - loss: 0.7055 - acc: 0.8815 - val_loss: 0.7276 - val_acc: 0.9667\n","Epoch 70/200\n","270/270 [==============================] - 0s 116us/step - loss: 0.6978 - acc: 0.8852 - val_loss: 0.7199 - val_acc: 0.9667\n","Epoch 71/200\n","270/270 [==============================] - 0s 126us/step - loss: 0.6904 - acc: 0.8852 - val_loss: 0.7124 - val_acc: 0.9667\n","Epoch 72/200\n","270/270 [==============================] - 0s 113us/step - loss: 0.6831 - acc: 0.8852 - val_loss: 0.7050 - val_acc: 0.9667\n","Epoch 73/200\n","270/270 [==============================] - 0s 122us/step - loss: 0.6756 - acc: 0.8852 - val_loss: 0.6980 - val_acc: 0.9333\n","Epoch 74/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.6682 - acc: 0.8889 - val_loss: 0.6904 - val_acc: 0.9667\n","Epoch 75/200\n","270/270 [==============================] - 0s 149us/step - loss: 0.6611 - acc: 0.8852 - val_loss: 0.6832 - val_acc: 0.9333\n","Epoch 76/200\n","270/270 [==============================] - 0s 153us/step - loss: 0.6540 - acc: 0.8852 - val_loss: 0.6757 - val_acc: 0.9333\n","Epoch 77/200\n","270/270 [==============================] - 0s 124us/step - loss: 0.6467 - acc: 0.8926 - val_loss: 0.6689 - val_acc: 0.9333\n","Epoch 78/200\n","270/270 [==============================] - 0s 116us/step - loss: 0.6397 - acc: 0.8889 - val_loss: 0.6615 - val_acc: 0.9333\n","Epoch 79/200\n","270/270 [==============================] - 0s 118us/step - loss: 0.6326 - acc: 0.8926 - val_loss: 0.6544 - val_acc: 0.9333\n","Epoch 80/200\n","270/270 [==============================] - 0s 120us/step - loss: 0.6258 - acc: 0.8889 - val_loss: 0.6475 - val_acc: 0.9333\n","Epoch 81/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.6190 - acc: 0.8889 - val_loss: 0.6410 - val_acc: 0.9333\n","Epoch 82/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.6122 - acc: 0.8889 - val_loss: 0.6341 - val_acc: 0.9333\n","Epoch 83/200\n","270/270 [==============================] - 0s 115us/step - loss: 0.6055 - acc: 0.8889 - val_loss: 0.6267 - val_acc: 0.9333\n","Epoch 84/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.5990 - acc: 0.8889 - val_loss: 0.6200 - val_acc: 0.9333\n","Epoch 85/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.5924 - acc: 0.8889 - val_loss: 0.6134 - val_acc: 0.9333\n","Epoch 86/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.5860 - acc: 0.8889 - val_loss: 0.6068 - val_acc: 0.9333\n","Epoch 87/200\n","270/270 [==============================] - 0s 118us/step - loss: 0.5796 - acc: 0.8889 - val_loss: 0.6000 - val_acc: 0.9333\n","Epoch 88/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.5734 - acc: 0.8889 - val_loss: 0.5928 - val_acc: 0.9333\n","Epoch 89/200\n","270/270 [==============================] - 0s 104us/step - loss: 0.5670 - acc: 0.8926 - val_loss: 0.5864 - val_acc: 0.9333\n","Epoch 90/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.5610 - acc: 0.8926 - val_loss: 0.5804 - val_acc: 0.9333\n","Epoch 91/200\n","270/270 [==============================] - 0s 114us/step - loss: 0.5549 - acc: 0.8926 - val_loss: 0.5745 - val_acc: 0.9333\n","Epoch 92/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.5491 - acc: 0.8926 - val_loss: 0.5686 - val_acc: 0.9333\n","Epoch 93/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.5431 - acc: 0.8926 - val_loss: 0.5625 - val_acc: 0.9333\n","Epoch 94/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.5373 - acc: 0.8963 - val_loss: 0.5567 - val_acc: 0.9333\n","Epoch 95/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.5317 - acc: 0.8963 - val_loss: 0.5511 - val_acc: 0.9333\n","Epoch 96/200\n","270/270 [==============================] - 0s 117us/step - loss: 0.5261 - acc: 0.8963 - val_loss: 0.5454 - val_acc: 0.9333\n","Epoch 97/200\n","270/270 [==============================] - 0s 114us/step - loss: 0.5205 - acc: 0.8963 - val_loss: 0.5393 - val_acc: 0.9333\n","Epoch 98/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.5151 - acc: 0.8963 - val_loss: 0.5343 - val_acc: 0.9333\n","Epoch 99/200\n","270/270 [==============================] - 0s 118us/step - loss: 0.5097 - acc: 0.8963 - val_loss: 0.5284 - val_acc: 0.9333\n","Epoch 100/200\n","270/270 [==============================] - 0s 115us/step - loss: 0.5044 - acc: 0.9000 - val_loss: 0.5231 - val_acc: 0.9333\n","Epoch 101/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.4992 - acc: 0.9000 - val_loss: 0.5178 - val_acc: 0.9333\n","Epoch 102/200\n","270/270 [==============================] - 0s 106us/step - loss: 0.4941 - acc: 0.9000 - val_loss: 0.5127 - val_acc: 0.9333\n","Epoch 103/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.4890 - acc: 0.9000 - val_loss: 0.5077 - val_acc: 0.9333\n","Epoch 104/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.4841 - acc: 0.9000 - val_loss: 0.5030 - val_acc: 0.9333\n","Epoch 105/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.4790 - acc: 0.9000 - val_loss: 0.4985 - val_acc: 0.9333\n","Epoch 106/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.4744 - acc: 0.8963 - val_loss: 0.4946 - val_acc: 0.9333\n","Epoch 107/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.4696 - acc: 0.8963 - val_loss: 0.4898 - val_acc: 0.9333\n","Epoch 108/200\n","270/270 [==============================] - 0s 133us/step - loss: 0.4650 - acc: 0.8963 - val_loss: 0.4851 - val_acc: 0.9333\n","Epoch 109/200\n","270/270 [==============================] - 0s 150us/step - loss: 0.4603 - acc: 0.8963 - val_loss: 0.4811 - val_acc: 0.9333\n","Epoch 110/200\n","270/270 [==============================] - 0s 120us/step - loss: 0.4558 - acc: 0.9000 - val_loss: 0.4768 - val_acc: 0.9333\n","Epoch 111/200\n","270/270 [==============================] - 0s 105us/step - loss: 0.4514 - acc: 0.9000 - val_loss: 0.4726 - val_acc: 0.9333\n","Epoch 112/200\n","270/270 [==============================] - 0s 118us/step - loss: 0.4470 - acc: 0.9000 - val_loss: 0.4680 - val_acc: 0.9333\n","Epoch 113/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.4428 - acc: 0.9000 - val_loss: 0.4645 - val_acc: 0.9333\n","Epoch 114/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.4385 - acc: 0.9000 - val_loss: 0.4606 - val_acc: 0.9333\n","Epoch 115/200\n","270/270 [==============================] - 0s 132us/step - loss: 0.4345 - acc: 0.9000 - val_loss: 0.4566 - val_acc: 0.9333\n","Epoch 116/200\n","270/270 [==============================] - 0s 121us/step - loss: 0.4304 - acc: 0.9000 - val_loss: 0.4529 - val_acc: 0.9333\n","Epoch 117/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.4265 - acc: 0.9000 - val_loss: 0.4489 - val_acc: 0.9333\n","Epoch 118/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.4226 - acc: 0.9037 - val_loss: 0.4454 - val_acc: 0.9333\n","Epoch 119/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.4187 - acc: 0.9000 - val_loss: 0.4423 - val_acc: 0.9333\n","Epoch 120/200\n","270/270 [==============================] - 0s 118us/step - loss: 0.4151 - acc: 0.9037 - val_loss: 0.4391 - val_acc: 0.9333\n","Epoch 121/200\n","270/270 [==============================] - 0s 126us/step - loss: 0.4114 - acc: 0.9037 - val_loss: 0.4354 - val_acc: 0.9333\n","Epoch 122/200\n","270/270 [==============================] - 0s 114us/step - loss: 0.4078 - acc: 0.9037 - val_loss: 0.4324 - val_acc: 0.9333\n","Epoch 123/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.4044 - acc: 0.9037 - val_loss: 0.4297 - val_acc: 0.9333\n","Epoch 124/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.4009 - acc: 0.9037 - val_loss: 0.4267 - val_acc: 0.9333\n","Epoch 125/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.3975 - acc: 0.9037 - val_loss: 0.4239 - val_acc: 0.9333\n","Epoch 126/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.3942 - acc: 0.9037 - val_loss: 0.4207 - val_acc: 0.9333\n","Epoch 127/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.3909 - acc: 0.9037 - val_loss: 0.4181 - val_acc: 0.9333\n","Epoch 128/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.3877 - acc: 0.9037 - val_loss: 0.4151 - val_acc: 0.9333\n","Epoch 129/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.3845 - acc: 0.9037 - val_loss: 0.4123 - val_acc: 0.9333\n","Epoch 130/200\n","270/270 [==============================] - 0s 104us/step - loss: 0.3815 - acc: 0.9037 - val_loss: 0.4097 - val_acc: 0.9333\n","Epoch 131/200\n","270/270 [==============================] - 0s 113us/step - loss: 0.3784 - acc: 0.9000 - val_loss: 0.4066 - val_acc: 0.9333\n","Epoch 132/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.3755 - acc: 0.9000 - val_loss: 0.4040 - val_acc: 0.9333\n","Epoch 133/200\n","270/270 [==============================] - 0s 113us/step - loss: 0.3727 - acc: 0.9000 - val_loss: 0.4014 - val_acc: 0.9333\n","Epoch 134/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.3698 - acc: 0.9000 - val_loss: 0.3987 - val_acc: 0.9333\n","Epoch 135/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.3671 - acc: 0.9000 - val_loss: 0.3964 - val_acc: 0.9333\n","Epoch 136/200\n","270/270 [==============================] - 0s 113us/step - loss: 0.3643 - acc: 0.9037 - val_loss: 0.3941 - val_acc: 0.9333\n","Epoch 137/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.3617 - acc: 0.9037 - val_loss: 0.3919 - val_acc: 0.9333\n","Epoch 138/200\n","270/270 [==============================] - 0s 113us/step - loss: 0.3591 - acc: 0.9037 - val_loss: 0.3894 - val_acc: 0.9333\n","Epoch 139/200\n","270/270 [==============================] - 0s 113us/step - loss: 0.3565 - acc: 0.9037 - val_loss: 0.3876 - val_acc: 0.9333\n","Epoch 140/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.3540 - acc: 0.9037 - val_loss: 0.3853 - val_acc: 0.9333\n","Epoch 141/200\n","270/270 [==============================] - 0s 181us/step - loss: 0.3515 - acc: 0.9074 - val_loss: 0.3828 - val_acc: 0.9333\n","Epoch 142/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.3492 - acc: 0.9074 - val_loss: 0.3809 - val_acc: 0.9333\n","Epoch 143/200\n","270/270 [==============================] - 0s 115us/step - loss: 0.3469 - acc: 0.9074 - val_loss: 0.3793 - val_acc: 0.9333\n","Epoch 144/200\n","270/270 [==============================] - 0s 104us/step - loss: 0.3446 - acc: 0.9074 - val_loss: 0.3772 - val_acc: 0.9333\n","Epoch 145/200\n","270/270 [==============================] - 0s 103us/step - loss: 0.3424 - acc: 0.9074 - val_loss: 0.3761 - val_acc: 0.9333\n","Epoch 146/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.3402 - acc: 0.9037 - val_loss: 0.3747 - val_acc: 0.9333\n","Epoch 147/200\n","270/270 [==============================] - 0s 105us/step - loss: 0.3380 - acc: 0.9037 - val_loss: 0.3734 - val_acc: 0.9333\n","Epoch 148/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.3359 - acc: 0.9037 - val_loss: 0.3720 - val_acc: 0.9333\n","Epoch 149/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.3339 - acc: 0.9074 - val_loss: 0.3702 - val_acc: 0.9333\n","Epoch 150/200\n","270/270 [==============================] - 0s 114us/step - loss: 0.3318 - acc: 0.9074 - val_loss: 0.3691 - val_acc: 0.9333\n","Epoch 151/200\n","270/270 [==============================] - 0s 105us/step - loss: 0.3299 - acc: 0.9074 - val_loss: 0.3676 - val_acc: 0.9333\n","Epoch 152/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.3279 - acc: 0.9037 - val_loss: 0.3663 - val_acc: 0.9333\n","Epoch 153/200\n","270/270 [==============================] - 0s 106us/step - loss: 0.3261 - acc: 0.9037 - val_loss: 0.3653 - val_acc: 0.9333\n","Epoch 154/200\n","270/270 [==============================] - 0s 128us/step - loss: 0.3243 - acc: 0.9037 - val_loss: 0.3638 - val_acc: 0.9333\n","Epoch 155/200\n","270/270 [==============================] - 0s 124us/step - loss: 0.3224 - acc: 0.9037 - val_loss: 0.3619 - val_acc: 0.9333\n","Epoch 156/200\n","270/270 [==============================] - 0s 139us/step - loss: 0.3207 - acc: 0.9074 - val_loss: 0.3598 - val_acc: 0.9333\n","Epoch 157/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.3190 - acc: 0.9074 - val_loss: 0.3588 - val_acc: 0.9333\n","Epoch 158/200\n","270/270 [==============================] - 0s 123us/step - loss: 0.3174 - acc: 0.9074 - val_loss: 0.3574 - val_acc: 0.9333\n","Epoch 159/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.3157 - acc: 0.9074 - val_loss: 0.3567 - val_acc: 0.9333\n","Epoch 160/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.3140 - acc: 0.9074 - val_loss: 0.3557 - val_acc: 0.9333\n","Epoch 161/200\n","270/270 [==============================] - 0s 117us/step - loss: 0.3125 - acc: 0.9074 - val_loss: 0.3548 - val_acc: 0.9333\n","Epoch 162/200\n","270/270 [==============================] - 0s 118us/step - loss: 0.3109 - acc: 0.9074 - val_loss: 0.3534 - val_acc: 0.9333\n","Epoch 163/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.3094 - acc: 0.9074 - val_loss: 0.3526 - val_acc: 0.9333\n","Epoch 164/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.3078 - acc: 0.9074 - val_loss: 0.3513 - val_acc: 0.9333\n","Epoch 165/200\n","270/270 [==============================] - 0s 116us/step - loss: 0.3063 - acc: 0.9074 - val_loss: 0.3502 - val_acc: 0.9333\n","Epoch 166/200\n","270/270 [==============================] - 0s 114us/step - loss: 0.3049 - acc: 0.9111 - val_loss: 0.3493 - val_acc: 0.9333\n","Epoch 167/200\n","270/270 [==============================] - 0s 115us/step - loss: 0.3035 - acc: 0.9111 - val_loss: 0.3485 - val_acc: 0.9333\n","Epoch 168/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.3020 - acc: 0.9111 - val_loss: 0.3469 - val_acc: 0.9333\n","Epoch 169/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.3007 - acc: 0.9111 - val_loss: 0.3458 - val_acc: 0.9333\n","Epoch 170/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.2993 - acc: 0.9111 - val_loss: 0.3452 - val_acc: 0.9333\n","Epoch 171/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.2980 - acc: 0.9111 - val_loss: 0.3443 - val_acc: 0.9333\n","Epoch 172/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.2966 - acc: 0.9111 - val_loss: 0.3434 - val_acc: 0.9333\n","Epoch 173/200\n","270/270 [==============================] - 0s 143us/step - loss: 0.2954 - acc: 0.9111 - val_loss: 0.3424 - val_acc: 0.9333\n","Epoch 174/200\n","270/270 [==============================] - 0s 146us/step - loss: 0.2942 - acc: 0.9111 - val_loss: 0.3415 - val_acc: 0.9333\n","Epoch 175/200\n","270/270 [==============================] - 0s 114us/step - loss: 0.2931 - acc: 0.9111 - val_loss: 0.3416 - val_acc: 0.9000\n","Epoch 176/200\n","270/270 [==============================] - 0s 121us/step - loss: 0.2919 - acc: 0.9111 - val_loss: 0.3407 - val_acc: 0.9000\n","Epoch 177/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.2907 - acc: 0.9111 - val_loss: 0.3399 - val_acc: 0.9000\n","Epoch 178/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.2897 - acc: 0.9111 - val_loss: 0.3390 - val_acc: 0.9333\n","Epoch 179/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.2886 - acc: 0.9111 - val_loss: 0.3379 - val_acc: 0.9333\n","Epoch 180/200\n","270/270 [==============================] - 0s 115us/step - loss: 0.2876 - acc: 0.9111 - val_loss: 0.3377 - val_acc: 0.9000\n","Epoch 181/200\n","270/270 [==============================] - 0s 105us/step - loss: 0.2865 - acc: 0.9111 - val_loss: 0.3376 - val_acc: 0.9000\n","Epoch 182/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.2855 - acc: 0.9111 - val_loss: 0.3368 - val_acc: 0.9000\n","Epoch 183/200\n","270/270 [==============================] - 0s 105us/step - loss: 0.2844 - acc: 0.9111 - val_loss: 0.3363 - val_acc: 0.9000\n","Epoch 184/200\n","270/270 [==============================] - 0s 111us/step - loss: 0.2835 - acc: 0.9111 - val_loss: 0.3359 - val_acc: 0.9000\n","Epoch 185/200\n","270/270 [==============================] - 0s 112us/step - loss: 0.2826 - acc: 0.9111 - val_loss: 0.3350 - val_acc: 0.9000\n","Epoch 186/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.2816 - acc: 0.9111 - val_loss: 0.3348 - val_acc: 0.9000\n","Epoch 187/200\n","270/270 [==============================] - 0s 113us/step - loss: 0.2806 - acc: 0.9111 - val_loss: 0.3338 - val_acc: 0.9000\n","Epoch 188/200\n","270/270 [==============================] - 0s 117us/step - loss: 0.2797 - acc: 0.9111 - val_loss: 0.3330 - val_acc: 0.9000\n","Epoch 189/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.2789 - acc: 0.9111 - val_loss: 0.3331 - val_acc: 0.9000\n","Epoch 190/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.2779 - acc: 0.9111 - val_loss: 0.3323 - val_acc: 0.9000\n","Epoch 191/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.2770 - acc: 0.9111 - val_loss: 0.3314 - val_acc: 0.9000\n","Epoch 192/200\n","270/270 [==============================] - 0s 106us/step - loss: 0.2761 - acc: 0.9111 - val_loss: 0.3312 - val_acc: 0.9000\n","Epoch 193/200\n","270/270 [==============================] - 0s 108us/step - loss: 0.2753 - acc: 0.9111 - val_loss: 0.3307 - val_acc: 0.9000\n","Epoch 194/200\n","270/270 [==============================] - 0s 126us/step - loss: 0.2745 - acc: 0.9111 - val_loss: 0.3305 - val_acc: 0.9000\n","Epoch 195/200\n","270/270 [==============================] - 0s 109us/step - loss: 0.2737 - acc: 0.9111 - val_loss: 0.3302 - val_acc: 0.9000\n","Epoch 196/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.2729 - acc: 0.9111 - val_loss: 0.3291 - val_acc: 0.9000\n","Epoch 197/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.2721 - acc: 0.9111 - val_loss: 0.3283 - val_acc: 0.9000\n","Epoch 198/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.2714 - acc: 0.9111 - val_loss: 0.3278 - val_acc: 0.9000\n","Epoch 199/200\n","270/270 [==============================] - 0s 110us/step - loss: 0.2707 - acc: 0.9148 - val_loss: 0.3279 - val_acc: 0.9000\n","Epoch 200/200\n","270/270 [==============================] - 0s 107us/step - loss: 0.2699 - acc: 0.9148 - val_loss: 0.3279 - val_acc: 0.9000\n","Accuracy :0.833\n","[[ 8  0  2]\n"," [ 2  7  1]\n"," [ 0  0 10]]\n","---------------------------------------------------------------------------------------------------------------------------\n","NH = 100\n","Nepochs = 200\n","N = 200\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_9 (InputLayer)         (None, 2)                 0         \n","_________________________________________________________________\n","dense_17 (Dense)             (None, 100)               300       \n","_________________________________________________________________\n","dense_18 (Dense)             (None, 3)                 303       \n","=================================================================\n","Total params: 603\n","Trainable params: 603\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 540 samples, validate on 60 samples\n","Epoch 1/200\n","540/540 [==============================] - 0s 779us/step - loss: 1.0970 - acc: 0.5056 - val_loss: 1.0952 - val_acc: 0.4333\n","Epoch 2/200\n","540/540 [==============================] - 0s 92us/step - loss: 1.0937 - acc: 0.4593 - val_loss: 1.0923 - val_acc: 0.4333\n","Epoch 3/200\n","540/540 [==============================] - 0s 96us/step - loss: 1.0904 - acc: 0.4815 - val_loss: 1.0891 - val_acc: 0.5333\n","Epoch 4/200\n","540/540 [==============================] - 0s 98us/step - loss: 1.0870 - acc: 0.5870 - val_loss: 1.0862 - val_acc: 0.6167\n","Epoch 5/200\n","540/540 [==============================] - 0s 114us/step - loss: 1.0831 - acc: 0.6963 - val_loss: 1.0822 - val_acc: 0.7333\n","Epoch 6/200\n","540/540 [==============================] - 0s 108us/step - loss: 1.0791 - acc: 0.7667 - val_loss: 1.0783 - val_acc: 0.7667\n","Epoch 7/200\n","540/540 [==============================] - 0s 98us/step - loss: 1.0744 - acc: 0.7648 - val_loss: 1.0737 - val_acc: 0.7667\n","Epoch 8/200\n","540/540 [==============================] - 0s 94us/step - loss: 1.0692 - acc: 0.7870 - val_loss: 1.0683 - val_acc: 0.8333\n","Epoch 9/200\n","540/540 [==============================] - 0s 130us/step - loss: 1.0636 - acc: 0.8037 - val_loss: 1.0626 - val_acc: 0.8333\n","Epoch 10/200\n","540/540 [==============================] - 0s 90us/step - loss: 1.0572 - acc: 0.8074 - val_loss: 1.0563 - val_acc: 0.8333\n","Epoch 11/200\n","540/540 [==============================] - 0s 93us/step - loss: 1.0502 - acc: 0.8167 - val_loss: 1.0487 - val_acc: 0.8333\n","Epoch 12/200\n","540/540 [==============================] - 0s 95us/step - loss: 1.0427 - acc: 0.8241 - val_loss: 1.0410 - val_acc: 0.8333\n","Epoch 13/200\n","540/540 [==============================] - 0s 102us/step - loss: 1.0342 - acc: 0.8481 - val_loss: 1.0315 - val_acc: 0.9167\n","Epoch 14/200\n","540/540 [==============================] - 0s 96us/step - loss: 1.0252 - acc: 0.8630 - val_loss: 1.0220 - val_acc: 0.9167\n","Epoch 15/200\n","540/540 [==============================] - 0s 94us/step - loss: 1.0154 - acc: 0.8704 - val_loss: 1.0116 - val_acc: 0.9167\n","Epoch 16/200\n","540/540 [==============================] - 0s 98us/step - loss: 1.0052 - acc: 0.8722 - val_loss: 1.0009 - val_acc: 0.9167\n","Epoch 17/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.9940 - acc: 0.8870 - val_loss: 0.9890 - val_acc: 0.9167\n","Epoch 18/200\n","540/540 [==============================] - 0s 99us/step - loss: 0.9823 - acc: 0.8889 - val_loss: 0.9767 - val_acc: 0.9167\n","Epoch 19/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.9700 - acc: 0.8944 - val_loss: 0.9635 - val_acc: 0.9333\n","Epoch 20/200\n","540/540 [==============================] - 0s 99us/step - loss: 0.9572 - acc: 0.8963 - val_loss: 0.9499 - val_acc: 0.9333\n","Epoch 21/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.9438 - acc: 0.8981 - val_loss: 0.9362 - val_acc: 0.9333\n","Epoch 22/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.9304 - acc: 0.8963 - val_loss: 0.9222 - val_acc: 0.9333\n","Epoch 23/200\n","540/540 [==============================] - 0s 105us/step - loss: 0.9161 - acc: 0.9000 - val_loss: 0.9069 - val_acc: 0.9333\n","Epoch 24/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.9019 - acc: 0.9037 - val_loss: 0.8913 - val_acc: 0.9333\n","Epoch 25/200\n","540/540 [==============================] - 0s 100us/step - loss: 0.8873 - acc: 0.9037 - val_loss: 0.8756 - val_acc: 0.9500\n","Epoch 26/200\n","540/540 [==============================] - 0s 98us/step - loss: 0.8724 - acc: 0.9037 - val_loss: 0.8602 - val_acc: 0.9500\n","Epoch 27/200\n","540/540 [==============================] - 0s 102us/step - loss: 0.8574 - acc: 0.9037 - val_loss: 0.8439 - val_acc: 0.9500\n","Epoch 28/200\n","540/540 [==============================] - 0s 134us/step - loss: 0.8425 - acc: 0.9037 - val_loss: 0.8286 - val_acc: 0.9500\n","Epoch 29/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.8272 - acc: 0.9074 - val_loss: 0.8117 - val_acc: 0.9500\n","Epoch 30/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.8120 - acc: 0.9074 - val_loss: 0.7960 - val_acc: 0.9333\n","Epoch 31/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.7970 - acc: 0.9074 - val_loss: 0.7792 - val_acc: 0.9333\n","Epoch 32/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.7820 - acc: 0.9074 - val_loss: 0.7634 - val_acc: 0.9333\n","Epoch 33/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.7672 - acc: 0.9093 - val_loss: 0.7468 - val_acc: 0.9333\n","Epoch 34/200\n","540/540 [==============================] - 0s 117us/step - loss: 0.7521 - acc: 0.9111 - val_loss: 0.7308 - val_acc: 0.9333\n","Epoch 35/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.7375 - acc: 0.9111 - val_loss: 0.7146 - val_acc: 0.9333\n","Epoch 36/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.7231 - acc: 0.9111 - val_loss: 0.6989 - val_acc: 0.9333\n","Epoch 37/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.7088 - acc: 0.9111 - val_loss: 0.6830 - val_acc: 0.9333\n","Epoch 38/200\n","540/540 [==============================] - 0s 109us/step - loss: 0.6946 - acc: 0.9093 - val_loss: 0.6676 - val_acc: 0.9333\n","Epoch 39/200\n","540/540 [==============================] - 0s 101us/step - loss: 0.6808 - acc: 0.9130 - val_loss: 0.6523 - val_acc: 0.9333\n","Epoch 40/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.6672 - acc: 0.9111 - val_loss: 0.6371 - val_acc: 0.9333\n","Epoch 41/200\n","540/540 [==============================] - 0s 101us/step - loss: 0.6540 - acc: 0.9111 - val_loss: 0.6225 - val_acc: 0.9333\n","Epoch 42/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.6409 - acc: 0.9111 - val_loss: 0.6080 - val_acc: 0.9333\n","Epoch 43/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.6284 - acc: 0.9093 - val_loss: 0.5939 - val_acc: 0.9333\n","Epoch 44/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.6158 - acc: 0.9093 - val_loss: 0.5797 - val_acc: 0.9333\n","Epoch 45/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.6037 - acc: 0.9093 - val_loss: 0.5660 - val_acc: 0.9333\n","Epoch 46/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.5921 - acc: 0.9093 - val_loss: 0.5523 - val_acc: 0.9333\n","Epoch 47/200\n","540/540 [==============================] - 0s 139us/step - loss: 0.5806 - acc: 0.9093 - val_loss: 0.5391 - val_acc: 0.9333\n","Epoch 48/200\n","540/540 [==============================] - 0s 98us/step - loss: 0.5694 - acc: 0.9074 - val_loss: 0.5265 - val_acc: 0.9333\n","Epoch 49/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.5588 - acc: 0.9074 - val_loss: 0.5145 - val_acc: 0.9333\n","Epoch 50/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.5482 - acc: 0.9074 - val_loss: 0.5025 - val_acc: 0.9333\n","Epoch 51/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.5381 - acc: 0.9074 - val_loss: 0.4908 - val_acc: 0.9333\n","Epoch 52/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.5283 - acc: 0.9074 - val_loss: 0.4789 - val_acc: 0.9333\n","Epoch 53/200\n","540/540 [==============================] - 0s 102us/step - loss: 0.5186 - acc: 0.9074 - val_loss: 0.4679 - val_acc: 0.9333\n","Epoch 54/200\n","540/540 [==============================] - 0s 100us/step - loss: 0.5096 - acc: 0.9093 - val_loss: 0.4567 - val_acc: 0.9333\n","Epoch 55/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.5006 - acc: 0.9093 - val_loss: 0.4466 - val_acc: 0.9333\n","Epoch 56/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.4919 - acc: 0.9093 - val_loss: 0.4365 - val_acc: 0.9333\n","Epoch 57/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.4836 - acc: 0.9093 - val_loss: 0.4263 - val_acc: 0.9333\n","Epoch 58/200\n","540/540 [==============================] - 0s 99us/step - loss: 0.4756 - acc: 0.9093 - val_loss: 0.4165 - val_acc: 0.9500\n","Epoch 59/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.4680 - acc: 0.9093 - val_loss: 0.4066 - val_acc: 0.9500\n","Epoch 60/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.4603 - acc: 0.9093 - val_loss: 0.3983 - val_acc: 0.9500\n","Epoch 61/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.4532 - acc: 0.9093 - val_loss: 0.3894 - val_acc: 0.9500\n","Epoch 62/200\n","540/540 [==============================] - 0s 102us/step - loss: 0.4462 - acc: 0.9093 - val_loss: 0.3811 - val_acc: 0.9500\n","Epoch 63/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.4396 - acc: 0.9093 - val_loss: 0.3726 - val_acc: 0.9500\n","Epoch 64/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.4332 - acc: 0.9093 - val_loss: 0.3644 - val_acc: 0.9500\n","Epoch 65/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.4271 - acc: 0.9074 - val_loss: 0.3568 - val_acc: 0.9500\n","Epoch 66/200\n","540/540 [==============================] - 0s 105us/step - loss: 0.4210 - acc: 0.9074 - val_loss: 0.3492 - val_acc: 0.9500\n","Epoch 67/200\n","540/540 [==============================] - 0s 130us/step - loss: 0.4154 - acc: 0.9074 - val_loss: 0.3419 - val_acc: 0.9500\n","Epoch 68/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.4097 - acc: 0.9056 - val_loss: 0.3350 - val_acc: 0.9500\n","Epoch 69/200\n","540/540 [==============================] - 0s 100us/step - loss: 0.4046 - acc: 0.9056 - val_loss: 0.3281 - val_acc: 0.9500\n","Epoch 70/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3995 - acc: 0.9056 - val_loss: 0.3213 - val_acc: 0.9500\n","Epoch 71/200\n","540/540 [==============================] - 0s 99us/step - loss: 0.3946 - acc: 0.9037 - val_loss: 0.3150 - val_acc: 0.9500\n","Epoch 72/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3900 - acc: 0.9056 - val_loss: 0.3089 - val_acc: 0.9667\n","Epoch 73/200\n","540/540 [==============================] - 0s 102us/step - loss: 0.3854 - acc: 0.9056 - val_loss: 0.3030 - val_acc: 0.9667\n","Epoch 74/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.3811 - acc: 0.9056 - val_loss: 0.2974 - val_acc: 0.9667\n","Epoch 75/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3770 - acc: 0.9074 - val_loss: 0.2917 - val_acc: 0.9667\n","Epoch 76/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.3731 - acc: 0.9056 - val_loss: 0.2866 - val_acc: 0.9667\n","Epoch 77/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.3693 - acc: 0.9056 - val_loss: 0.2813 - val_acc: 0.9667\n","Epoch 78/200\n","540/540 [==============================] - 0s 98us/step - loss: 0.3656 - acc: 0.9074 - val_loss: 0.2765 - val_acc: 0.9667\n","Epoch 79/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.3622 - acc: 0.9074 - val_loss: 0.2717 - val_acc: 0.9667\n","Epoch 80/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.3588 - acc: 0.9074 - val_loss: 0.2672 - val_acc: 0.9667\n","Epoch 81/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.3557 - acc: 0.9074 - val_loss: 0.2628 - val_acc: 0.9667\n","Epoch 82/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3527 - acc: 0.9074 - val_loss: 0.2585 - val_acc: 0.9667\n","Epoch 83/200\n","540/540 [==============================] - 0s 101us/step - loss: 0.3496 - acc: 0.9074 - val_loss: 0.2543 - val_acc: 0.9667\n","Epoch 84/200\n","540/540 [==============================] - 0s 99us/step - loss: 0.3469 - acc: 0.9074 - val_loss: 0.2501 - val_acc: 0.9667\n","Epoch 85/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.3441 - acc: 0.9074 - val_loss: 0.2464 - val_acc: 0.9667\n","Epoch 86/200\n","540/540 [==============================] - 0s 140us/step - loss: 0.3415 - acc: 0.9074 - val_loss: 0.2429 - val_acc: 0.9667\n","Epoch 87/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3391 - acc: 0.9074 - val_loss: 0.2390 - val_acc: 0.9667\n","Epoch 88/200\n","540/540 [==============================] - 0s 99us/step - loss: 0.3367 - acc: 0.9074 - val_loss: 0.2356 - val_acc: 0.9667\n","Epoch 89/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3345 - acc: 0.9074 - val_loss: 0.2322 - val_acc: 0.9667\n","Epoch 90/200\n","540/540 [==============================] - 0s 90us/step - loss: 0.3323 - acc: 0.9074 - val_loss: 0.2289 - val_acc: 0.9667\n","Epoch 91/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3302 - acc: 0.9074 - val_loss: 0.2258 - val_acc: 0.9667\n","Epoch 92/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.3282 - acc: 0.9074 - val_loss: 0.2227 - val_acc: 0.9667\n","Epoch 93/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3262 - acc: 0.9074 - val_loss: 0.2199 - val_acc: 0.9667\n","Epoch 94/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.3244 - acc: 0.9074 - val_loss: 0.2171 - val_acc: 0.9667\n","Epoch 95/200\n","540/540 [==============================] - 0s 119us/step - loss: 0.3226 - acc: 0.9074 - val_loss: 0.2143 - val_acc: 0.9667\n","Epoch 96/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3209 - acc: 0.9074 - val_loss: 0.2118 - val_acc: 0.9667\n","Epoch 97/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.3194 - acc: 0.9074 - val_loss: 0.2092 - val_acc: 0.9667\n","Epoch 98/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.3177 - acc: 0.9074 - val_loss: 0.2067 - val_acc: 0.9667\n","Epoch 99/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3163 - acc: 0.9074 - val_loss: 0.2045 - val_acc: 0.9667\n","Epoch 100/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.3148 - acc: 0.9074 - val_loss: 0.2022 - val_acc: 0.9667\n","Epoch 101/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.3135 - acc: 0.9074 - val_loss: 0.1996 - val_acc: 0.9667\n","Epoch 102/200\n","540/540 [==============================] - 0s 101us/step - loss: 0.3121 - acc: 0.9074 - val_loss: 0.1975 - val_acc: 0.9667\n","Epoch 103/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.3108 - acc: 0.9074 - val_loss: 0.1957 - val_acc: 0.9667\n","Epoch 104/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.3097 - acc: 0.9074 - val_loss: 0.1933 - val_acc: 0.9667\n","Epoch 105/200\n","540/540 [==============================] - 0s 130us/step - loss: 0.3084 - acc: 0.9074 - val_loss: 0.1915 - val_acc: 0.9667\n","Epoch 106/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.3074 - acc: 0.9056 - val_loss: 0.1896 - val_acc: 0.9667\n","Epoch 107/200\n","540/540 [==============================] - 0s 99us/step - loss: 0.3063 - acc: 0.9074 - val_loss: 0.1879 - val_acc: 0.9667\n","Epoch 108/200\n","540/540 [==============================] - 0s 99us/step - loss: 0.3052 - acc: 0.9074 - val_loss: 0.1861 - val_acc: 0.9667\n","Epoch 109/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.3041 - acc: 0.9074 - val_loss: 0.1842 - val_acc: 0.9667\n","Epoch 110/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.3032 - acc: 0.9074 - val_loss: 0.1826 - val_acc: 0.9667\n","Epoch 111/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.3022 - acc: 0.9074 - val_loss: 0.1809 - val_acc: 0.9667\n","Epoch 112/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.3013 - acc: 0.9074 - val_loss: 0.1793 - val_acc: 0.9667\n","Epoch 113/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.3005 - acc: 0.9093 - val_loss: 0.1779 - val_acc: 0.9667\n","Epoch 114/200\n","540/540 [==============================] - 0s 103us/step - loss: 0.2996 - acc: 0.9093 - val_loss: 0.1765 - val_acc: 0.9667\n","Epoch 115/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.2988 - acc: 0.9074 - val_loss: 0.1750 - val_acc: 0.9667\n","Epoch 116/200\n","540/540 [==============================] - 0s 98us/step - loss: 0.2980 - acc: 0.9074 - val_loss: 0.1734 - val_acc: 0.9667\n","Epoch 117/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.2972 - acc: 0.9093 - val_loss: 0.1721 - val_acc: 0.9667\n","Epoch 118/200\n","540/540 [==============================] - 0s 99us/step - loss: 0.2965 - acc: 0.9093 - val_loss: 0.1706 - val_acc: 0.9667\n","Epoch 119/200\n","540/540 [==============================] - 0s 98us/step - loss: 0.2958 - acc: 0.9093 - val_loss: 0.1695 - val_acc: 0.9500\n","Epoch 120/200\n","540/540 [==============================] - 0s 101us/step - loss: 0.2951 - acc: 0.9093 - val_loss: 0.1681 - val_acc: 0.9667\n","Epoch 121/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.2945 - acc: 0.9093 - val_loss: 0.1668 - val_acc: 0.9500\n","Epoch 122/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.2937 - acc: 0.9111 - val_loss: 0.1657 - val_acc: 0.9500\n","Epoch 123/200\n","540/540 [==============================] - 0s 98us/step - loss: 0.2932 - acc: 0.9093 - val_loss: 0.1643 - val_acc: 0.9500\n","Epoch 124/200\n","540/540 [==============================] - 0s 137us/step - loss: 0.2927 - acc: 0.9093 - val_loss: 0.1630 - val_acc: 0.9667\n","Epoch 125/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.2920 - acc: 0.9093 - val_loss: 0.1622 - val_acc: 0.9500\n","Epoch 126/200\n","540/540 [==============================] - 0s 98us/step - loss: 0.2914 - acc: 0.9111 - val_loss: 0.1611 - val_acc: 0.9500\n","Epoch 127/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.2911 - acc: 0.9093 - val_loss: 0.1597 - val_acc: 0.9667\n","Epoch 128/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.2903 - acc: 0.9093 - val_loss: 0.1589 - val_acc: 0.9500\n","Epoch 129/200\n","540/540 [==============================] - 0s 109us/step - loss: 0.2898 - acc: 0.9093 - val_loss: 0.1579 - val_acc: 0.9500\n","Epoch 130/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.2894 - acc: 0.9093 - val_loss: 0.1569 - val_acc: 0.9500\n","Epoch 131/200\n","540/540 [==============================] - 0s 102us/step - loss: 0.2889 - acc: 0.9093 - val_loss: 0.1560 - val_acc: 0.9500\n","Epoch 132/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.2885 - acc: 0.9111 - val_loss: 0.1552 - val_acc: 0.9500\n","Epoch 133/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.2881 - acc: 0.9111 - val_loss: 0.1540 - val_acc: 0.9500\n","Epoch 134/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.2875 - acc: 0.9111 - val_loss: 0.1533 - val_acc: 0.9500\n","Epoch 135/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.2872 - acc: 0.9111 - val_loss: 0.1524 - val_acc: 0.9500\n","Epoch 136/200\n","540/540 [==============================] - 0s 99us/step - loss: 0.2867 - acc: 0.9130 - val_loss: 0.1516 - val_acc: 0.9500\n","Epoch 137/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.2864 - acc: 0.9130 - val_loss: 0.1505 - val_acc: 0.9500\n","Epoch 138/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.2859 - acc: 0.9130 - val_loss: 0.1498 - val_acc: 0.9500\n","Epoch 139/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.2857 - acc: 0.9130 - val_loss: 0.1488 - val_acc: 0.9500\n","Epoch 140/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.2852 - acc: 0.9130 - val_loss: 0.1483 - val_acc: 0.9500\n","Epoch 141/200\n","540/540 [==============================] - 0s 100us/step - loss: 0.2848 - acc: 0.9130 - val_loss: 0.1477 - val_acc: 0.9500\n","Epoch 142/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.2845 - acc: 0.9130 - val_loss: 0.1469 - val_acc: 0.9500\n","Epoch 143/200\n","540/540 [==============================] - 0s 142us/step - loss: 0.2841 - acc: 0.9130 - val_loss: 0.1461 - val_acc: 0.9500\n","Epoch 144/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.2840 - acc: 0.9130 - val_loss: 0.1455 - val_acc: 0.9500\n","Epoch 145/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.2835 - acc: 0.9130 - val_loss: 0.1446 - val_acc: 0.9500\n","Epoch 146/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.2833 - acc: 0.9130 - val_loss: 0.1440 - val_acc: 0.9500\n","Epoch 147/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.2830 - acc: 0.9130 - val_loss: 0.1432 - val_acc: 0.9500\n","Epoch 148/200\n","540/540 [==============================] - 0s 104us/step - loss: 0.2826 - acc: 0.9130 - val_loss: 0.1424 - val_acc: 0.9500\n","Epoch 149/200\n","540/540 [==============================] - 0s 106us/step - loss: 0.2823 - acc: 0.9130 - val_loss: 0.1419 - val_acc: 0.9500\n","Epoch 150/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.2821 - acc: 0.9130 - val_loss: 0.1411 - val_acc: 0.9500\n","Epoch 151/200\n","540/540 [==============================] - 0s 99us/step - loss: 0.2818 - acc: 0.9130 - val_loss: 0.1404 - val_acc: 0.9500\n","Epoch 152/200\n","540/540 [==============================] - 0s 110us/step - loss: 0.2816 - acc: 0.9130 - val_loss: 0.1401 - val_acc: 0.9500\n","Epoch 153/200\n","540/540 [==============================] - 0s 102us/step - loss: 0.2813 - acc: 0.9130 - val_loss: 0.1396 - val_acc: 0.9500\n","Epoch 154/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.2810 - acc: 0.9130 - val_loss: 0.1390 - val_acc: 0.9500\n","Epoch 155/200\n","540/540 [==============================] - 0s 102us/step - loss: 0.2807 - acc: 0.9130 - val_loss: 0.1383 - val_acc: 0.9500\n","Epoch 156/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.2805 - acc: 0.9130 - val_loss: 0.1377 - val_acc: 0.9500\n","Epoch 157/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.2803 - acc: 0.9130 - val_loss: 0.1372 - val_acc: 0.9500\n","Epoch 158/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.2800 - acc: 0.9130 - val_loss: 0.1366 - val_acc: 0.9500\n","Epoch 159/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.2799 - acc: 0.9130 - val_loss: 0.1362 - val_acc: 0.9500\n","Epoch 160/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.2797 - acc: 0.9130 - val_loss: 0.1355 - val_acc: 0.9500\n","Epoch 161/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.2794 - acc: 0.9130 - val_loss: 0.1349 - val_acc: 0.9500\n","Epoch 162/200\n","540/540 [==============================] - 0s 136us/step - loss: 0.2793 - acc: 0.9130 - val_loss: 0.1343 - val_acc: 0.9500\n","Epoch 163/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.2791 - acc: 0.9148 - val_loss: 0.1342 - val_acc: 0.9500\n","Epoch 164/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.2788 - acc: 0.9148 - val_loss: 0.1338 - val_acc: 0.9500\n","Epoch 165/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.2787 - acc: 0.9148 - val_loss: 0.1332 - val_acc: 0.9500\n","Epoch 166/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.2784 - acc: 0.9148 - val_loss: 0.1327 - val_acc: 0.9500\n","Epoch 167/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.2783 - acc: 0.9148 - val_loss: 0.1322 - val_acc: 0.9500\n","Epoch 168/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.2782 - acc: 0.9148 - val_loss: 0.1316 - val_acc: 0.9500\n","Epoch 169/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.2779 - acc: 0.9148 - val_loss: 0.1315 - val_acc: 0.9500\n","Epoch 170/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.2778 - acc: 0.9148 - val_loss: 0.1311 - val_acc: 0.9500\n","Epoch 171/200\n","540/540 [==============================] - 0s 99us/step - loss: 0.2776 - acc: 0.9148 - val_loss: 0.1308 - val_acc: 0.9500\n","Epoch 172/200\n","540/540 [==============================] - 0s 98us/step - loss: 0.2775 - acc: 0.9148 - val_loss: 0.1301 - val_acc: 0.9500\n","Epoch 173/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.2772 - acc: 0.9148 - val_loss: 0.1297 - val_acc: 0.9500\n","Epoch 174/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.2771 - acc: 0.9148 - val_loss: 0.1293 - val_acc: 0.9500\n","Epoch 175/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.2769 - acc: 0.9148 - val_loss: 0.1289 - val_acc: 0.9500\n","Epoch 176/200\n","540/540 [==============================] - 0s 98us/step - loss: 0.2768 - acc: 0.9148 - val_loss: 0.1285 - val_acc: 0.9500\n","Epoch 177/200\n","540/540 [==============================] - 0s 101us/step - loss: 0.2766 - acc: 0.9148 - val_loss: 0.1282 - val_acc: 0.9500\n","Epoch 178/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.2767 - acc: 0.9148 - val_loss: 0.1281 - val_acc: 0.9500\n","Epoch 179/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.2764 - acc: 0.9148 - val_loss: 0.1274 - val_acc: 0.9500\n","Epoch 180/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.2763 - acc: 0.9148 - val_loss: 0.1273 - val_acc: 0.9500\n","Epoch 181/200\n","540/540 [==============================] - 0s 105us/step - loss: 0.2761 - acc: 0.9148 - val_loss: 0.1269 - val_acc: 0.9500\n","Epoch 182/200\n","540/540 [==============================] - 0s 107us/step - loss: 0.2759 - acc: 0.9148 - val_loss: 0.1265 - val_acc: 0.9500\n","Epoch 183/200\n","540/540 [==============================] - 0s 98us/step - loss: 0.2758 - acc: 0.9148 - val_loss: 0.1262 - val_acc: 0.9500\n","Epoch 184/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.2758 - acc: 0.9148 - val_loss: 0.1255 - val_acc: 0.9500\n","Epoch 185/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.2755 - acc: 0.9148 - val_loss: 0.1254 - val_acc: 0.9500\n","Epoch 186/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.2754 - acc: 0.9148 - val_loss: 0.1251 - val_acc: 0.9500\n","Epoch 187/200\n","540/540 [==============================] - 0s 104us/step - loss: 0.2753 - acc: 0.9148 - val_loss: 0.1249 - val_acc: 0.9500\n","Epoch 188/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.2752 - acc: 0.9148 - val_loss: 0.1244 - val_acc: 0.9500\n","Epoch 189/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.2751 - acc: 0.9148 - val_loss: 0.1242 - val_acc: 0.9500\n","Epoch 190/200\n","540/540 [==============================] - 0s 94us/step - loss: 0.2750 - acc: 0.9148 - val_loss: 0.1239 - val_acc: 0.9500\n","Epoch 191/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.2749 - acc: 0.9148 - val_loss: 0.1237 - val_acc: 0.9500\n","Epoch 192/200\n","540/540 [==============================] - 0s 91us/step - loss: 0.2748 - acc: 0.9148 - val_loss: 0.1233 - val_acc: 0.9500\n","Epoch 193/200\n","540/540 [==============================] - 0s 97us/step - loss: 0.2747 - acc: 0.9148 - val_loss: 0.1231 - val_acc: 0.9500\n","Epoch 194/200\n","540/540 [==============================] - 0s 99us/step - loss: 0.2745 - acc: 0.9148 - val_loss: 0.1230 - val_acc: 0.9500\n","Epoch 195/200\n","540/540 [==============================] - 0s 93us/step - loss: 0.2745 - acc: 0.9148 - val_loss: 0.1226 - val_acc: 0.9500\n","Epoch 196/200\n","540/540 [==============================] - 0s 96us/step - loss: 0.2743 - acc: 0.9148 - val_loss: 0.1222 - val_acc: 0.9500\n","Epoch 197/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.2743 - acc: 0.9148 - val_loss: 0.1220 - val_acc: 0.9500\n","Epoch 198/200\n","540/540 [==============================] - 0s 95us/step - loss: 0.2741 - acc: 0.9148 - val_loss: 0.1219 - val_acc: 0.9500\n","Epoch 199/200\n","540/540 [==============================] - 0s 92us/step - loss: 0.2742 - acc: 0.9167 - val_loss: 0.1213 - val_acc: 0.9500\n","Epoch 200/200\n","540/540 [==============================] - 0s 104us/step - loss: 0.2740 - acc: 0.9167 - val_loss: 0.1212 - val_acc: 0.9500\n","Accuracy :0.833\n","[[9 1 0]\n"," [2 8 0]\n"," [2 0 8]]\n","---------------------------------------------------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PEA4OpyKG182","colab_type":"code","outputId":"3a0b509f-c6c1-41e6-e418-150b73afe493","executionInfo":{"status":"ok","timestamp":1560421146292,"user_tz":-330,"elapsed":71332,"user":{"displayName":"Soham Tiwari","photoUrl":"https://lh4.googleusercontent.com/-XiZ5rEdluPQ/AAAAAAAAAAI/AAAAAAAAH24/lvfjFM0g8Uw/s64/photo.jpg","userId":"06949155908663757402"}},"colab":{"base_uri":"https://localhost:8080/","height":706}},"source":["print(k)\n","i=0\n","for nh in Nh:\n","  for ne in Nepochs:\n","    for n in N:\n","      if(i<=k):\n","        print(\"NH =\",nh)\n","        print(\"Nepochs =\",ne)\n","        print(\"N =\",n)\n","        print(\"Accuracy =\",accuracy[i])\n","        i+=1\n","        print(\"=========================================================================================================================================\")"],"execution_count":20,"outputs":[{"output_type":"stream","text":["8\n","NH = 50\n","Nepochs = 100\n","N = 100\n","Accuracy = 0.9333333373069763\n","=========================================================================================================================================\n","NH = 50\n","Nepochs = 100\n","N = 200\n","Accuracy = 0.8999999761581421\n","=========================================================================================================================================\n","NH = 50\n","Nepochs = 200\n","N = 100\n","Accuracy = 0.8999999761581421\n","=========================================================================================================================================\n","NH = 50\n","Nepochs = 200\n","N = 200\n","Accuracy = 0.9666666388511658\n","=========================================================================================================================================\n","NH = 100\n","Nepochs = 100\n","N = 100\n","Accuracy = 0.8999999761581421\n","=========================================================================================================================================\n","NH = 100\n","Nepochs = 100\n","N = 200\n","Accuracy = 0.9666666388511658\n","=========================================================================================================================================\n","NH = 100\n","Nepochs = 200\n","N = 100\n","Accuracy = 0.8333333134651184\n","=========================================================================================================================================\n","NH = 100\n","Nepochs = 200\n","N = 200\n","Accuracy = 0.8333333134651184\n","=========================================================================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WM93IogvQSu_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}