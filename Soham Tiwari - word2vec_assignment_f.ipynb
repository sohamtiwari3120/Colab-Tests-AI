{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pMOpX0ShR1qw"
   },
   "source": [
    "# (Word Embedding Training, Visualization, Evaluation)\n",
    "Word Vectors are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, translation, etc., so it is important to build some intuitions as to their strengths and weaknesses.  <br>\n",
    "**Note on Terminology:**\n",
    "The terms \"word vectors\" and \"word embeddings\" are often used interchangeably. The term \"embedding\" refers to the fact that we are encoding aspects of a word's meaning in a lower dimensional space. As Wikipedia states, \"conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8eR7HwM9R1qx"
   },
   "source": [
    "# Collect Data\n",
    "The dataset  contains  10 sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YlzcCZNV6FXa"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LmpttdRER1qy"
   },
   "outputs": [],
   "source": [
    "corpus = ['king is a strong man', \n",
    "          'queen is a wise woman', \n",
    "          'boy is a young man',\n",
    "          'girl is a young woman',\n",
    "          'prince is a young king',\n",
    "          'princess is a young queen',\n",
    "          'man is strong', \n",
    "          'woman is pretty',\n",
    "          'prince is a boy will be king',\n",
    "          'princess is a girl will be queen',\n",
    "          'prince is a strong boy',\n",
    "          'boy is strong',\n",
    "          'girl is pretty',\n",
    "          'girl will be woman',\n",
    "          'boy will be man']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2CyRgqB8iQXE"
   },
   "source": [
    "# Tokenization\n",
    "For many NLP tasks, the first thing you need to do is to tokenize your raw text into lists of words. Suppose your have *text = \"king is a strong man\"*  You can just use *text.split(\"  \")* to break the sentences into a list of words. You will get output as \"['king', 'is', 'a', 'strong', 'man']\"\n",
    "Write and run your code in the next cell to tokenize all the sentences. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "Ox2DwyyGiq9U",
    "nbgrader": {
     "checksum": "a3a8ba36e7889f61285f44cf84e61d38",
     "grade": false,
     "grade_id": "cell-f44fb1bedc1345df",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def Tokenization(corpus):\n",
    "    '''\n",
    "    Input:\n",
    "      corpus: list of sentences(Eg., The list 'corpus' contains 10 sentences as defined above)\n",
    "    \n",
    "    Output:\n",
    "           y: list of lists, each sentence in corpus is broken into a list of words (obtained by tokenizing all the sentences)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "J6uScs9c6A9E",
    "outputId": "e0f6b433-21d1-44c1-f9e7-d7f71073f8d9"
   },
   "outputs": [],
   "source": [
    "'''test for Tokenization'''\n",
    "def test_Tokenization():\n",
    "  y = Tokenization(corpus)\n",
    "  assert y[0]==['king', 'is', 'a', 'strong', 'man']\n",
    "  assert y[9]==['princess', 'is', 'a', 'girl', 'will', 'be', 'queen']\n",
    "  print('Test passed', '\\U0001F44D')\n",
    "test_Tokenization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ECxqEIUR1q2"
   },
   "source": [
    "# Remove stop words\n",
    "## Stopwords: \n",
    "Words such as articles and some verbs are usually considered stop words because they donâ€™t help us to find the context or the true meaning of a sentence. These are words that can be removed without any negative consequences to the final model that you are training.\n",
    "In order for efficiency of creating word vectors, we will remove commonly used words.<br>\n",
    "For our case, lets take following list as stopwords. <br>\n",
    "stop_words = ['is', 'a', 'will', 'be'] <br>\n",
    "For example, ouput corrosponding to *\"king is a strong man\"* will be ['king', 'strong', 'man'] and your function should return list which don't have stop-words in it.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "TDn2SGWoR1q2",
    "nbgrader": {
     "checksum": "65ae7c6bcd2f507a58d56f61472033bc",
     "grade": false,
     "grade_id": "cell-5f1142df8e0ecf96",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(corpus):\n",
    "    '''\n",
    "    Input:\n",
    "      corpus: list of sentences(Eg., The list 'corpus' contains 10 sentences as defined above)\n",
    "    \n",
    "    Output:\n",
    "      corpus_wo_stopwords: list of lists, each sentence in corpus is broken into a list of words excluding stop words \n",
    "                           (obtained after tokenizing the corpus followed by removing stop words)\n",
    "    '''\n",
    "    \n",
    "    # Get stop-word list\n",
    "    stop_words = ['is', 'a', 'will', 'be']\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    return corpus_wo_stopwords\n",
    "    \n",
    "PP_corpus = remove_stop_words(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "929fMMYy9MtR",
    "outputId": "001c9288-d7a6-49d1-c852-39ed5a6c37b4"
   },
   "outputs": [],
   "source": [
    "'''test for remove_stop_words'''\n",
    "def test_remove_stop_words():\n",
    "  assert set(PP_corpus[0])==set(['king', 'strong', 'man'])\n",
    "  assert set(PP_corpus[9])==set(['princess', 'girl', 'queen'])\n",
    "  print('Test passed', '\\U0001F44D')\n",
    "test_remove_stop_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_mJe1FMznuAY"
   },
   "source": [
    "# Create vocabulary\n",
    "Building a vocabulary is nothing more than assigning a unique integer id to each word in the dataset. So, a vocabulary is basically a python dictionary data structure. The dictionary will map the word to a number. <br>\n",
    " E.g. dictionary['love'] = 520\n",
    "Your function should return dictionary for unique words.<br>\n",
    "For example, if you have three unique words, namely, *'princess', 'queen', 'girl',* then your output should be {'princess': 0, 'queen': 1, 'girl': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "763v5Kt15Bx1"
   },
   "source": [
    "### 1. Find out set of unique words in PP_corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "GECBCFdTR1q8",
    "nbgrader": {
     "checksum": "3cb7aa51b845e9ba3df1dde2a762e7e8",
     "grade": false,
     "grade_id": "cell-cd979bf579a19047",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_unique_words(PP_corpus):\n",
    "    '''\n",
    "    Input:\n",
    "        PP_corpus: list of lists containing the list of words (obtained after tokenizing the corpus followed by removing stop words)\n",
    "    \n",
    "    Output:\n",
    "        unique_words: set of unique words in PP_corpus\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return unique_words\n",
    "  \n",
    "unique_words = get_unique_words(PP_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "My73M_UQ8Mz2",
    "outputId": "5ac0a9bb-7b8f-4023-d8fe-a4d2fd5d401d"
   },
   "outputs": [],
   "source": [
    "'''test for remove_stop_words'''\n",
    "def test_unique_word():\n",
    "  assert unique_words=={'strong', 'pretty', 'wise', 'queen', 'man', 'prince', 'king', 'young', 'princess', 'woman', 'boy', 'girl'}\n",
    "  print('Test passed', '\\U0001F44D')\n",
    "test_unique_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3E7dPehU6Yyo"
   },
   "source": [
    "### 2. Map the unique words to integers starting from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "532tx5mE5AzU",
    "nbgrader": {
     "checksum": "f45bcc7463ac3de2d3f6e56823bfe2a0",
     "grade": false,
     "grade_id": "cell-97dd7d55f27d6439",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mapping(unique_words):\n",
    "    '''\n",
    "    Input:\n",
    "        unique_words: set of unique words in PP_corpus  \n",
    "    Output:\n",
    "        word2int: dictionary which maps the words in set unique_words to integers starting from 0 (of same length as unique_words)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    return word2int\n",
    "    \n",
    "word2int = mapping(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "PRAWAwJS9Nel",
    "outputId": "c1b0451c-f405-466e-8532-380764c435fc"
   },
   "outputs": [],
   "source": [
    "'''test for mapping'''\n",
    "def test_mapping():\n",
    "  assert len(word2int)==12\n",
    "  assert np.unique(list(word2int.values())).shape[0]==12\n",
    "  print('Test passed', '\\U0001F44D')\n",
    "test_mapping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wtktjLGgR1q-"
   },
   "source": [
    "\n",
    "## Prepare data for Skip-Gram Model \n",
    "In skip gram architecture of word2vec, the input is the center word and the predictions are the context words. Consider an array of words W, if W(i) is the input (center word), then W(i-2), W(i-1), W(i+1), and W(i+2) are the context words, for a sliding window size of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wXKd_rxIWQ8R"
   },
   "source": [
    "![Sliding Window](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/word2vec_diagram-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lGwWjSDOR1rE"
   },
   "source": [
    "# data generation\n",
    "The final structure of your data should be a list of tuples $(x, y)$.\n",
    "$x$ is the id of the target word (the center word in current window) and $y$ is the id of the context word. This is well illustrated in above figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "K1bFfUxqR1rF",
    "nbgrader": {
     "checksum": "fb1e822e32613070e6d895426a72470b",
     "grade": false,
     "grade_id": "cell-459cc7de8466c0fc",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def data_gen(PP_corpus, window_size):\n",
    "    '''\n",
    "    Input:\n",
    "        PP_corpus: list of lists (obtained after tokenizing the corpus followed by removing stop words) \n",
    "        window_size: int, window size as described above\n",
    "    Output:\n",
    "        data: list of tuples (x, y). x is the id of the target word (the center word in current window) and y is the id of the context word\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    return data\n",
    "    \n",
    "data = data_gen(PP_corpus, 2)\n",
    "df = pd.DataFrame(data, columns = ['input', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "Suj9HRjCBhnM",
    "outputId": "463934f6-8602-4902-e9be-918f82723033"
   },
   "outputs": [],
   "source": [
    "'''test for data_gen'''\n",
    "def test_data_gen():\n",
    "  assert df.shape == (66, 2)\n",
    "  print('Test passed', '\\U0001F44D')\n",
    "test_data_gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJIzl-HFukA9"
   },
   "source": [
    "# One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "zUt3-mDHus1l",
    "nbgrader": {
     "checksum": "36379a74d776c369a1ffc816c7acb388",
     "grade": false,
     "grade_id": "cell-b07927358b5097a5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def to_one_hot_encoding(data_point_index, one_hot_dim):\n",
    "    '''\n",
    "    Input:\n",
    "        data_point_index: integer value between 0 and one_hot_dim (index at which the one_hot_encoding array value will be 1 and 0 otherwise)\n",
    "        one_hot_dim : int, vocabulary size\n",
    "    Output:\n",
    "        one_hot_encoding: np array of size (vocabulary size, ) containing one hot encoding\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    return one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "72LPkj0jFsNX",
    "outputId": "0479ea3d-8d9c-4b2e-be89-f60c212f949c"
   },
   "outputs": [],
   "source": [
    "'''test for to_one_hot_encoding'''\n",
    "def test_to_one_hot_encoding():\n",
    "  arr = to_one_hot_encoding(4, len(unique_words))\n",
    "  assert arr.tolist() == [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "  print('Test passed', '\\U0001F44D')\n",
    "test_to_one_hot_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WdYK9998uu-X"
   },
   "source": [
    "## Change  data into 1-hot encoding for Skip-gram Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "rEvHx2rBTtKr",
    "nbgrader": {
     "checksum": "3df08213f7b824de9f4bdcf4c551bd0e",
     "grade": false,
     "grade_id": "cell-faf55ea0c88d541e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_for_skip_gram(word2int, data):\n",
    "    '''\n",
    "    Input:\n",
    "        word2int: dictionary, mapping from vocabulary words to ints\n",
    "            data: list of tuples (x, y) list of tuples (x, y). x is the id of the target word (the center word in current window) and y is the id of the context word\n",
    "    Output:\n",
    "               X: numpy array of shape (samples, vocabulary_size) containing input words\n",
    "               Y: numpy array of shape (samples, vocabulary_size) containing target words corresponding to input words in X\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    return X, Y\n",
    "    \n",
    "X_train,Y_train = one_hot_for_skip_gram(word2int, data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "__5xfrAbH3nP",
    "outputId": "2c48ec0a-798e-4036-adbe-3200690691c3"
   },
   "outputs": [],
   "source": [
    "'''test for Skip_gram_to_one_hot'''\n",
    "def test_one_hot_for_skip_gram():\n",
    "  assert Y_train.shape == (66,12)\n",
    "  print('Test passed', '\\U0001F44D')\n",
    "test_one_hot_for_skip_gram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dlwktzeuR1rV"
   },
   "source": [
    "# Define Model Architecture\n",
    "You could now train your model  using whatever optimizer you want.\n",
    "In order to keep track of your training, you should also print out the loss every 3000 iterations.\n",
    "Write your code in the cell below. Print out the loss every 3000 steps. Run your model for 20K epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYmSl5ieUpdU"
   },
   "source": [
    "![Skip Gram Model Architecture](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/Skip-gram-architecture-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c6M_jqI6QSJo"
   },
   "source": [
    "## Building skip gram network in Keras\n",
    "Description of the Network\n",
    "- There is only one hidden layer with 2 neurons and no activation\n",
    "- Input and output layers have same shape as one-hot encoded vectors\n",
    "- Output layer has activation softmax\n",
    "- Loss used is categorical_crossentropy\n",
    "\n",
    "\n",
    "Build this network using keras and train for at least 1000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "deletable": false,
    "id": "pubaFm43jG6V",
    "nbgrader": {
     "checksum": "75f3406499fa9c2b95926391b54cd39b",
     "grade": false,
     "grade_id": "cell-8e26d84ba4e2c91c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "f0badb50-45ba-4a0b-fdde-71503b1d12d9"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        None\n",
    "    Outputs:\n",
    "        model: compiled keras model for skipgram architecture\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "deletable": false,
    "id": "LSmU8J0GkhGn",
    "nbgrader": {
     "checksum": "b115b64b88ccd04693308a271809b4b5",
     "grade": false,
     "grade_id": "cell-14ace4e93b8e0d44",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "4ac101a8-5bb2-41ab-beb6-f5c16263f76b"
   },
   "outputs": [],
   "source": [
    "def get_weights_and_bias(model, layer_index):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        model: compiled keras model\n",
    "        layer_index: index of the layer\n",
    "    Outputs:\n",
    "        weights: weights of hidden layer\n",
    "        biases: biases of the hidden layer\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test for get_weights_and_bias\"\"\"\n",
    "model = create_model()\n",
    "w, b = get_weights_and_bias(model, 1)\n",
    "assert w.shape[1] == 2\n",
    "assert b.shape[0] == 2\n",
    "print('Test passed', '\\U0001F44D')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember the model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model_before = copy.copy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, epochs = 1000, batch_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKAEes3OzMit"
   },
   "source": [
    " ## (1) Word embedding extraction <br>\n",
    "Extract your word embedding matrix from the model and print out its shape.\n",
    "(The size should be [vocabulary_size, embedding_dimension])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "54ab467aa8a24fc349258b4590ad641b",
     "grade": false,
     "grade_id": "cell-1bfee504e9fc6b4b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings(model, flag):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        model: compiled keras model\n",
    "        flag:int, {0 or 1}, if 0 represents input vectors else represents output vectors\n",
    "    Outputs:\n",
    "        embeddings: numpy array of shape(vocabulary_size, embedding_dimension), word embeddings of all words\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_before = get_embeddings(model_before, 1)\n",
    "embeddings_after_input = get_embeddings(model, 1).T\n",
    "embeddings_after_output = get_embeddings(model, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VTlEFYOXzquh"
   },
   "source": [
    "## (2) Visualization\n",
    "<p>In this step, you need to visualize your word vectors by dimension reduction. (e.g. PCA or t-SNE)</p>\n",
    "<p>If you are not satisfied with the quality of your word vector from visualization (in most cases), you could try to change some parameters in your model (e.g. vocabulary_size, embedding_dimension) and re-train your word embedding.</p>\n",
    "\n",
    "Visualize the word vectors before and after training by changing vectors to either embeddings_before or embeddings_after.\n",
    "\n",
    "Visulaize the word vectors of the learned input vectors and learned output vectors and see the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "GrHYj4VjR1rh",
    "outputId": "98961f56-d734-4be2-84f6-98570abd527b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Set vectors\n",
    "vectors = embeddings_after_input\n",
    "\n",
    "\n",
    "# Build dataframe for vectors corrosponding to unique words where first column will be word.\n",
    "w2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])\n",
    "w2v_df['word'] = unique_words\n",
    "w2v_df = w2v_df[['word', 'x1', 'x2']]\n",
    "\n",
    "\n",
    "# Plot the vectors\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for word, x1, x2 in zip(w2v_df['word'], w2v_df['x1'], w2v_df['x2']):\n",
    "    ax.annotate(word, (x1,x2), bbox={'facecolor':'red', 'alpha':0.5, 'pad':5})\n",
    "    \n",
    "# w2v_df = pd.DataFrame(vec_before, columns = ['x1', 'x2'])\n",
    "# w2v_df['word'] = unique_words\n",
    "# w2v_df = w2v_df[['word', 'x1', 'x2']]\n",
    "# for word, x1, x2 in zip(w2v_df['word'], w2v_df['x1'], w2v_df['x2']):\n",
    "#     ax.annotate(word, (x1,x2), bbox={'facecolor':'blue', 'alpha':0.5, 'pad':5})\n",
    "\n",
    "    \n",
    "PADDING = 1.0\n",
    "x_axis_min = np.amin(vectors, axis=0)[0] - PADDING\n",
    "y_axis_min = np.amin(vectors, axis=0)[1] - PADDING\n",
    "x_axis_max = np.amax(vectors, axis=0)[0] + PADDING\n",
    "y_axis_max = np.amax(vectors, axis=0)[1] + PADDING\n",
    " \n",
    "plt.xlim(x_axis_min,x_axis_max)\n",
    "plt.ylim(y_axis_min,y_axis_max)\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Experiments\n",
    "- Tune hyperparameters to see if you can get better representations\n",
    "- Try to add more sentences using the same vocabulary (or expanding the vocabulary only slightly) to see if you can learn better representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_1_NLP_f.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
